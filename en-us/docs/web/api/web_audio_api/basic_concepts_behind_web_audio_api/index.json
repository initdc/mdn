{"doc":{"isMarkdown":true,"isTranslated":false,"isActive":true,"flaws":{},"title":"Basic concepts behind Web Audio API","mdn_url":"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","locale":"en-US","native":"English (US)","sidebarHTML":"<ol><li><strong><a href=\"/en-US/docs/Web/API/Web_Audio_API\">Web Audio API</a></strong></li><li class=\"toggle\"><details open=\"\"><summary>Guides</summary><ol><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API\">Using the Web Audio API</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API\">Basic concepts behind Web Audio API</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Best_practices\">Web Audio API best practices</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Advanced_techniques\">Advanced techniques: Creating and sequencing audio</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_AudioWorklet\">Background audio processing using AudioWorklet</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Controlling_multiple_parameters_with_ConstantSourceNode\">Controlling multiple parameters with ConstantSourceNode</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Migrating_from_webkitAudioContext\">Migrating from webkitAudioContext</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Simple_synth\">Example and tutorial: Simple synth keyboard</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Tools\">Tools for analyzing Web Audio usage</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_IIR_filters\">Using IIR filters</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">Visualizations with Web Audio API</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\">Web audio spatialization basics</a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>Interfaces</summary><ol><li><a href=\"/en-US/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioBufferSourceNode\"><code>AudioBufferSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioDestinationNode\"><code>AudioDestinationNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioListener\"><code>AudioListener</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioNode\"><code>AudioNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioParam\"><code>AudioParam</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioProcessingEvent\"><code>AudioProcessingEvent</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioScheduledSourceNode\"><code>AudioScheduledSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorklet\"><code>AudioWorklet</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletGlobalScope\"><code>AudioWorkletGlobalScope</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletNode\"><code>AudioWorkletNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletProcessor\"><code>AudioWorkletProcessor</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext\"><code>BaseAudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/BiquadFilterNode\"><code>BiquadFilterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ConstantSourceNode\"><code>ConstantSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ConvolverNode\"><code>ConvolverNode</code></a></li><li><a href=\"/en-US/docs/Web/API/DelayNode\"><code>DelayNode</code></a></li><li><a href=\"/en-US/docs/Web/API/DynamicsCompressorNode\"><code>DynamicsCompressorNode</code></a></li><li><a href=\"/en-US/docs/Web/API/GainNode\"><code>GainNode</code></a></li><li><a href=\"/en-US/docs/Web/API/IIRFilterNode\"><code>IIRFilterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaElementAudioSourceNode\"><code>MediaElementAudioSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaStreamAudioDestinationNode\"><code>MediaStreamAudioDestinationNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaStreamAudioSourceNode\"><code>MediaStreamAudioSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/OfflineAudioCompletionEvent\"><code>OfflineAudioCompletionEvent</code></a></li><li><a href=\"/en-US/docs/Web/API/OfflineAudioContext\"><code>OfflineAudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/OscillatorNode\"><code>OscillatorNode</code></a></li><li><a href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/PeriodicWave\"><code>PeriodicWave</code></a></li><li><a href=\"/en-US/docs/Web/API/WaveShaperNode\"><code>WaveShaperNode</code></a></li><li><a href=\"/en-US/docs/Web/API/StereoPannerNode\"><code>StereoPannerNode</code></a></li></ol></details></li></ol>","body":[{"type":"prose","value":{"id":null,"title":null,"isH3":false,"content":"<p>This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does.</p>"}},{"type":"prose","value":{"id":"audio_graphs","title":"Audio graphs","isH3":false,"content":"<p>The <a href=\"/en-US/docs/Web/API/Web_Audio_API\">Web Audio API</a> involves handling audio operations inside an <a href=\"/en-US/docs/Web/API/AudioContext\">audio context</a>, and has been designed to allow <em>modular routing</em>. Each <a href=\"/en-US/docs/Web/API/AudioNode\">audio node</a> performs a basic audio operation and is linked with one more other audio nodes to form an <a href=\"/en-US/docs/Web/API/AudioNode#the_audio_routing_graph\">audio routing graph</a>. Several sources with different channel layouts are supported, even within a single context. This modular design provides the flexibility to create complex audio functions with dynamic effects.</p>\n<p>Audio nodes are linked via their inputs and outputs, forming a chain that starts with one or more sources, goes through one or more nodes, then ends up at a destination (although you don't have to provide a destination if you only want to visualize some audio data). A simple, typical workflow for web audio would look something like this:</p>\n<ol>\n  <li>Create the audio context.</li>\n  <li>Create audio sources inside the context (such as <a href=\"/en-US/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a>, an oscillator, or stream).</li>\n  <li>Create audio effects (such as the reverb, biquad filter, panner, or compressor nodes).</li>\n  <li>Choose the final destination for the audio (such as the user's computer speakers).</li>\n  <li>Connect the source nodes to zero or more effect nodes and then to the chosen destination.</li>\n</ol>\n<div class=\"notecard note\" id=\"sect1\">\n  <p><strong>Note:</strong> The <a href=\"https://en.wikipedia.org/wiki/Surround_sound#Channel_notation\" class=\"external\" rel=\" noopener\">channel notation</a> is a numeric value, such as <em>2.0</em> or <em>5.1</em>, representing the number of audio channels available on a signal. The first number is the number of full frequency range audio channels the signal includes. The number appearing after the period indicates the number of those channels reserved for low-frequency effect (LFE) outputs; these are often called <strong>subwoofers</strong>.</p>\n</div>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudioapi_en.svg\" alt=\"A simple box diagram with an outer box labeled Audio context and three inner boxes labeled Sources, Effects, and Destination. The three inner boxes have arrows between them pointing from left to right, indicating the flow of audio information.\" width=\"643\" height=\"143\" loading=\"lazy\">\n</p>\n<p>Each input or output is composed of one or more audio <strong>channels</strong>, which together represent a specific audio layout. Any discrete channel structure is supported, including <em>mono</em>, <em>stereo</em>, <em>quad</em>, <em>5.1</em>, and so on.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/mdn.png\" alt=\"Show the ability of audio nodes to connect via their inputs and outputs and the channels inside these inputs/outputs.\" width=\"1308\" height=\"750\" loading=\"lazy\">\n</p>\n<p>You have several ways to obtain audio:</p>\n<ul>\n  <li>Sound can be generated directly in JavaScript by an audio node (such as an oscillator).</li>\n  <li>It can be created from raw <a href=\"https://en.wikipedia.org/wiki/Pulse-code_modulation\" class=\"external\" rel=\" noopener\">PCM</a> data (such as .WAV files or other formats supported by <a href=\"/en-US/docs/Web/API/BaseAudioContext/decodeAudioData\" title=\"decodeAudioData()\"><code>decodeAudioData()</code></a>).</li>\n  <li>It can be generated from HTML media elements, such as <a href=\"/en-US/docs/Web/HTML/Element/video\"><code>&lt;video&gt;</code></a> or <a href=\"/en-US/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a>.</li>\n  <li>It can be obtained from a <a href=\"/en-US/docs/Web/API/WebRTC_API\">WebRTC</a> <a href=\"/en-US/docs/Web/API/MediaStream\"><code>MediaStream</code></a>, such as a webcam or microphone.</li>\n</ul>"}},{"type":"prose","value":{"id":"audio_data_whats_in_a_sample","title":"Audio data: what's in a sample","isH3":false,"content":"<p>When an audio signal is processed, sampling happens. <strong>Sampling</strong> is the conversion of a <a href=\"https://en.wikipedia.org/wiki/Continuous_signal\" class=\"external\" rel=\" noopener\">continuous signal</a> to a <a href=\"https://en.wikipedia.org/wiki/Discrete_signal\" class=\"external\" rel=\" noopener\">discrete signal</a>. Put another way, a continuous sound wave, such as a band playing live, is converted into a sequence of digital samples (a discrete-time signal) that allows a computer to handle the audio in distinct blocks.</p>\n<p>You'll find more information on the Wikipedia page <a href=\"https://en.wikipedia.org/wiki/Sampling_%28signal_processing%29\" class=\"external\" rel=\" noopener\"><em>Sampling (signal processing)</em></a>.</p>"}},{"type":"prose","value":{"id":"audio_buffers_frames_samples_and_channels","title":"Audio buffers: frames, samples, and channels","isH3":false,"content":"<p>An <a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a> is defined with three parameters:</p>\n<ul>\n  <li>the number of channels (1 for mono, 2 for stereo, etc.),</li>\n  <li>its length, meaning the number of sample frames inside the buffer,</li>\n  <li>and the sample rate, the number of sample frames played per second.</li>\n</ul>\n<p>A <em>sample</em> is a single 32-bit floating point value representing the value of the audio stream at each specific moment in time within a particular channel (left or right, if in the case of stereo). A <em>frame</em>, or <em>sample frame</em>, is the set of all values for all channels that will play at a specific moment in time: all the samples of all the channels that play at the same time (two for a stereo sound, six for 5.1, etc.).</p>\n<p>The <em>sample rate</em> is the quantity of those samples (or frames, since all samples of a frame play at the same time) that will play in one second, measured in Hz. The higher the sample rate, the better the sound quality.</p>\n<p>Let's look at a <em>mono</em> and a <em>stereo</em> audio buffer, each one second long at a rate of 44100Hz:</p>\n<ul>\n  <li>The <em>mono</em> buffer will have 44,100 samples and 44,100 frames. The <code>length</code> property will be 44,100.</li>\n  <li>The <em>stereo</em> buffer will have 88,200 samples but still 44,100 frames. The <code>length</code> property will still be 44100 since it equals the number of frames.</li>\n</ul>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/sampleframe-english.png\" alt=\"A diagram showing several frames in an audio buffer in a long line, each one containing two samples, as the buffer has two channels, it is stereo.\" width=\"1245\" height=\"219\" loading=\"lazy\">\n</p>\n<p>When a buffer plays, you will first hear the leftmost sample frame, then the one right next to it, then the next, <em>and so on</em>, until the end of the buffer. In the case of stereo, you will hear both channels simultaneously. Sample frames are handy because they are independent of the number of channels and represent time in an ideal way for precise audio manipulation.</p>\n<div class=\"notecard note\" id=\"sect2\">\n  <p><strong>Note:</strong> To get a time in seconds from a frame count, divide the number of frames by the sample rate. To get the number of frames from the number of samples, you only need to divide the latter value by the channel count.</p>\n</div>\n<p>Here are a couple of simple examples:</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">const</span> context <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> buffer <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioBuffer</span><span class=\"token punctuation\">(</span>context<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token literal-property property\">numberOfChannels</span><span class=\"token operator\">:</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">length</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">sampleRate</span><span class=\"token operator\">:</span> <span class=\"token number\">44100</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<div class=\"notecard note\" id=\"sect3\">\n  <p><strong>Note:</strong> In <a href=\"https://en.wikipedia.org/wiki/Digital_audio\" class=\"external\" rel=\" noopener\">digital audio</a>, <strong>44,100 <a href=\"https://en.wikipedia.org/wiki/Hertz\" class=\"external\" rel=\" noopener\">Hz</a></strong> (alternately represented as <strong>44.1 kHz</strong>) is a common <a href=\"https://en.wikipedia.org/wiki/Sampling_frequency\" class=\"external\" rel=\" noopener\">sampling frequency</a>. Why 44.1 kHz?</p>\n  <p>Firstly, because the <a href=\"https://en.wikipedia.org/wiki/Hearing_range\" class=\"external\" rel=\" noopener\">hearing range</a> of human ears is roughly 20 Hz to 20,000 Hz. Via the <a href=\"https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\" class=\"external\" rel=\" noopener\">Nyquist–Shannon sampling theorem</a>, the sampling frequency must be greater than twice the maximum frequency one wishes to reproduce. Therefore, the sampling rate has to be <em>greater</em> than 40,000 Hz.</p>\n  <p>Secondly, signals must be <a href=\"https://en.wikipedia.org/wiki/Low-pass_filter\" class=\"external\" rel=\" noopener\">low-pass filtered</a> before sampling, otherwise <a href=\"https://en.wikipedia.org/wiki/Aliasing\" class=\"external\" rel=\" noopener\">aliasing</a> occurs. While an ideal low-pass filter would perfectly pass frequencies below 20 kHz (without attenuating them) and perfectly cut off frequencies above 20 kHz, in practice, a <a href=\"https://en.wikipedia.org/wiki/Transition_band\" class=\"external\" rel=\" noopener\">transition band</a> is necessary, where frequencies are partly attenuated. The wider this transition band is, the easier and more economical it is to make an <a href=\"https://en.wikipedia.org/wiki/Anti-aliasing_filter\" class=\"external\" rel=\" noopener\">anti-aliasing filter</a>. The 44.1 kHz sampling frequency allows for a 2.05 kHz transition band.</p>\n</div>\n<p>If you use this call above, you will get a stereo buffer with two channels that, when played back on an <a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a> running at 44100 Hz (very common, most normal sound cards run at this rate), will last for 0.5 seconds: 22,050 frames/44,100 Hz = 0.5 seconds.</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">const</span> context <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> buffer <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioBuffer</span><span class=\"token punctuation\">(</span>context<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token literal-property property\">numberOfChannels</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">length</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">sampleRate</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<p>If you use this call, you will get a mono buffer (single-channel buffer) that, when played back on an <a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a> running at 44,100 Hz, will be automatically <em>resampled</em> to 44,100 Hz (and therefore yield 44,100 frames), and last for 1.0 second: 44,100 frames/44,100 Hz = 1 second.</p>\n<div class=\"notecard note\" id=\"sect4\">\n  <p><strong>Note:</strong> Audio resampling is very similar to image resizing. Say you've got a 16 x 16 image but want it to fill a 32 x 32 area. You resize (or resample) it. The result has less quality (it can be blurry or edgy, depending on the resizing algorithm), but it works, with the resized image taking up less space. Resampled audio is the same: you save space, but, in practice, you cannot correctly reproduce high-frequency content or treble sound.</p>\n</div>"}},{"type":"prose","value":{"id":"planar_versus_interleaved_buffers","title":"Planar versus interleaved buffers","isH3":true,"content":"<p>The Web Audio API uses a planar buffer format. The left and right channels are stored like this:</p>\n<pre class=\"notranslate\">LLLLLLLLLLLLLLLLRRRRRRRRRRRRRRRR (for a buffer of 16 frames)\n</pre>\n<p>This structure is widespread in audio processing, making it easy to process each channel independently.</p>\n<p>The alternative is to use an interleaved buffer format:</p>\n<pre class=\"notranslate\">LRLRLRLRLRLRLRLRLRLRLRLRLRLRLRLR (for a buffer of 16 frames)\n</pre>\n<p>This format is prevalent for storing and playing back audio without much processing, for example: .WAV files or a decoded MP3 stream.</p>\n<p>Because the Web Audio API is designed for processing, it exposes <em>only</em> planar buffers. It uses the planar format but converts the audio to the interleaved format when it sends it to the sound card for playback. Conversely, when the API decodes an MP3, it starts with the interleaved format and converts it to the planar format for processing.</p>"}},{"type":"prose","value":{"id":"audio_channels","title":"Audio channels","isH3":false,"content":"<p>Each audio buffer may contain different numbers of channels. Most modern audio devices use the basic <em>mono</em> (only one channel) and <em>stereo</em> (left and right channels) settings. Some more complex sets support <em>surround sound</em> settings (like <em>quad</em> and <em>5.1</em>), which can lead to a richer sound experience thanks to their high channel count. We usually represent the channels with the standard abbreviations detailed in the table below:</p>\n<table>\n  <thead>\n    <tr>\n      <th>Name</th>\n      <th>Channels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><em>Mono</em></td>\n      <td><code>0: M: mono</code></td>\n    </tr>\n    <tr>\n      <td><em>Stereo</em></td>\n      <td><code>0: L: left 1: R: right</code></td>\n    </tr>\n    <tr>\n      <td><em>Quad</em></td>\n      <td><code>0: L: left 1: R: right 2: SL: surround left 3: SR: surround right</code></td>\n    </tr>\n    <tr>\n      <td><em>5.1</em></td>\n      <td><code>0: L: left 1: R: right 2: C: center 3: LFE: subwoofer 4: SL: surround left 5: SR: surround right</code></td>\n    </tr>\n  </tbody>\n</table>"}},{"type":"prose","value":{"id":"up-mixing_and_down-mixing","title":"Up-mixing and down-mixing","isH3":true,"content":"<p>When the numbers of channels of the input and the output don't match, up-mixing, or down-mixing, must be done. The following rules, controlled by setting the <a href=\"/en-US/docs/Web/API/AudioNode/channelInterpretation\"><code>AudioNode.channelInterpretation</code></a> property to <code>speakers</code> or <code>discrete</code>, apply:</p>\n<table class=\"standard-table\">\n  <thead>\n    <tr>\n      <th scope=\"row\">Interpretation</th>\n      <th scope=\"col\">Input channels</th>\n      <th scope=\"col\">Output channels</th>\n      <th scope=\"col\">Mixing rules</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"13\" scope=\"row\"><code>speakers</code></th>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Up-mix from mono to stereo</em>.<br>The <code>M</code> input\n        channel is used for both output channels (<code>L</code> and\n        <code>R</code>).<br><code>output.L = input.M<br>output.R = input.M</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Up-mix from mono to quad.</em><br>The <code>M</code> input channel\n        is used for non-surround output channels (<code>L</code> and\n        <code>R</code>). Surround output channels (<code>SL</code> and\n        <code>SR</code>) are silent.<br><code>output.L = input.M<br>output.R = input.M<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from mono to 5.1.</em><br>The <code>M</code> input channel\n        is used for the center output channel (<code>C</code>). All the others\n        (<code>L</code>, <code>R</code>, <code>LFE</code>, <code>SL</code>, and\n        <code>SR</code>) are silent.<br><code>output.L = 0<br>output.R = 0</code><br><code>output.C = input.M<br>output.LFE = 0<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from stereo to mono</em>.<br>Both input channels (<code>L</code>\n        and <code>R</code>) are equally combined to produce the unique output\n        channel (<code>M</code>).<br><code>output.M = 0.5 * (input.L + input.R)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Up-mix from stereo to quad.</em><br>The <code>L</code> and\n        <code>R </code>input channels are used for their non-surround respective\n        output channels (<code>L</code> and <code>R</code>). Surround output\n        channels (<code>SL</code> and <code>SR</code>) are silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from stereo to 5.1.</em><br>The <code>L</code> and\n        <code>R </code>input channels are used for their non-surround respective\n        output channels (<code>L</code> and <code>R</code>). Surround output\n        channels (<code>SL</code> and <code>SR</code>), as well as the center\n        (<code>C</code>) and subwoofer (<code>LFE</code>) channels, are left\n        silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = 0<br>output.SR = 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from quad to mono</em>.<br>All four input channels\n        (<code>L</code>, <code>R</code>, <code>SL</code>, and <code>SR</code>)\n        are equally combined to produce the unique output channel\n        (<code>M</code>).<br><code>output.M = 0.25 * (input.L + input.R + </code><code>input.SL + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Down-mix from quad to stereo</em>.<br>Both left input channels\n        (<code>L</code> and <code>SL</code>) are equally combined to produce the\n        unique left output channel (<code>L</code>). And similarly, both right\n        input channels (<code>R</code> and <code>SR</code>) are equally combined\n        to produce the unique right output channel (<code>R</code>).<br><code>output.L = 0.5 * (input.L + input.SL</code><code>)</code><br><code>output.R = 0.5 * (input.R + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from quad to 5.1.</em><br>The <code>L</code>,\n        <code>R</code>, <code>SL</code>, and <code>SR</code> input channels are\n        used for their respective output channels (<code>L</code> and\n        <code>R</code>). Center (<code>C</code>) and subwoofer\n        (<code>LFE</code>) channels are left silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = input.SL<br>output.SR = input.SR</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to mono.</em><br>The left (<code>L</code> and\n        <code>SL</code>), right (<code>R</code> and <code>SR</code>) and central\n        channels are all mixed together. The surround channels are slightly\n        attenuated, and the regular lateral channels are power-compensated to\n        make them count as a single channel by multiplying by <code>√2/2</code>.\n        The subwoofer (<code>LFE</code>) channel is lost.<br><code>output.M = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL +\ninput.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to stereo.</em><br>The central channel\n        (<code>C</code>) is summed with each lateral surround channel (<code>SL</code>\n        or <code>SR</code>) and mixed to each lateral channel. As it is mixed\n        down to two channels, it is mixed at a lower power: in each case, it is\n        multiplied by <code>√2/2</code>. The subwoofer (<code>LFE</code>)\n        channel is lost.<br><code>output.L = input.L + 0.7071 * (input.C + input.SL)<br>output.R =\ninput.R </code><code>+ 0.7071 * (input.C + input.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to quad.</em><br>The central (<code>C</code>) is\n        mixed with the lateral non-surround channels (<code>L</code> and\n        <code>R</code>). As it is mixed down to two channels, it is mixed at a\n        lower power: in each case, it is multiplied by <code>√2/2</code>. The\n        surround channels are passed unchanged. The subwoofer (<code>LFE</code>)\n        channel is lost.<br><code>output.L = input.L + 0.7071 * input.C<br>output.R = input.R +\n0.7071 * input.C<br>output.SL = input.SL<br>output.SR =\ninput.SR</code>\n      </td>\n    </tr>\n    <tr>\n      <td colspan=\"2\">Other, non-standard layouts</td>\n      <td>\n        Non-standard channel layouts behave as if\n        <code>channelInterpretation</code> is set to\n        <code>discrete</code>.<br>The specification explicitly allows the future definition of new speaker layouts. Therefore, this fallback is not future-proof as the behavior of the browsers for a specific number of channels may change in the future.\n      </td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" scope=\"row\"><code>discrete</code></th>\n      <td>any (<code>x</code>)</td>\n      <td>any (<code>y</code>) where <code>x&lt;y</code></td>\n      <td>\n        <em>Up-mix discrete channels.</em><br>Fill each output channel with\n        its input counterpart — that is, the input channel with the same index.\n        Channels with no corresponding input channels are left silent.\n      </td>\n    </tr>\n    <tr>\n      <td>any (<code>x</code>)</td>\n      <td>any (<code>y</code>) where <code>x&gt;y</code></td>\n      <td>\n        <em>Down-mix discrete channels.</em><br>Fill each output channel with\n        its input counterpart — that is, the input channel with the same index.\n        Input channels with no corresponding output channels are dropped.\n      </td>\n    </tr>\n  </tbody>\n</table>"}},{"type":"prose","value":{"id":"visualizations","title":"Visualizations","isH3":false,"content":"<p>In general, we get the output over time to produce audio visualizations, usually reading its gain or frequency data. Then, using a graphical tool, we turn the obtained data into a visual representation, such as a graph. The Web Audio API has an <a href=\"/en-US/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a> available that doesn't alter the audio signal passing through it. Additionally, it outputs the audio data, allowing us to process it via a technology such as <a href=\"/en-US/docs/Web/HTML/Element/canvas\"><code>&lt;canvas&gt;</code></a>.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fttaudiodata_en.svg\" alt=\"Without modifying the audio stream, the node allows to get the frequency and time-domain data associated with it, using an FFT.\" width=\"693\" height=\"206\" loading=\"lazy\">\n</p>\n<p>You can grab data using the following methods:</p>\n<dl>\n  <dt id=\"analysernode.getfloatfrequencydata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData\"><code>AnalyserNode.getFloatFrequencyData()</code></a></dt>\n  <dd>\n    <p>Copies the current frequency data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code></a> array passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getbytefrequencydata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData\"><code>AnalyserNode.getByteFrequencyData()</code></a></dt>\n  <dd>\n    <p>Copies the current frequency data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getfloattimedomaindata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getFloatTimeDomainData\"><code>AnalyserNode.getFloatTimeDomainData()</code></a></dt>\n  <dd>\n    <p>Copies the current waveform, or time-domain, data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code></a> array passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getbytetimedomaindata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getByteTimeDomainData\"><code>AnalyserNode.getByteTimeDomainData()</code></a></dt>\n  <dd>\n    <p>Copies the current waveform, or time-domain, data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>\n  </dd>\n</dl>\n<div class=\"notecard note\" id=\"sect5\">\n  <p><strong>Note:</strong> For more information, see our <a href=\"/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">Visualizations with Web Audio API</a> article.</p>\n</div>"}},{"type":"prose","value":{"id":"spatializations","title":"Spatializations","isH3":false,"content":"<p>Audio spatialization allows us to model the position and behavior of an audio signal at a certain point in physical space, simulating the listener hearing that audio. In the Web Audio API, spatialization is handled by the <a href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code></a> and the <a href=\"/en-US/docs/Web/API/AudioListener\"><code>AudioListener</code></a>.</p>\n<p>The panner uses right-hand Cartesian coordinates to describe the audio source's <em>position</em> as a vector and its <em>orientation</em> as a 3D directional cone. The cone can be pretty large, for example, for omnidirectional sources.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/pannernode_en.svg\" alt=\"The PannerNode defines a spatial position and orientation for a given signal.\" width=\"799\" height=\"340\" loading=\"lazy\">\n</p>\n<p>Similarly, the Web Audio API describes the listener using right-hand Cartesian coordinates: their <em>position</em> as one vector and their <em>orientation</em> as two direction vectors, <em>up</em> and <em>front</em>. These vectors define the direction of the top of the listener's head and the direction the listener's nose is pointing. The vectors are perpendicular to one another.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudiolistenerreduced.png\" alt=\"We see the position, up, and front vectors of an AudioListener, with the up and front vectors at 90° from the other.\" width=\"634\" height=\"250\" loading=\"lazy\">\n</p>\n<div class=\"notecard note\" id=\"sect6\">\n  <p><strong>Note:</strong> For more information, see our <a href=\"/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\">Web audio spatialization basics</a> article.</p>\n</div>"}},{"type":"prose","value":{"id":"fan-in_and_fan-out","title":"Fan-in and Fan-out","isH3":false,"content":"<p>In audio terms, <strong>fan-in</strong> describes the process by which a <a href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a> takes a series of <em>mono</em> input sources and outputs a single multi-channel signal:</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanin.svg\" alt=\"\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>\n<p><strong>Fan-out</strong> describes the opposite process, whereby a <a href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code></a> takes a multi-channel input source and outputs multiple <em>mono</em> output signals:</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanout.svg\" alt=\"\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>"}}],"toc":[{"text":"Audio graphs","id":"audio_graphs"},{"text":"Audio data: what's in a sample","id":"audio_data_whats_in_a_sample"},{"text":"Audio buffers: frames, samples, and channels","id":"audio_buffers_frames_samples_and_channels"},{"text":"Audio channels","id":"audio_channels"},{"text":"Visualizations","id":"visualizations"},{"text":"Spatializations","id":"spatializations"},{"text":"Fan-in and Fan-out","id":"fan-in_and_fan-out"}],"summary":"This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does.","popularity":0.0011,"modified":"2022-08-15T04:42:34.000Z","other_translations":[{"title":"Les concepts de base de la Web Audio API","locale":"fr","native":"Français"},{"title":"Basic concepts behind Web Audio API","locale":"ja","native":"日本語"},{"title":"Web Audio API의 기본 개념","locale":"ko","native":"한국어"},{"title":"网页音频接口的基本概念","locale":"zh-CN","native":"中文 (简体)"}],"source":{"folder":"en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api","github_url":"https://github.com/mdn/content/blob/style/old/files/en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md","last_commit_url":"https://github.com/mdn/content/commit/98913882aa1e4648b84b4e008d50fe50960eb10c","filename":"index.md"},"parents":[{"uri":"/en-US/docs/Web","title":"Web technology for developers"},{"uri":"/en-US/docs/Web/API","title":"Web APIs"},{"uri":"/en-US/docs/Web/API/Web_Audio_API","title":"Web Audio API"},{"uri":"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","title":"Basic concepts behind Web Audio API"}],"pageTitle":"Basic concepts behind Web Audio API - Web APIs | MDN","noIndexing":false}}