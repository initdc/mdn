<!DOCTYPE html><html lang="en-US" prefix="og: https://ogp.me/ns#"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="/favicon-48x48.97046865.png"><link rel="apple-touch-icon" href="/apple-touch-icon.0ea0fa02.png"><meta name="theme-color" content="#ffffff"><link rel="manifest" href="/manifest.56b1cedc.json"><link rel="search" type="application/opensearchdescription+xml" href="/opensearch.xml" title="MDN Web Docs"><script>Array.prototype.flat&&Array.prototype.includes||document.write('<script src="https://polyfill.io/v3/polyfill.min.js?features=Array.prototype.flat%2Ces6"><\/script>')</script><title>Basic concepts behind Web Audio API - Web APIs | MDN</title><link rel="preload" as="font" type="font/woff2" crossorigin="" href="/static/media/ZillaSlab-Bold.subset.0beac26b.woff2"><link rel="alternate" title="Basic concepts behind Web Audio API" href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="en"><link rel="alternate" title="ÁΩëÈ°µÈü≥È¢ëÊé•Âè£ÁöÑÂü∫Êú¨Ê¶ÇÂøµ" href="https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="zh"><link rel="alternate" title="Web Audio APIÏùò Í∏∞Î≥∏ Í∞úÎÖê" href="https://developer.mozilla.org/ko/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="ko"><link rel="alternate" title="Basic concepts behind Web Audio API" href="https://developer.mozilla.org/ja/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="ja"><link rel="alternate" title="Les concepts de base de la Web Audio API" href="https://developer.mozilla.org/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API" hreflang="fr"><meta name="description" content="This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does."><meta property="og:url" content="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API"><meta property="og:title" content="Basic concepts behind Web Audio API - Web APIs | MDN"><meta property="og:description" content="This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does."><meta property="og:locale" content="en-US"><meta property="og:image" content="https://developer.mozilla.org/mdn-social-share.0ca9dbda.png"><meta property="twitter:card" content="summary_large_image"><meta name="robots" content="index, follow"><link rel="canonical" href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API"><style media="print">.breadcrumbs-container,.document-toc-container,.language-menu,.language-toggle,.on-github,.page-footer,.page-header-main,nav.sidebar,ul.prev-next{display:none!important}.main-page-content,.main-page-content pre{padding:2px}.main-page-content pre{border-left-width:2px}</style><link href="/static/css/main.55b68e13.chunk.css" rel="stylesheet"><script src="/static/js/runtime-main.4bb4c356.js" defer=""></script><script src="/static/js/4.a756dea3.chunk.js" defer=""></script><script src="/static/js/main.d4d565e0.chunk.js" defer=""></script></head><body><div id="root"><ul id="nav-access" class="a11y-nav"><li><a id="skip-main" href="#content">Skip to main content</a></li><li><a id="skip-search" href="#main-q">Skip to search</a></li><li><a id="skip-select-language" href="#select-language">Skip to select language</a></li></ul><div class="page-wrapper document-page"><header class="page-header"><a href="/en-US/" class="logo" aria-label="MDN Web Docs"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 451.74 135"><path d="M7.14 8.35v111.06h111.05V8.35zm103.71 56c-.48.92-1 1.79-1.46 2.71a3.44 3.44 0 01-3.54 2 2.4 2.4 0 00-1.55.5c-1.37.9-2.76 1.79-4.18 2.63a7.33 7.33 0 01-6.35.34 29.71 29.71 0 00-10.63-2 11.7 11.7 0 00-9.46 4.31 14.84 14.84 0 00-2.13 4.29c-1.24 3.07-2.3 21.38-2.3 26.05 0 0-17.62-3.42-34.15-20.34l4.31-11.32h-13.5l9.76-10.35h-16.8l9.77-10.34H12.69L30.45 34a40.9 40.9 0 0119.77-10.83c7.1-1.22 8.93-.53 13.31.77l2.43.73.85.25 3.1.95a12.56 12.56 0 006.21.09 11.37 11.37 0 018.25 1 8.24 8.24 0 014.1 6.22 7.29 7.29 0 003.61 5.49 59.45 59.45 0 009.32 4.11c2.27.86 4.54 1.84 6.79 2.72a6.81 6.81 0 012.86 2.06 4.81 4.81 0 011.1 2.73c.14 2 .37 4 .47 6a15.24 15.24 0 01-1.77 8.03zM320.12 39.62a5.42 5.42 0 00-4.53 2.13 7.36 7.36 0 00-1.7 4.43v2.36a6.28 6.28 0 001.7 4.46 5.63 5.63 0 004.3 1.82 5.12 5.12 0 004.57-2.27A9.7 9.7 0 00326 47a8.11 8.11 0 00-1.67-5.52 5.36 5.36 0 00-4.21-1.86zM387.38 39.53a5.52 5.52 0 00-4.7 2.15 8.8 8.8 0 00-1.63 5.49 9.23 9.23 0 001.58 5.45 5.38 5.38 0 004.7 2.25 5.61 5.61 0 004.74-2.2 8.91 8.91 0 001.68-5.59 8.24 8.24 0 00-1.75-5.52 5.76 5.76 0 00-4.62-2.03zM299.47 41.35a4.34 4.34 0 00-4-1.92 4.55 4.55 0 00-3.89 1.73 8.37 8.37 0 00-1.58 4.17h10.48a6.3 6.3 0 00-1.01-3.98zM357.74 30.75H352v23.31h5.72q5.47 0 8.35-3t2.93-8.65q0-5.43-2.88-8.55t-8.38-3.11z"></path><path d="M121.55 8.35v70.8h323V8.35zm42.21 22.45h-4V54h3.68v3.73h-11.25V54h3.31V36.79h-.19l-9.63 19.12h-2.12l-10-19.4h-.19V54h3.45v3.73h-11.15V54h3.68V30.8h-4v-3.73H133l11.66 22.56h.19l11.18-22.56h7.7zm29.12 22.67q-4.11 4.28-11.38 4.28h-14.06v-3.69h3.73V30.75h-3.73v-3.68h13.83q7.59 0 11.66 4.29a15.4 15.4 0 014 11 15.33 15.33 0 01-4.05 11.11zm38.89-22.67h-3.68v27h-2.6L208.08 35h-.19v19h4.67v3.73h-12.22V54h3.49V30.8h-4v-3.73h7.08l16.9 22.09h.19V30.8h-4.58v-3.73h12.32zm43.8 27h-3.31l-7.83-23.18h-.19l-7.55 23.18h-3.35l-8.78-27h-2.65v-3.73H253v3.73h-3.87L255 50.71h.23l6.61-19.91H259v-3.73h11v3.73h-2.78l6.61 20.1h.23l5.43-20.1h-4.15v-3.73h11v3.73h-2.54zm26.71-1.51a9.66 9.66 0 01-6.42 2 10.2 10.2 0 01-7.41-2.74c-1.89-1.82-2.83-4.47-2.83-7.93a12.37 12.37 0 012.64-8.12 9 9 0 017.32-3.21 8.62 8.62 0 016.75 2.69 9.65 9.65 0 012.45 6.52 13.67 13.67 0 01-.28 2.69H290q.29 6.71 6.18 6.7a5.2 5.2 0 003.71-1.18 5.82 5.82 0 001.67-2.83l3.45.71a7.21 7.21 0 01-2.73 4.65zm25.77-1.63c-1.51 2.4-3.92 3.61-7.22 3.61s-5.84-1.29-7.22-3.87c0 .25-.1.82-.21 1.7s-.19 1.44-.22 1.7H309c.16-1 .31-2 .47-3.07a21.42 21.42 0 00.24-3.16v-23h-3.4v-3.3h7.55V40.9a9.76 9.76 0 012.67-3.28 7.33 7.33 0 014.74-1.4 8.48 8.48 0 016.5 2.78q2.55 2.74 2.55 7.74a14.6 14.6 0 01-2.27 7.87zm41.39-1.14q-4.11 4.28-11.37 4.28H344v-3.74h3.73V30.75H344v-3.68h13.83q7.59 0 11.66 4.29a15.41 15.41 0 014.06 11 15.34 15.34 0 01-4.11 11.11zm25.65 1.68a10.53 10.53 0 01-7.9 3.07 10 10 0 01-7.63-3 10.93 10.93 0 01-2.8-7.83 12.13 12.13 0 012.69-7.93q2.69-3.3 8-3.3t8 3.28a12 12 0 012.64 7.76 10.86 10.86 0 01-3 7.9zm22.61.57c-1.4 1.66-3.63 2.5-6.68 2.5a9.58 9.58 0 01-7.15-2.76q-2.72-2.76-2.71-7.91a12.25 12.25 0 012.69-8 9.17 9.17 0 017.5-3.28 15 15 0 013.82.48 10.37 10.37 0 013.5 1.65l.85 5.47-3.35.38-.76-3.54a8.07 8.07 0 00-4.11-1 4.9 4.9 0 00-4.39 2.15 9.93 9.93 0 00-1.41 5.55 8.9 8.9 0 001.5 5.38 5.23 5.23 0 004.44 2c2.92 0 4.67-1.7 5.23-5.1l3.5.71a10.34 10.34 0 01-2.47 5.27zm20.48.75a11.68 11.68 0 01-6.63 1.75 15.52 15.52 0 01-8.26-2.08L424 51l3.26.33-.1 2.74a7 7 0 002.06.66 12.63 12.63 0 002.19.19 8.68 8.68 0 003.66-.75 2.5 2.5 0 001.63-2.36 2.25 2.25 0 00-1.32-2.2 12.65 12.65 0 00-3.28-1 47.39 47.39 0 01-3.9-.82 7.5 7.5 0 01-3.25-1.7 4.67 4.67 0 01-1.33-3.66c0-2.36.88-4 2.62-4.91a12 12 0 015.6-1.37 15 15 0 014.08.55 16.65 16.65 0 013.47 1.39l.47 5.1-3.3.37-.48-3.3a9.5 9.5 0 00-4.06-.9 5.62 5.62 0 00-2.87.66 2.33 2.33 0 00-1.15 2.25 2.13 2.13 0 001.3 2.07 11.91 11.91 0 003.21.92 36.69 36.69 0 013.82.83 7.46 7.46 0 013.21 1.74 4.9 4.9 0 011.3 3.73 5.56 5.56 0 01-2.66 4.91z"></path><path d="M181.17 30.75h-5.71v23.31h5.71q5.47 0 8.36-3t2.88-8.61q0-5.43-2.88-8.55t-8.36-3.15zM121.63 119.32V81.74h114.91v37.58zM153.22 109h-2v-6.85a4.8 4.8 0 00-1.58-4 5.57 5.57 0 00-3.55-1.26 5 5 0 00-4.92 3.26 4.19 4.19 0 00-1.88-2.46 5.82 5.82 0 00-3-.8 4.89 4.89 0 00-4.56 2.56v-2.21h-6.28v3.26h2v8.5h-2v3.23h9.11V109h-2.86v-5.25a4.4 4.4 0 01.69-2.56 2.47 2.47 0 012.21-1q2.57 0 2.56 3.63v8.41h6.29V109h-2v-5.25a4.47 4.47 0 01.67-2.56 2.42 2.42 0 012.19-1q2.63 0 2.63 3.63v8.41h6.28zm9.88-12.07q-4 0-6 2.36a8.41 8.41 0 00-2 5.66 7.25 7.25 0 002.17 5.62 8 8 0 005.65 2 8.54 8.54 0 005.94-2.11 7.27 7.27 0 002.34-5.67 8.21 8.21 0 00-2-5.51q-2.07-2.34-6.1-2.34zm-.1 12.35a3 3 0 01-2.63-1.33 5.68 5.68 0 01-.9-3.26 5 5 0 011-3.28 3.23 3.23 0 012.61-1.18 3.5 3.5 0 012.59 1.08 4.56 4.56 0 011.07 3.31 5.21 5.21 0 01-1 3.41 3.33 3.33 0 01-2.74 1.25zm25-2.3l-3.39-.29-.7 2.32H179l8.32-9.54-.32-2.23h-13.19l-.53 5.25 3.16.34.67-2.36h4.65l-8.25 9.53.44 2.26h13.13zm7.62-9.74h-4.46v5.39h4.46zm0 9.61h-4.46v5.39h4.46zm13.54-17.49h-4.23l-6.48 22.88h4.22zm8.68 0h-4.23l-6.45 22.88h4.19zm15 22.51l-.07-2.26a1.22 1.22 0 01-.56.1c-.69 0-1-.39-1-1.16v-6.49a4.39 4.39 0 00-1.8-3.84 7 7 0 00-4.16-1.28 14.55 14.55 0 00-3.16.3 24.14 24.14 0 00-3.29 1.06l-.56 3.46 3.39.4.5-1.69a2.78 2.78 0 011.08-.37 11.3 11.3 0 011.25-.07c1.19 0 1.89.37 2.09 1.1a8.55 8.55 0 01.3 2.26v.5a8.91 8.91 0 00-1.18-.11h-1.21a12.64 12.64 0 00-4.81.88 3.53 3.53 0 00-2.18 3.64 3.66 3.66 0 001.48 3.33 5.63 5.63 0 003.11 1 4.67 4.67 0 003-.91 6.78 6.78 0 001.8-2 3 3 0 003.33 3 5.54 5.54 0 002.66-.85zm-9.25-2.32a1.69 1.69 0 01-1.36-.52 1.81 1.81 0 01-.43-1.21 1.67 1.67 0 01.86-1.68 4.63 4.63 0 012-.42 7.69 7.69 0 011.07.07l1.06.13a3.58 3.58 0 01-1.08 2.74 3.24 3.24 0 01-2.11.89z"></path></svg></a><button type="button" class="ghost main-menu-toggle" aria-haspopup="true" aria-label="Show Menu"></button><div class="page-header-main "><nav class="main-nav" aria-label="Main menu"><ul class="main-menu nojs"><li class="top-level-entry-container"><button id="technologies-button" type="button" class="top-level-entry" aria-haspopup="menu" aria-expanded="false">Technologies</button><ul class="technologies " role="menu" aria-labelledby="technologies-button"><li role="none"><a tabindex="-1" href="/en-US/docs/Web" role="menuitem">Technologies Overview</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/HTML" role="menuitem">HTML</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/CSS" role="menuitem">CSS</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/JavaScript" role="menuitem">JavaScript</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/Guide/Graphics" role="menuitem">Graphics</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/HTTP" role="menuitem">HTTP</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/API" role="menuitem">APIs</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Mozilla/Add-ons/WebExtensions" role="menuitem">Browser Extensions</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/MathML" role="menuitem">MathML</a></li></ul></li><li class="top-level-entry-container"><button id="references-guides-button" type="button" class="top-level-entry" aria-haspopup="menu" aria-expanded="false">References &amp; Guides</button><ul class="references-guides " role="menu" aria-labelledby="references-guides-button"><li role="none"><a tabindex="-1" href="/en-US/docs/Learn" role="menuitem">Learn web development</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/Tutorials" role="menuitem">Tutorials</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/Reference" role="menuitem">References</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/Guide" role="menuitem">Developer Guides</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web/Accessibility" role="menuitem">Accessibility</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Games" role="menuitem">Game development</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/Web" role="menuitem">...more docs</a></li></ul></li><li class="top-level-entry-container"><button id="feedback-button" type="button" class="top-level-entry" aria-haspopup="menu" aria-expanded="false">Feedback</button><ul class="feedback " role="menu" aria-labelledby="feedback-button"><li role="none"><a tabindex="-1" href="/en-US/docs/MDN/Contribute/Feedback" role="menuitem">Send Feedback</a></li><li role="none"><a tabindex="-1" href="/en-US/docs/MDN/Contribute" role="menuitem">Contribute to MDN</a></li><li role="none"><a tabindex="-1" target="_blank" rel="noopener noreferrer" href="https://github.com/mdn/content/issues/new" role="menuitem">Report a content issue<!-- --> üåê</a></li><li role="none"><a tabindex="-1" target="_blank" rel="noopener noreferrer" href="https://github.com/mdn/yari/issues/new" role="menuitem">Report a platform issue<!-- --> üåê</a></li></ul></li></ul></nav><div class="header-search"><form action="/en-US/search" class="search-form search-widget" role="search"><label for="main-q" class="visually-hidden">Search MDN</label><input type="search" name="q" id="main-q" class="search-input-field" placeholder="Site search... (Press &quot;/&quot; to focus)" pattern="(.|\s)*\S(.|\s)*" required="" value=""><input type="submit" class="ghost search-button" value="" aria-label="Search"></form></div><div class="auth-container"></div></div></header><div class="breadcrumb-locale-container"><nav class="breadcrumbs-container" aria-label="Breadcrumb navigation"><ol typeof="BreadcrumbList" vocab="https://schema.org/" aria-label="breadcrumbs"><li property="itemListElement" typeof="ListItem"><a class="breadcrumb" property="item" typeof="WebPage" href="/en-US/docs/Web"><span property="name">Web technology for developers</span></a><meta property="position" content="1"></li><li property="itemListElement" typeof="ListItem"><a class="breadcrumb" property="item" typeof="WebPage" href="/en-US/docs/Web/API"><span property="name">Web APIs</span></a><meta property="position" content="2"></li><li property="itemListElement" typeof="ListItem"><a class="breadcrumb-penultimate" property="item" typeof="WebPage" href="/en-US/docs/Web/API/Web_Audio_API"><span property="name">Web Audio API</span></a><meta property="position" content="3"></li><li property="itemListElement" typeof="ListItem"><a class="breadcrumb-current-page" property="item" typeof="WebPage" href="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API"><span property="name">Basic concepts behind Web Audio API</span></a><meta property="position" content="4"></li></ol></nav><ul class="language-toggle single-option"><li><a href="#select-language" class="language-icon"><span class="show-desktop">Change language</span></a></li></ul></div><aside class="document-toc-container"><section class="document-toc"><header><h2>Table of contents</h2><button type="button" class="ghost toc-trigger-mobile" aria-controls="toc-entries" aria-expanded="false">Table of contents</button></header><ul id="toc-entries"><li><a href="#audio_graphs">Audio graphs</a></li><li><a href="#audio_data_whats_in_a_sample">Audio data: what's in a sample</a></li><li><a href="#audio_buffers_frames_samples_and_channels">Audio buffers: frames, samples, and channels</a></li><li><a href="#audio_channels">Audio channels</a></li><li><a href="#visualizations">Visualizations</a></li><li><a href="#spatializations">Spatializations</a></li><li><a href="#fan-in_and_fan-out">Fan-in and Fan-out</a></li></ul></section></aside><main id="content" class="main-content" role="main"><article class="main-page-content" lang="en-US"><h1>Basic concepts behind Web Audio API</h1><div><p>This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does.</p></div><h2 id="audio_graphs"><a href="#audio_graphs" title="Permalink to Audio graphs">Audio graphs</a></h2><div><p>The <a href="/en-US/docs/Web/API/Web_Audio_API">Web Audio API</a> involves handling audio operations inside an <a href="/en-US/docs/Web/API/AudioContext">audio context</a>, and has been designed to allow <em>modular routing</em>. Each <a href="/en-US/docs/Web/API/AudioNode">audio node</a> performs a basic audio operation and is linked with one more other audio nodes to form an <a href="/en-US/docs/Web/API/AudioNode#the_audio_routing_graph">audio routing graph</a>. Several sources with different channel layouts are supported, even within a single context. This modular design provides the flexibility to create complex audio functions with dynamic effects.</p>
<p>Audio nodes are linked via their inputs and outputs, forming a chain that starts with one or more sources, goes through one or more nodes, then ends up at a destination (although you don't have to provide a destination if you only want to visualize some audio data). A simple, typical workflow for web audio would look something like this:</p>
<ol>
  <li>Create the audio context.</li>
  <li>Create audio sources inside the context (such as <a href="/en-US/docs/Web/HTML/Element/audio"><code>&lt;audio&gt;</code></a>, an oscillator, or stream).</li>
  <li>Create audio effects (such as the reverb, biquad filter, panner, or compressor nodes).</li>
  <li>Choose the final destination for the audio (such as the user's computer speakers).</li>
  <li>Connect the source nodes to zero or more effect nodes and then to the chosen destination.</li>
</ol>
<div class="notecard note" id="sect1">
  <p><strong>Note:</strong> The <a href="https://en.wikipedia.org/wiki/Surround_sound#Channel_notation" class="external" rel=" noopener">channel notation</a> is a numeric value, such as <em>2.0</em> or <em>5.1</em>, representing the number of audio channels available on a signal. The first number is the number of full frequency range audio channels the signal includes. The number appearing after the period indicates the number of those channels reserved for low-frequency effect (LFE) outputs; these are often called <strong>subwoofers</strong>.</p>
</div>
<p>
  <img src="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudioapi_en.svg" alt="A simple box diagram with an outer box labeled Audio context and three inner boxes labeled Sources, Effects, and Destination. The three inner boxes have arrows between them pointing from left to right, indicating the flow of audio information." width="643" height="143" loading="lazy">
</p>
<p>Each input or output is composed of one or more audio <strong>channels</strong>, which together represent a specific audio layout. Any discrete channel structure is supported, including <em>mono</em>, <em>stereo</em>, <em>quad</em>, <em>5.1</em>, and so on.</p>
<p>
  <img src="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/mdn.png" alt="Show the ability of audio nodes to connect via their inputs and outputs and the channels inside these inputs/outputs." width="1308" height="750" loading="lazy">
</p>
<p>You have several ways to obtain audio:</p>
<ul>
  <li>Sound can be generated directly in JavaScript by an audio node (such as an oscillator).</li>
  <li>It can be created from raw <a href="https://en.wikipedia.org/wiki/Pulse-code_modulation" class="external" rel=" noopener">PCM</a> data (such as .WAV files or other formats supported by <a href="/en-US/docs/Web/API/BaseAudioContext/decodeAudioData" title="decodeAudioData()"><code>decodeAudioData()</code></a>).</li>
  <li>It can be generated from HTML media elements, such as <a href="/en-US/docs/Web/HTML/Element/video"><code>&lt;video&gt;</code></a> or <a href="/en-US/docs/Web/HTML/Element/audio"><code>&lt;audio&gt;</code></a>.</li>
  <li>It can be obtained from a <a href="/en-US/docs/Web/API/WebRTC_API">WebRTC</a> <a href="/en-US/docs/Web/API/MediaStream"><code>MediaStream</code></a>, such as a webcam or microphone.</li>
</ul></div><h2 id="audio_data_whats_in_a_sample"><a href="#audio_data_whats_in_a_sample" title="Permalink to Audio data: what's in a sample">Audio data: what's in a sample</a></h2><div><p>When an audio signal is processed, sampling happens. <strong>Sampling</strong> is the conversion of a <a href="https://en.wikipedia.org/wiki/Continuous_signal" class="external" rel=" noopener">continuous signal</a> to a <a href="https://en.wikipedia.org/wiki/Discrete_signal" class="external" rel=" noopener">discrete signal</a>. Put another way, a continuous sound wave, such as a band playing live, is converted into a sequence of digital samples (a discrete-time signal) that allows a computer to handle the audio in distinct blocks.</p>
<p>You'll find more information on the Wikipedia page <a href="https://en.wikipedia.org/wiki/Sampling_%28signal_processing%29" class="external" rel=" noopener"><em>Sampling (signal processing)</em></a>.</p></div><h2 id="audio_buffers_frames_samples_and_channels"><a href="#audio_buffers_frames_samples_and_channels" title="Permalink to Audio buffers: frames, samples, and channels">Audio buffers: frames, samples, and channels</a></h2><div><p>An <a href="/en-US/docs/Web/API/AudioBuffer"><code>AudioBuffer</code></a> is defined with three parameters:</p>
<ul>
  <li>the number of channels (1 for mono, 2 for stereo, etc.),</li>
  <li>its length, meaning the number of sample frames inside the buffer,</li>
  <li>and the sample rate, the number of sample frames played per second.</li>
</ul>
<p>A <em>sample</em> is a single 32-bit floating point value representing the value of the audio stream at each specific moment in time within a particular channel (left or right, if in the case of stereo). A <em>frame</em>, or <em>sample frame</em>, is the set of all values for all channels that will play at a specific moment in time: all the samples of all the channels that play at the same time (two for a stereo sound, six for 5.1, etc.).</p>
<p>The <em>sample rate</em> is the quantity of those samples (or frames, since all samples of a frame play at the same time) that will play in one second, measured in Hz. The higher the sample rate, the better the sound quality.</p>
<p>Let's look at a <em>mono</em> and a <em>stereo</em> audio buffer, each one second long at a rate of 44100Hz:</p>
<ul>
  <li>The <em>mono</em> buffer will have 44,100 samples and 44,100 frames. The <code>length</code> property will be 44,100.</li>
  <li>The <em>stereo</em> buffer will have 88,200 samples but still 44,100 frames. The <code>length</code> property will still be 44100 since it equals the number of frames.</li>
</ul>
<p>
  <img src="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/sampleframe-english.png" alt="A diagram showing several frames in an audio buffer in a long line, each one containing two samples, as the buffer has two channels, it is stereo." width="1245" height="219" loading="lazy">
</p>
<p>When a buffer plays, you will first hear the leftmost sample frame, then the one right next to it, then the next, <em>and so on</em>, until the end of the buffer. In the case of stereo, you will hear both channels simultaneously. Sample frames are handy because they are independent of the number of channels and represent time in an ideal way for precise audio manipulation.</p>
<div class="notecard note" id="sect2">
  <p><strong>Note:</strong> To get a time in seconds from a frame count, divide the number of frames by the sample rate. To get the number of frames from the number of samples, you only need to divide the latter value by the channel count.</p>
</div>
<p>Here are a couple of simple examples:</p>
<div class="code-example"><pre class="brush: js notranslate"><code><span class="token keyword">const</span> context <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AudioContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> buffer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AudioBuffer</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> <span class="token punctuation">{</span>
  <span class="token literal-property property">numberOfChannels</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
  <span class="token literal-property property">length</span><span class="token operator">:</span> <span class="token number">22050</span><span class="token punctuation">,</span>
  <span class="token literal-property property">sampleRate</span><span class="token operator">:</span> <span class="token number">44100</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div>
<div class="notecard note" id="sect3">
  <p><strong>Note:</strong> In <a href="https://en.wikipedia.org/wiki/Digital_audio" class="external" rel=" noopener">digital audio</a>, <strong>44,100 <a href="https://en.wikipedia.org/wiki/Hertz" class="external" rel=" noopener">Hz</a></strong> (alternately represented as <strong>44.1 kHz</strong>) is a common <a href="https://en.wikipedia.org/wiki/Sampling_frequency" class="external" rel=" noopener">sampling frequency</a>. Why 44.1 kHz?</p>
  <p>Firstly, because the <a href="https://en.wikipedia.org/wiki/Hearing_range" class="external" rel=" noopener">hearing range</a> of human ears is roughly 20 Hz to 20,000 Hz. Via the <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" class="external" rel=" noopener">Nyquist‚ÄìShannon sampling theorem</a>, the sampling frequency must be greater than twice the maximum frequency one wishes to reproduce. Therefore, the sampling rate has to be <em>greater</em> than 40,000 Hz.</p>
  <p>Secondly, signals must be <a href="https://en.wikipedia.org/wiki/Low-pass_filter" class="external" rel=" noopener">low-pass filtered</a> before sampling, otherwise <a href="https://en.wikipedia.org/wiki/Aliasing" class="external" rel=" noopener">aliasing</a> occurs. While an ideal low-pass filter would perfectly pass frequencies below 20 kHz (without attenuating them) and perfectly cut off frequencies above 20 kHz, in practice, a <a href="https://en.wikipedia.org/wiki/Transition_band" class="external" rel=" noopener">transition band</a> is necessary, where frequencies are partly attenuated. The wider this transition band is, the easier and more economical it is to make an <a href="https://en.wikipedia.org/wiki/Anti-aliasing_filter" class="external" rel=" noopener">anti-aliasing filter</a>. The 44.1 kHz sampling frequency allows for a 2.05 kHz transition band.</p>
</div>
<p>If you use this call above, you will get a stereo buffer with two channels that, when played back on an <a href="/en-US/docs/Web/API/AudioContext"><code>AudioContext</code></a> running at 44100 Hz (very common, most normal sound cards run at this rate), will last for 0.5 seconds: 22,050 frames/44,100 Hz = 0.5 seconds.</p>
<div class="code-example"><pre class="brush: js notranslate"><code><span class="token keyword">const</span> context <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AudioContext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> buffer <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AudioBuffer</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> <span class="token punctuation">{</span>
  <span class="token literal-property property">numberOfChannels</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
  <span class="token literal-property property">length</span><span class="token operator">:</span> <span class="token number">22050</span><span class="token punctuation">,</span>
  <span class="token literal-property property">sampleRate</span><span class="token operator">:</span> <span class="token number">22050</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre></div>
<p>If you use this call, you will get a mono buffer (single-channel buffer) that, when played back on an <a href="/en-US/docs/Web/API/AudioContext"><code>AudioContext</code></a> running at 44,100 Hz, will be automatically <em>resampled</em> to 44,100 Hz (and therefore yield 44,100 frames), and last for 1.0 second: 44,100 frames/44,100 Hz = 1 second.</p>
<div class="notecard note" id="sect4">
  <p><strong>Note:</strong> Audio resampling is very similar to image resizing. Say you've got a 16 x 16 image but want it to fill a 32 x 32 area. You resize (or resample) it. The result has less quality (it can be blurry or edgy, depending on the resizing algorithm), but it works, with the resized image taking up less space. Resampled audio is the same: you save space, but, in practice, you cannot correctly reproduce high-frequency content or treble sound.</p>
</div></div><h3 id="planar_versus_interleaved_buffers"><a href="#planar_versus_interleaved_buffers" title="Permalink to Planar versus interleaved buffers">Planar versus interleaved buffers</a></h3><div><p>The Web Audio API uses a planar buffer format. The left and right channels are stored like this:</p>
<pre class="notranslate">LLLLLLLLLLLLLLLLRRRRRRRRRRRRRRRR (for a buffer of 16 frames)
</pre>
<p>This structure is widespread in audio processing, making it easy to process each channel independently.</p>
<p>The alternative is to use an interleaved buffer format:</p>
<pre class="notranslate">LRLRLRLRLRLRLRLRLRLRLRLRLRLRLRLR (for a buffer of 16 frames)
</pre>
<p>This format is prevalent for storing and playing back audio without much processing, for example: .WAV files or a decoded MP3 stream.</p>
<p>Because the Web Audio API is designed for processing, it exposes <em>only</em> planar buffers. It uses the planar format but converts the audio to the interleaved format when it sends it to the sound card for playback. Conversely, when the API decodes an MP3, it starts with the interleaved format and converts it to the planar format for processing.</p></div><h2 id="audio_channels"><a href="#audio_channels" title="Permalink to Audio channels">Audio channels</a></h2><div><p>Each audio buffer may contain different numbers of channels. Most modern audio devices use the basic <em>mono</em> (only one channel) and <em>stereo</em> (left and right channels) settings. Some more complex sets support <em>surround sound</em> settings (like <em>quad</em> and <em>5.1</em>), which can lead to a richer sound experience thanks to their high channel count. We usually represent the channels with the standard abbreviations detailed in the table below:</p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Channels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Mono</em></td>
      <td><code>0: M: mono</code></td>
    </tr>
    <tr>
      <td><em>Stereo</em></td>
      <td><code>0: L: left 1: R: right</code></td>
    </tr>
    <tr>
      <td><em>Quad</em></td>
      <td><code>0: L: left 1: R: right 2: SL: surround left 3: SR: surround right</code></td>
    </tr>
    <tr>
      <td><em>5.1</em></td>
      <td><code>0: L: left 1: R: right 2: C: center 3: LFE: subwoofer 4: SL: surround left 5: SR: surround right</code></td>
    </tr>
  </tbody>
</table></div><h3 id="up-mixing_and_down-mixing"><a href="#up-mixing_and_down-mixing" title="Permalink to Up-mixing and down-mixing">Up-mixing and down-mixing</a></h3><div><p>When the numbers of channels of the input and the output don't match, up-mixing, or down-mixing, must be done. The following rules, controlled by setting the <a href="/en-US/docs/Web/API/AudioNode/channelInterpretation"><code>AudioNode.channelInterpretation</code></a> property to <code>speakers</code> or <code>discrete</code>, apply:</p>
<table class="standard-table">
  <thead>
    <tr>
      <th scope="row">Interpretation</th>
      <th scope="col">Input channels</th>
      <th scope="col">Output channels</th>
      <th scope="col">Mixing rules</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="13" scope="row"><code>speakers</code></th>
      <td><code>1</code> <em>(Mono)</em></td>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td>
        <em>Up-mix from mono to stereo</em>.<br>The <code>M</code> input
        channel is used for both output channels (<code>L</code> and
        <code>R</code>).<br><code>output.L = input.M<br>output.R = input.M</code>
      </td>
    </tr>
    <tr>
      <td><code>1</code> <em>(Mono)</em></td>
      <td><code>4</code> <em>(Quad)</em></td>
      <td>
        <em>Up-mix from mono to quad.</em><br>The <code>M</code> input channel
        is used for non-surround output channels (<code>L</code> and
        <code>R</code>). Surround output channels (<code>SL</code> and
        <code>SR</code>) are silent.<br><code>output.L = input.M<br>output.R = input.M<br>output.SL = 0<br>output.SR
= 0</code>
      </td>
    </tr>
    <tr>
      <td><code>1</code> <em>(Mono)</em></td>
      <td><code>6</code> <em>(5.1)</em></td>
      <td>
        <em>Up-mix from mono to 5.1.</em><br>The <code>M</code> input channel
        is used for the center output channel (<code>C</code>). All the others
        (<code>L</code>, <code>R</code>, <code>LFE</code>, <code>SL</code>, and
        <code>SR</code>) are silent.<br><code>output.L = 0<br>output.R = 0</code><br><code>output.C = input.M<br>output.LFE = 0<br>output.SL = 0<br>output.SR
= 0</code>
      </td>
    </tr>
    <tr>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td><code>1</code> <em>(Mono)</em></td>
      <td>
        <em>Down-mix from stereo to mono</em>.<br>Both input channels (<code>L</code>
        and <code>R</code>) are equally combined to produce the unique output
        channel (<code>M</code>).<br><code>output.M = 0.5 * (input.L + input.R)</code>
      </td>
    </tr>
    <tr>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td><code>4</code> <em>(Quad)</em></td>
      <td>
        <em>Up-mix from stereo to quad.</em><br>The <code>L</code> and
        <code>R </code>input channels are used for their non-surround respective
        output channels (<code>L</code> and <code>R</code>). Surround output
        channels (<code>SL</code> and <code>SR</code>) are silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.SL = 0<br>output.SR
= 0</code>
      </td>
    </tr>
    <tr>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td><code>6</code> <em>(5.1)</em></td>
      <td>
        <em>Up-mix from stereo to 5.1.</em><br>The <code>L</code> and
        <code>R </code>input channels are used for their non-surround respective
        output channels (<code>L</code> and <code>R</code>). Surround output
        channels (<code>SL</code> and <code>SR</code>), as well as the center
        (<code>C</code>) and subwoofer (<code>LFE</code>) channels, are left
        silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE
= 0<br>output.SL = 0<br>output.SR = 0</code>
      </td>
    </tr>
    <tr>
      <td><code>4</code> <em>(Quad)</em></td>
      <td><code>1</code> <em>(Mono)</em></td>
      <td>
        <em>Down-mix from quad to mono</em>.<br>All four input channels
        (<code>L</code>, <code>R</code>, <code>SL</code>, and <code>SR</code>)
        are equally combined to produce the unique output channel
        (<code>M</code>).<br><code>output.M = 0.25 * (input.L + input.R + </code><code>input.SL + input.SR</code><code>)</code>
      </td>
    </tr>
    <tr>
      <td><code>4</code> <em>(Quad)</em></td>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td>
        <em>Down-mix from quad to stereo</em>.<br>Both left input channels
        (<code>L</code> and <code>SL</code>) are equally combined to produce the
        unique left output channel (<code>L</code>). And similarly, both right
        input channels (<code>R</code> and <code>SR</code>) are equally combined
        to produce the unique right output channel (<code>R</code>).<br><code>output.L = 0.5 * (input.L + input.SL</code><code>)</code><br><code>output.R = 0.5 * (input.R + input.SR</code><code>)</code>
      </td>
    </tr>
    <tr>
      <td><code>4</code> <em>(Quad)</em></td>
      <td><code>6</code> <em>(5.1)</em></td>
      <td>
        <em>Up-mix from quad to 5.1.</em><br>The <code>L</code>,
        <code>R</code>, <code>SL</code>, and <code>SR</code> input channels are
        used for their respective output channels (<code>L</code> and
        <code>R</code>). Center (<code>C</code>) and subwoofer
        (<code>LFE</code>) channels are left silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE
= 0<br>output.SL = input.SL<br>output.SR = input.SR</code>
      </td>
    </tr>
    <tr>
      <td><code>6</code> <em>(5.1)</em></td>
      <td><code>1</code> <em>(Mono)</em></td>
      <td>
        <em>Down-mix from 5.1 to mono.</em><br>The left (<code>L</code> and
        <code>SL</code>), right (<code>R</code> and <code>SR</code>) and central
        channels are all mixed together. The surround channels are slightly
        attenuated, and the regular lateral channels are power-compensated to
        make them count as a single channel by multiplying by <code>‚àö2/2</code>.
        The subwoofer (<code>LFE</code>) channel is lost.<br><code>output.M = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL +
input.SR)</code>
      </td>
    </tr>
    <tr>
      <td><code>6</code> <em>(5.1)</em></td>
      <td><code>2</code> <em>(Stereo)</em></td>
      <td>
        <em>Down-mix from 5.1 to stereo.</em><br>The central channel
        (<code>C</code>) is summed with each lateral surround channel (<code>SL</code>
        or <code>SR</code>) and mixed to each lateral channel. As it is mixed
        down to two channels, it is mixed at a lower power: in each case, it is
        multiplied by <code>‚àö2/2</code>. The subwoofer (<code>LFE</code>)
        channel is lost.<br><code>output.L = input.L + 0.7071 * (input.C + input.SL)<br>output.R =
input.R </code><code>+ 0.7071 * (input.C + input.SR)</code>
      </td>
    </tr>
    <tr>
      <td><code>6</code> <em>(5.1)</em></td>
      <td><code>4</code> <em>(Quad)</em></td>
      <td>
        <em>Down-mix from 5.1 to quad.</em><br>The central (<code>C</code>) is
        mixed with the lateral non-surround channels (<code>L</code> and
        <code>R</code>). As it is mixed down to two channels, it is mixed at a
        lower power: in each case, it is multiplied by <code>‚àö2/2</code>. The
        surround channels are passed unchanged. The subwoofer (<code>LFE</code>)
        channel is lost.<br><code>output.L = input.L + 0.7071 * input.C<br>output.R = input.R +
0.7071 * input.C<br>output.SL = input.SL<br>output.SR =
input.SR</code>
      </td>
    </tr>
    <tr>
      <td colspan="2">Other, non-standard layouts</td>
      <td>
        Non-standard channel layouts behave as if
        <code>channelInterpretation</code> is set to
        <code>discrete</code>.<br>The specification explicitly allows the future definition of new speaker layouts. Therefore, this fallback is not future-proof as the behavior of the browsers for a specific number of channels may change in the future.
      </td>
    </tr>
    <tr>
      <th rowspan="2" scope="row"><code>discrete</code></th>
      <td>any (<code>x</code>)</td>
      <td>any (<code>y</code>) where <code>x&lt;y</code></td>
      <td>
        <em>Up-mix discrete channels.</em><br>Fill each output channel with
        its input counterpart ‚Äî that is, the input channel with the same index.
        Channels with no corresponding input channels are left silent.
      </td>
    </tr>
    <tr>
      <td>any (<code>x</code>)</td>
      <td>any (<code>y</code>) where <code>x&gt;y</code></td>
      <td>
        <em>Down-mix discrete channels.</em><br>Fill each output channel with
        its input counterpart ‚Äî that is, the input channel with the same index.
        Input channels with no corresponding output channels are dropped.
      </td>
    </tr>
  </tbody>
</table></div><h2 id="visualizations"><a href="#visualizations" title="Permalink to Visualizations">Visualizations</a></h2><div><p>In general, we get the output over time to produce audio visualizations, usually reading its gain or frequency data. Then, using a graphical tool, we turn the obtained data into a visual representation, such as a graph. The Web Audio API has an <a href="/en-US/docs/Web/API/AnalyserNode"><code>AnalyserNode</code></a> available that doesn't alter the audio signal passing through it. Additionally, it outputs the audio data, allowing us to process it via a technology such as <a href="/en-US/docs/Web/HTML/Element/canvas"><code>&lt;canvas&gt;</code></a>.</p>
<p>
  <img src="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fttaudiodata_en.svg" alt="Without modifying the audio stream, the node allows to get the frequency and time-domain data associated with it, using an FFT." width="693" height="206" loading="lazy">
</p>
<p>You can grab data using the following methods:</p>
<dl>
  <dt id="analysernode.getfloatfrequencydata"><a href="/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData"><code>AnalyserNode.getFloatFrequencyData()</code></a></dt>
  <dd>
    <p>Copies the current frequency data into a <a href="/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array"><code>Float32Array</code></a> array passed into it.</p>
  </dd>
  <dt id="analysernode.getbytefrequencydata"><a href="/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData"><code>AnalyserNode.getByteFrequencyData()</code></a></dt>
  <dd>
    <p>Copies the current frequency data into a <a href="/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>
  </dd>
  <dt id="analysernode.getfloattimedomaindata"><a href="/en-US/docs/Web/API/AnalyserNode/getFloatTimeDomainData"><code>AnalyserNode.getFloatTimeDomainData()</code></a></dt>
  <dd>
    <p>Copies the current waveform, or time-domain, data into a <a href="/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array"><code>Float32Array</code></a> array passed into it.</p>
  </dd>
  <dt id="analysernode.getbytetimedomaindata"><a href="/en-US/docs/Web/API/AnalyserNode/getByteTimeDomainData"><code>AnalyserNode.getByteTimeDomainData()</code></a></dt>
  <dd>
    <p>Copies the current waveform, or time-domain, data into a <a href="/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>
  </dd>
</dl>
<div class="notecard note" id="sect5">
  <p><strong>Note:</strong> For more information, see our <a href="/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API">Visualizations with Web Audio API</a> article.</p>
</div></div><h2 id="spatializations"><a href="#spatializations" title="Permalink to Spatializations">Spatializations</a></h2><div><p>Audio spatialization allows us to model the position and behavior of an audio signal at a certain point in physical space, simulating the listener hearing that audio. In the Web Audio API, spatialization is handled by the <a href="/en-US/docs/Web/API/PannerNode"><code>PannerNode</code></a> and the <a href="/en-US/docs/Web/API/AudioListener"><code>AudioListener</code></a>.</p>
<p>The panner uses right-hand Cartesian coordinates to describe the audio source's <em>position</em> as a vector and its <em>orientation</em> as a 3D directional cone. The cone can be pretty large, for example, for omnidirectional sources.</p>
<p>
  <img src="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/pannernode_en.svg" alt="The PannerNode defines a spatial position and orientation for a given signal." width="799" height="340" loading="lazy">
</p>
<p>Similarly, the Web Audio API describes the listener using right-hand Cartesian coordinates: their <em>position</em> as one vector and their <em>orientation</em> as two direction vectors, <em>up</em> and <em>front</em>. These vectors define the direction of the top of the listener's head and the direction the listener's nose is pointing. The vectors are perpendicular to one another.</p>
<p>
  <img src="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudiolistenerreduced.png" alt="We see the position, up, and front vectors of an AudioListener, with the up and front vectors at 90¬∞ from the other." width="634" height="250" loading="lazy">
</p>
<div class="notecard note" id="sect6">
  <p><strong>Note:</strong> For more information, see our <a href="/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics">Web audio spatialization basics</a> article.</p>
</div></div><h2 id="fan-in_and_fan-out"><a href="#fan-in_and_fan-out" title="Permalink to Fan-in and Fan-out">Fan-in and Fan-out</a></h2><div><p>In audio terms, <strong>fan-in</strong> describes the process by which a <a href="/en-US/docs/Web/API/ChannelMergerNode"><code>ChannelMergerNode</code></a> takes a series of <em>mono</em> input sources and outputs a single multi-channel signal:</p>
<p>
  <img src="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanin.svg" alt="" width="325" height="258" loading="lazy">
</p>
<p><strong>Fan-out</strong> describes the opposite process, whereby a <a href="/en-US/docs/Web/API/ChannelSplitterNode"><code>ChannelSplitterNode</code></a> takes a multi-channel input source and outputs multiple <em>mono</em> output signals:</p>
<p>
  <img src="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanout.svg" alt="" width="325" height="258" loading="lazy">
</p></div></article><aside class="metadata"><div class="metadata-content-container"><div id="on-github" class="on-github"><h3>Found a problem with this page?</h3><ul><li><a href="https://github.com/mdn/content/edit/style/old/files/en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md" title="You're going to need to sign in to GitHub first (Opens in a new tab)" target="_blank" rel="noopener noreferrer">Edit on <b>GitHub</b></a></li><li><a href="https://github.com/mdn/content/blob/style/old/files/en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md" title="Folder: en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api (Opens in a new tab)" target="_blank" rel="noopener noreferrer">Source on <b>GitHub</b></a></li><li><a href="https://github.com/mdn/content/issues/new?body=MDN+URL%3A+https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FWeb_Audio_API%2FBasic_concepts_behind_Web_Audio_API%0A%0A%23%23%23%23+What+information+was+incorrect%2C+unhelpful%2C+or+incomplete%3F%0A%0A%0A%23%23%23%23+Specific+section+or+headline%3F%0A%0A%0A%23%23%23%23+What+did+you+expect+to+see%3F%0A%0A%0A%23%23%23%23+Did+you+test+this%3F+If+so%2C+how%3F%0A%0A%0A%3C%21--+Do+not+make+changes+below+this+line+--%3E%0A%3Cdetails%3E%0A%3Csummary%3EMDN+Content+page+report+details%3C%2Fsummary%3E%0A%0A*+Folder%3A+%60en-us%2Fweb%2Fapi%2Fweb_audio_api%2Fbasic_concepts_behind_web_audio_api%60%0A*+MDN+URL%3A+https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FWeb_Audio_API%2FBasic_concepts_behind_Web_Audio_API%0A*+GitHub+URL%3A+https%3A%2F%2Fgithub.com%2Fmdn%2Fcontent%2Fblob%2Fstyle%2Fold%2Ffiles%2Fen-us%2Fweb%2Fapi%2Fweb_audio_api%2Fbasic_concepts_behind_web_audio_api%2Findex.md%0A*+Last+commit%3A+https%3A%2F%2Fgithub.com%2Fmdn%2Fcontent%2Fcommit%2F98913882aa1e4648b84b4e008d50fe50960eb10c%0A*+Document+last+modified%3A+2022-08-15T04%3A42%3A34.000Z%0A%0A%3C%2Fdetails%3E&amp;title=Issue+with+%22Basic+concepts+behind+Web+Audio+API%22%3A+%28short+summary+here+please%29&amp;labels=needs-triage%2CContent%3AWebAPI" title="This will take you to https://github.com/mdn/content to file a new issue" target="_blank" rel="noopener noreferrer">Report a problem with this content on <b>GitHub</b></a></li><li>Want to fix the problem yourself? See<!-- --> <a href="https://github.com/mdn/content/blob/main/README.md" target="_blank" rel="noopener noreferrer">our Contribution guide</a>.</li></ul></div><p class="last-modified-date"><b>Last modified:</b> <time datetime="2022-08-15T04:42:34.000Z">Aug 15, 2022</time>,<!-- --> <a href="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/contributors.txt">by MDN contributors</a></p><form class="language-menu"><fieldset id="select-language"><legend>Change your language</legend><label for="language-selector" class="visually-hidden">Select your preferred language</label> <select id="language-selector" name="language"><option selected="" value="en-US">English (US)</option><option value="fr">Fran√ßais</option><option value="ja">Êó•Êú¨Ë™û</option><option value="ko">ÌïúÍµ≠Ïñ¥</option><option value="zh-CN">‰∏≠Êñá (ÁÆÄ‰Ωì)</option></select> <button type="submit" class="button minimal">Change language</button></fieldset></form></div></aside></main><nav id="sidebar-quicklinks" class="sidebar"><h4>Related Topics</h4><div><ol><li><strong><a href="/en-US/docs/Web/API/Web_Audio_API">Web Audio API</a></strong></li><li class="toggle"><details open=""><summary>Guides</summary><ol><li><a href="/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API">Using the Web Audio API</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API">Basic concepts behind Web Audio API</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Best_practices">Web Audio API best practices</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Advanced_techniques">Advanced techniques: Creating and sequencing audio</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Using_AudioWorklet">Background audio processing using AudioWorklet</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Controlling_multiple_parameters_with_ConstantSourceNode">Controlling multiple parameters with ConstantSourceNode</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Migrating_from_webkitAudioContext">Migrating from webkitAudioContext</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Simple_synth">Example and tutorial: Simple synth keyboard</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Tools">Tools for analyzing Web Audio usage</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Using_IIR_filters">Using IIR filters</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API">Visualizations with Web Audio API</a></li><li><a href="/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics">Web audio spatialization basics</a></li></ol></details></li><li class="toggle"><details open=""><summary>Interfaces</summary><ol><li><a href="/en-US/docs/Web/API/AnalyserNode"><code>AnalyserNode</code></a></li><li><a href="/en-US/docs/Web/API/AudioBuffer"><code>AudioBuffer</code></a></li><li><a href="/en-US/docs/Web/API/AudioBufferSourceNode"><code>AudioBufferSourceNode</code></a></li><li><a href="/en-US/docs/Web/API/AudioContext"><code>AudioContext</code></a></li><li><a href="/en-US/docs/Web/API/AudioDestinationNode"><code>AudioDestinationNode</code></a></li><li><a href="/en-US/docs/Web/API/AudioListener"><code>AudioListener</code></a></li><li><a href="/en-US/docs/Web/API/AudioNode"><code>AudioNode</code></a></li><li><a href="/en-US/docs/Web/API/AudioParam"><code>AudioParam</code></a></li><li><a href="/en-US/docs/Web/API/AudioProcessingEvent"><code>AudioProcessingEvent</code></a></li><li><a href="/en-US/docs/Web/API/AudioScheduledSourceNode"><code>AudioScheduledSourceNode</code></a></li><li><a href="/en-US/docs/Web/API/AudioWorklet"><code>AudioWorklet</code></a></li><li><a href="/en-US/docs/Web/API/AudioWorkletGlobalScope"><code>AudioWorkletGlobalScope</code></a></li><li><a href="/en-US/docs/Web/API/AudioWorkletNode"><code>AudioWorkletNode</code></a></li><li><a href="/en-US/docs/Web/API/AudioWorkletProcessor"><code>AudioWorkletProcessor</code></a></li><li><a href="/en-US/docs/Web/API/BaseAudioContext"><code>BaseAudioContext</code></a></li><li><a href="/en-US/docs/Web/API/BiquadFilterNode"><code>BiquadFilterNode</code></a></li><li><a href="/en-US/docs/Web/API/ChannelMergerNode"><code>ChannelMergerNode</code></a></li><li><a href="/en-US/docs/Web/API/ChannelSplitterNode"><code>ChannelSplitterNode</code></a></li><li><a href="/en-US/docs/Web/API/ConstantSourceNode"><code>ConstantSourceNode</code></a></li><li><a href="/en-US/docs/Web/API/ConvolverNode"><code>ConvolverNode</code></a></li><li><a href="/en-US/docs/Web/API/DelayNode"><code>DelayNode</code></a></li><li><a href="/en-US/docs/Web/API/DynamicsCompressorNode"><code>DynamicsCompressorNode</code></a></li><li><a href="/en-US/docs/Web/API/GainNode"><code>GainNode</code></a></li><li><a href="/en-US/docs/Web/API/IIRFilterNode"><code>IIRFilterNode</code></a></li><li><a href="/en-US/docs/Web/API/MediaElementAudioSourceNode"><code>MediaElementAudioSourceNode</code></a></li><li><a href="/en-US/docs/Web/API/MediaStreamAudioDestinationNode"><code>MediaStreamAudioDestinationNode</code></a></li><li><a href="/en-US/docs/Web/API/MediaStreamAudioSourceNode"><code>MediaStreamAudioSourceNode</code></a></li><li><a href="/en-US/docs/Web/API/OfflineAudioCompletionEvent"><code>OfflineAudioCompletionEvent</code></a></li><li><a href="/en-US/docs/Web/API/OfflineAudioContext"><code>OfflineAudioContext</code></a></li><li><a href="/en-US/docs/Web/API/OscillatorNode"><code>OscillatorNode</code></a></li><li><a href="/en-US/docs/Web/API/PannerNode"><code>PannerNode</code></a></li><li><a href="/en-US/docs/Web/API/PeriodicWave"><code>PeriodicWave</code></a></li><li><a href="/en-US/docs/Web/API/WaveShaperNode"><code>WaveShaperNode</code></a></li><li><a href="/en-US/docs/Web/API/StereoPannerNode"><code>StereoPannerNode</code></a></li></ol></details></li></ol></div></nav></div><footer id="nav-footer" class="page-footer"><div class="content-container"><div class="page-footer-logo"><a href="/en-US/" class="logo" aria-label="MDN Web Docs"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 451.74 135" fill="#fff"><path d="M7.14 8.35v111.06h111.05V8.35zm103.71 56c-.48.92-1 1.79-1.46 2.71a3.44 3.44 0 01-3.54 2 2.4 2.4 0 00-1.55.5c-1.37.9-2.76 1.79-4.18 2.63a7.33 7.33 0 01-6.35.34 29.71 29.71 0 00-10.63-2 11.7 11.7 0 00-9.46 4.31 14.84 14.84 0 00-2.13 4.29c-1.24 3.07-2.3 21.38-2.3 26.05 0 0-17.62-3.42-34.15-20.34l4.31-11.32h-13.5l9.76-10.35h-16.8l9.77-10.34H12.69L30.45 34a40.9 40.9 0 0119.77-10.83c7.1-1.22 8.93-.53 13.31.77l2.43.73.85.25 3.1.95a12.56 12.56 0 006.21.09 11.37 11.37 0 018.25 1 8.24 8.24 0 014.1 6.22 7.29 7.29 0 003.61 5.49 59.45 59.45 0 009.32 4.11c2.27.86 4.54 1.84 6.79 2.72a6.81 6.81 0 012.86 2.06 4.81 4.81 0 011.1 2.73c.14 2 .37 4 .47 6a15.24 15.24 0 01-1.77 8.03zM320.12 39.62a5.42 5.42 0 00-4.53 2.13 7.36 7.36 0 00-1.7 4.43v2.36a6.28 6.28 0 001.7 4.46 5.63 5.63 0 004.3 1.82 5.12 5.12 0 004.57-2.27A9.7 9.7 0 00326 47a8.11 8.11 0 00-1.67-5.52 5.36 5.36 0 00-4.21-1.86zM387.38 39.53a5.52 5.52 0 00-4.7 2.15 8.8 8.8 0 00-1.63 5.49 9.23 9.23 0 001.58 5.45 5.38 5.38 0 004.7 2.25 5.61 5.61 0 004.74-2.2 8.91 8.91 0 001.68-5.59 8.24 8.24 0 00-1.75-5.52 5.76 5.76 0 00-4.62-2.03zM299.47 41.35a4.34 4.34 0 00-4-1.92 4.55 4.55 0 00-3.89 1.73 8.37 8.37 0 00-1.58 4.17h10.48a6.3 6.3 0 00-1.01-3.98zM357.74 30.75H352v23.31h5.72q5.47 0 8.35-3t2.93-8.65q0-5.43-2.88-8.55t-8.38-3.11z"></path><path d="M121.55 8.35v70.8h323V8.35zm42.21 22.45h-4V54h3.68v3.73h-11.25V54h3.31V36.79h-.19l-9.63 19.12h-2.12l-10-19.4h-.19V54h3.45v3.73h-11.15V54h3.68V30.8h-4v-3.73H133l11.66 22.56h.19l11.18-22.56h7.7zm29.12 22.67q-4.11 4.28-11.38 4.28h-14.06v-3.69h3.73V30.75h-3.73v-3.68h13.83q7.59 0 11.66 4.29a15.4 15.4 0 014 11 15.33 15.33 0 01-4.05 11.11zm38.89-22.67h-3.68v27h-2.6L208.08 35h-.19v19h4.67v3.73h-12.22V54h3.49V30.8h-4v-3.73h7.08l16.9 22.09h.19V30.8h-4.58v-3.73h12.32zm43.8 27h-3.31l-7.83-23.18h-.19l-7.55 23.18h-3.35l-8.78-27h-2.65v-3.73H253v3.73h-3.87L255 50.71h.23l6.61-19.91H259v-3.73h11v3.73h-2.78l6.61 20.1h.23l5.43-20.1h-4.15v-3.73h11v3.73h-2.54zm26.71-1.51a9.66 9.66 0 01-6.42 2 10.2 10.2 0 01-7.41-2.74c-1.89-1.82-2.83-4.47-2.83-7.93a12.37 12.37 0 012.64-8.12 9 9 0 017.32-3.21 8.62 8.62 0 016.75 2.69 9.65 9.65 0 012.45 6.52 13.67 13.67 0 01-.28 2.69H290q.29 6.71 6.18 6.7a5.2 5.2 0 003.71-1.18 5.82 5.82 0 001.67-2.83l3.45.71a7.21 7.21 0 01-2.73 4.65zm25.77-1.63c-1.51 2.4-3.92 3.61-7.22 3.61s-5.84-1.29-7.22-3.87c0 .25-.1.82-.21 1.7s-.19 1.44-.22 1.7H309c.16-1 .31-2 .47-3.07a21.42 21.42 0 00.24-3.16v-23h-3.4v-3.3h7.55V40.9a9.76 9.76 0 012.67-3.28 7.33 7.33 0 014.74-1.4 8.48 8.48 0 016.5 2.78q2.55 2.74 2.55 7.74a14.6 14.6 0 01-2.27 7.87zm41.39-1.14q-4.11 4.28-11.37 4.28H344v-3.74h3.73V30.75H344v-3.68h13.83q7.59 0 11.66 4.29a15.41 15.41 0 014.06 11 15.34 15.34 0 01-4.11 11.11zm25.65 1.68a10.53 10.53 0 01-7.9 3.07 10 10 0 01-7.63-3 10.93 10.93 0 01-2.8-7.83 12.13 12.13 0 012.69-7.93q2.69-3.3 8-3.3t8 3.28a12 12 0 012.64 7.76 10.86 10.86 0 01-3 7.9zm22.61.57c-1.4 1.66-3.63 2.5-6.68 2.5a9.58 9.58 0 01-7.15-2.76q-2.72-2.76-2.71-7.91a12.25 12.25 0 012.69-8 9.17 9.17 0 017.5-3.28 15 15 0 013.82.48 10.37 10.37 0 013.5 1.65l.85 5.47-3.35.38-.76-3.54a8.07 8.07 0 00-4.11-1 4.9 4.9 0 00-4.39 2.15 9.93 9.93 0 00-1.41 5.55 8.9 8.9 0 001.5 5.38 5.23 5.23 0 004.44 2c2.92 0 4.67-1.7 5.23-5.1l3.5.71a10.34 10.34 0 01-2.47 5.27zm20.48.75a11.68 11.68 0 01-6.63 1.75 15.52 15.52 0 01-8.26-2.08L424 51l3.26.33-.1 2.74a7 7 0 002.06.66 12.63 12.63 0 002.19.19 8.68 8.68 0 003.66-.75 2.5 2.5 0 001.63-2.36 2.25 2.25 0 00-1.32-2.2 12.65 12.65 0 00-3.28-1 47.39 47.39 0 01-3.9-.82 7.5 7.5 0 01-3.25-1.7 4.67 4.67 0 01-1.33-3.66c0-2.36.88-4 2.62-4.91a12 12 0 015.6-1.37 15 15 0 014.08.55 16.65 16.65 0 013.47 1.39l.47 5.1-3.3.37-.48-3.3a9.5 9.5 0 00-4.06-.9 5.62 5.62 0 00-2.87.66 2.33 2.33 0 00-1.15 2.25 2.13 2.13 0 001.3 2.07 11.91 11.91 0 003.21.92 36.69 36.69 0 013.82.83 7.46 7.46 0 013.21 1.74 4.9 4.9 0 011.3 3.73 5.56 5.56 0 01-2.66 4.91z"></path><path d="M181.17 30.75h-5.71v23.31h5.71q5.47 0 8.36-3t2.88-8.61q0-5.43-2.88-8.55t-8.36-3.15zM121.63 119.32V81.74h114.91v37.58zM153.22 109h-2v-6.85a4.8 4.8 0 00-1.58-4 5.57 5.57 0 00-3.55-1.26 5 5 0 00-4.92 3.26 4.19 4.19 0 00-1.88-2.46 5.82 5.82 0 00-3-.8 4.89 4.89 0 00-4.56 2.56v-2.21h-6.28v3.26h2v8.5h-2v3.23h9.11V109h-2.86v-5.25a4.4 4.4 0 01.69-2.56 2.47 2.47 0 012.21-1q2.57 0 2.56 3.63v8.41h6.29V109h-2v-5.25a4.47 4.47 0 01.67-2.56 2.42 2.42 0 012.19-1q2.63 0 2.63 3.63v8.41h6.28zm9.88-12.07q-4 0-6 2.36a8.41 8.41 0 00-2 5.66 7.25 7.25 0 002.17 5.62 8 8 0 005.65 2 8.54 8.54 0 005.94-2.11 7.27 7.27 0 002.34-5.67 8.21 8.21 0 00-2-5.51q-2.07-2.34-6.1-2.34zm-.1 12.35a3 3 0 01-2.63-1.33 5.68 5.68 0 01-.9-3.26 5 5 0 011-3.28 3.23 3.23 0 012.61-1.18 3.5 3.5 0 012.59 1.08 4.56 4.56 0 011.07 3.31 5.21 5.21 0 01-1 3.41 3.33 3.33 0 01-2.74 1.25zm25-2.3l-3.39-.29-.7 2.32H179l8.32-9.54-.32-2.23h-13.19l-.53 5.25 3.16.34.67-2.36h4.65l-8.25 9.53.44 2.26h13.13zm7.62-9.74h-4.46v5.39h4.46zm0 9.61h-4.46v5.39h4.46zm13.54-17.49h-4.23l-6.48 22.88h4.22zm8.68 0h-4.23l-6.45 22.88h4.19zm15 22.51l-.07-2.26a1.22 1.22 0 01-.56.1c-.69 0-1-.39-1-1.16v-6.49a4.39 4.39 0 00-1.8-3.84 7 7 0 00-4.16-1.28 14.55 14.55 0 00-3.16.3 24.14 24.14 0 00-3.29 1.06l-.56 3.46 3.39.4.5-1.69a2.78 2.78 0 011.08-.37 11.3 11.3 0 011.25-.07c1.19 0 1.89.37 2.09 1.1a8.55 8.55 0 01.3 2.26v.5a8.91 8.91 0 00-1.18-.11h-1.21a12.64 12.64 0 00-4.81.88 3.53 3.53 0 00-2.18 3.64 3.66 3.66 0 001.48 3.33 5.63 5.63 0 003.11 1 4.67 4.67 0 003-.91 6.78 6.78 0 001.8-2 3 3 0 003.33 3 5.54 5.54 0 002.66-.85zm-9.25-2.32a1.69 1.69 0 01-1.36-.52 1.81 1.81 0 01-.43-1.21 1.67 1.67 0 01.86-1.68 4.63 4.63 0 012-.42 7.69 7.69 0 011.07.07l1.06.13a3.58 3.58 0 01-1.08 2.74 3.24 3.24 0 01-2.11.89z"></path></svg></a></div><ul class="link-list-mdn"><li><a href="/en-US/docs/Web">Web Technologies</a></li><li><a href="/en-US/docs/Learn">Learn Web Development</a></li><li><a href="/en-US/docs/MDN/About">About MDN</a></li><li><a href="/en-US/docs/MDN/Feedback">Feedback</a></li></ul><ul class="link-list-moz"><li><a href="https://www.mozilla.org/about/" target="_blank" rel="noopener noreferrer">About</a></li><li><a href="https://shop.spreadshirt.com/mdn-store/" target="_blank" rel="noopener noreferrer">MDN Web Docs Store</a></li><li><a href="https://www.mozilla.org/contact/" target="_blank" rel="noopener noreferrer">Contact Us</a></li><li><a href="https://www.mozilla.org/firefox/?utm_source=developer.mozilla.org&amp;utm_campaign=footer&amp;utm_medium=referral" target="_blank" rel="noopener noreferrer">Firefox</a></li></ul><div class="social social-mdn"><h4>MDN</h4><ul><li><a class="social-icon twitter" href="https://twitter.com/mozdevnet" target="_blank" rel="noopener noreferrer"><span class="visually-hidden">MDN on Twitter</span></a></li><li><a class="social-icon github" href="https://github.com/mdn/" target="_blank" rel="noopener noreferrer"><span class="visually-hidden">MDN on Github</span></a></li></ul></div><div class="social social-moz"><h4>Mozilla</h4><ul><li><a class="social-icon twitter" href="https://twitter.com/mozilla" target="_blank" rel="noopener noreferrer"><span class="visually-hidden">Mozilla on Twitter</span></a></li><li><a class="social-icon instagram" href="https://www.instagram.com/mozillagram/" target="_blank" rel="noopener noreferrer"><span class="visually-hidden">Mozilla on Instagram</span></a></li></ul></div><p id="license" class="footer-license">¬© 2005-<!-- -->2022<!-- --> Mozilla and individual contributors. Content is available under<!-- --> <a href="/docs/MDN/About#Copyrights_and_licenses">these licenses</a>.</p><ul class="footer-legal"><li><a href="https://www.mozilla.org/about/legal/terms/mozilla" target="_blank" rel="noopener noreferrer">Terms</a></li><li><a href="https://www.mozilla.org/privacy/websites/" target="_blank" rel="noopener noreferrer">Privacy</a></li><li><a href="https://www.mozilla.org/privacy/websites/#cookies" target="_blank" rel="noopener noreferrer">Cookies</a></li></ul></div></footer><div class="page-overlay hidden"></div></div><script type="application/json" id="hydration">{"doc":{"isMarkdown":true,"isTranslated":false,"isActive":true,"flaws":{},"title":"Basic concepts behind Web Audio API","mdn_url":"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","locale":"en-US","native":"English (US)","sidebarHTML":"<ol><li><strong><a href=\"/en-US/docs/Web/API/Web_Audio_API\">Web Audio API</a></strong></li><li class=\"toggle\"><details open=\"\"><summary>Guides</summary><ol><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API\">Using the Web Audio API</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API\">Basic concepts behind Web Audio API</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Best_practices\">Web Audio API best practices</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Advanced_techniques\">Advanced techniques: Creating and sequencing audio</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_AudioWorklet\">Background audio processing using AudioWorklet</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Controlling_multiple_parameters_with_ConstantSourceNode\">Controlling multiple parameters with ConstantSourceNode</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Migrating_from_webkitAudioContext\">Migrating from webkitAudioContext</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Simple_synth\">Example and tutorial: Simple synth keyboard</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Tools\">Tools for analyzing Web Audio usage</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_IIR_filters\">Using IIR filters</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">Visualizations with Web Audio API</a></li><li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\">Web audio spatialization basics</a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>Interfaces</summary><ol><li><a href=\"/en-US/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioBufferSourceNode\"><code>AudioBufferSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioDestinationNode\"><code>AudioDestinationNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioListener\"><code>AudioListener</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioNode\"><code>AudioNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioParam\"><code>AudioParam</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioProcessingEvent\"><code>AudioProcessingEvent</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioScheduledSourceNode\"><code>AudioScheduledSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorklet\"><code>AudioWorklet</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletGlobalScope\"><code>AudioWorkletGlobalScope</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletNode\"><code>AudioWorkletNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletProcessor\"><code>AudioWorkletProcessor</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext\"><code>BaseAudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/BiquadFilterNode\"><code>BiquadFilterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ConstantSourceNode\"><code>ConstantSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ConvolverNode\"><code>ConvolverNode</code></a></li><li><a href=\"/en-US/docs/Web/API/DelayNode\"><code>DelayNode</code></a></li><li><a href=\"/en-US/docs/Web/API/DynamicsCompressorNode\"><code>DynamicsCompressorNode</code></a></li><li><a href=\"/en-US/docs/Web/API/GainNode\"><code>GainNode</code></a></li><li><a href=\"/en-US/docs/Web/API/IIRFilterNode\"><code>IIRFilterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaElementAudioSourceNode\"><code>MediaElementAudioSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaStreamAudioDestinationNode\"><code>MediaStreamAudioDestinationNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaStreamAudioSourceNode\"><code>MediaStreamAudioSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/OfflineAudioCompletionEvent\"><code>OfflineAudioCompletionEvent</code></a></li><li><a href=\"/en-US/docs/Web/API/OfflineAudioContext\"><code>OfflineAudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/OscillatorNode\"><code>OscillatorNode</code></a></li><li><a href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/PeriodicWave\"><code>PeriodicWave</code></a></li><li><a href=\"/en-US/docs/Web/API/WaveShaperNode\"><code>WaveShaperNode</code></a></li><li><a href=\"/en-US/docs/Web/API/StereoPannerNode\"><code>StereoPannerNode</code></a></li></ol></details></li></ol>","body":[{"type":"prose","value":{"id":null,"title":null,"isH3":false,"content":"<p>This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does.</p>"}},{"type":"prose","value":{"id":"audio_graphs","title":"Audio graphs","isH3":false,"content":"<p>The <a href=\"/en-US/docs/Web/API/Web_Audio_API\">Web Audio API</a> involves handling audio operations inside an <a href=\"/en-US/docs/Web/API/AudioContext\">audio context</a>, and has been designed to allow <em>modular routing</em>. Each <a href=\"/en-US/docs/Web/API/AudioNode\">audio node</a> performs a basic audio operation and is linked with one more other audio nodes to form an <a href=\"/en-US/docs/Web/API/AudioNode#the_audio_routing_graph\">audio routing graph</a>. Several sources with different channel layouts are supported, even within a single context. This modular design provides the flexibility to create complex audio functions with dynamic effects.</p>\n<p>Audio nodes are linked via their inputs and outputs, forming a chain that starts with one or more sources, goes through one or more nodes, then ends up at a destination (although you don't have to provide a destination if you only want to visualize some audio data). A simple, typical workflow for web audio would look something like this:</p>\n<ol>\n  <li>Create the audio context.</li>\n  <li>Create audio sources inside the context (such as <a href=\"/en-US/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a>, an oscillator, or stream).</li>\n  <li>Create audio effects (such as the reverb, biquad filter, panner, or compressor nodes).</li>\n  <li>Choose the final destination for the audio (such as the user's computer speakers).</li>\n  <li>Connect the source nodes to zero or more effect nodes and then to the chosen destination.</li>\n</ol>\n<div class=\"notecard note\" id=\"sect1\">\n  <p><strong>Note:</strong> The <a href=\"https://en.wikipedia.org/wiki/Surround_sound#Channel_notation\" class=\"external\" rel=\" noopener\">channel notation</a> is a numeric value, such as <em>2.0</em> or <em>5.1</em>, representing the number of audio channels available on a signal. The first number is the number of full frequency range audio channels the signal includes. The number appearing after the period indicates the number of those channels reserved for low-frequency effect (LFE) outputs; these are often called <strong>subwoofers</strong>.</p>\n</div>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudioapi_en.svg\" alt=\"A simple box diagram with an outer box labeled Audio context and three inner boxes labeled Sources, Effects, and Destination. The three inner boxes have arrows between them pointing from left to right, indicating the flow of audio information.\" width=\"643\" height=\"143\" loading=\"lazy\">\n</p>\n<p>Each input or output is composed of one or more audio <strong>channels</strong>, which together represent a specific audio layout. Any discrete channel structure is supported, including <em>mono</em>, <em>stereo</em>, <em>quad</em>, <em>5.1</em>, and so on.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/mdn.png\" alt=\"Show the ability of audio nodes to connect via their inputs and outputs and the channels inside these inputs/outputs.\" width=\"1308\" height=\"750\" loading=\"lazy\">\n</p>\n<p>You have several ways to obtain audio:</p>\n<ul>\n  <li>Sound can be generated directly in JavaScript by an audio node (such as an oscillator).</li>\n  <li>It can be created from raw <a href=\"https://en.wikipedia.org/wiki/Pulse-code_modulation\" class=\"external\" rel=\" noopener\">PCM</a> data (such as .WAV files or other formats supported by <a href=\"/en-US/docs/Web/API/BaseAudioContext/decodeAudioData\" title=\"decodeAudioData()\"><code>decodeAudioData()</code></a>).</li>\n  <li>It can be generated from HTML media elements, such as <a href=\"/en-US/docs/Web/HTML/Element/video\"><code>&lt;video&gt;</code></a> or <a href=\"/en-US/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a>.</li>\n  <li>It can be obtained from a <a href=\"/en-US/docs/Web/API/WebRTC_API\">WebRTC</a> <a href=\"/en-US/docs/Web/API/MediaStream\"><code>MediaStream</code></a>, such as a webcam or microphone.</li>\n</ul>"}},{"type":"prose","value":{"id":"audio_data_whats_in_a_sample","title":"Audio data: what's in a sample","isH3":false,"content":"<p>When an audio signal is processed, sampling happens. <strong>Sampling</strong> is the conversion of a <a href=\"https://en.wikipedia.org/wiki/Continuous_signal\" class=\"external\" rel=\" noopener\">continuous signal</a> to a <a href=\"https://en.wikipedia.org/wiki/Discrete_signal\" class=\"external\" rel=\" noopener\">discrete signal</a>. Put another way, a continuous sound wave, such as a band playing live, is converted into a sequence of digital samples (a discrete-time signal) that allows a computer to handle the audio in distinct blocks.</p>\n<p>You'll find more information on the Wikipedia page <a href=\"https://en.wikipedia.org/wiki/Sampling_%28signal_processing%29\" class=\"external\" rel=\" noopener\"><em>Sampling (signal processing)</em></a>.</p>"}},{"type":"prose","value":{"id":"audio_buffers_frames_samples_and_channels","title":"Audio buffers: frames, samples, and channels","isH3":false,"content":"<p>An <a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a> is defined with three parameters:</p>\n<ul>\n  <li>the number of channels (1 for mono, 2 for stereo, etc.),</li>\n  <li>its length, meaning the number of sample frames inside the buffer,</li>\n  <li>and the sample rate, the number of sample frames played per second.</li>\n</ul>\n<p>A <em>sample</em> is a single 32-bit floating point value representing the value of the audio stream at each specific moment in time within a particular channel (left or right, if in the case of stereo). A <em>frame</em>, or <em>sample frame</em>, is the set of all values for all channels that will play at a specific moment in time: all the samples of all the channels that play at the same time (two for a stereo sound, six for 5.1, etc.).</p>\n<p>The <em>sample rate</em> is the quantity of those samples (or frames, since all samples of a frame play at the same time) that will play in one second, measured in Hz. The higher the sample rate, the better the sound quality.</p>\n<p>Let's look at a <em>mono</em> and a <em>stereo</em> audio buffer, each one second long at a rate of 44100Hz:</p>\n<ul>\n  <li>The <em>mono</em> buffer will have 44,100 samples and 44,100 frames. The <code>length</code> property will be 44,100.</li>\n  <li>The <em>stereo</em> buffer will have 88,200 samples but still 44,100 frames. The <code>length</code> property will still be 44100 since it equals the number of frames.</li>\n</ul>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/sampleframe-english.png\" alt=\"A diagram showing several frames in an audio buffer in a long line, each one containing two samples, as the buffer has two channels, it is stereo.\" width=\"1245\" height=\"219\" loading=\"lazy\">\n</p>\n<p>When a buffer plays, you will first hear the leftmost sample frame, then the one right next to it, then the next, <em>and so on</em>, until the end of the buffer. In the case of stereo, you will hear both channels simultaneously. Sample frames are handy because they are independent of the number of channels and represent time in an ideal way for precise audio manipulation.</p>\n<div class=\"notecard note\" id=\"sect2\">\n  <p><strong>Note:</strong> To get a time in seconds from a frame count, divide the number of frames by the sample rate. To get the number of frames from the number of samples, you only need to divide the latter value by the channel count.</p>\n</div>\n<p>Here are a couple of simple examples:</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">const</span> context <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> buffer <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioBuffer</span><span class=\"token punctuation\">(</span>context<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token literal-property property\">numberOfChannels</span><span class=\"token operator\">:</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">length</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">sampleRate</span><span class=\"token operator\">:</span> <span class=\"token number\">44100</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<div class=\"notecard note\" id=\"sect3\">\n  <p><strong>Note:</strong> In <a href=\"https://en.wikipedia.org/wiki/Digital_audio\" class=\"external\" rel=\" noopener\">digital audio</a>, <strong>44,100 <a href=\"https://en.wikipedia.org/wiki/Hertz\" class=\"external\" rel=\" noopener\">Hz</a></strong> (alternately represented as <strong>44.1 kHz</strong>) is a common <a href=\"https://en.wikipedia.org/wiki/Sampling_frequency\" class=\"external\" rel=\" noopener\">sampling frequency</a>. Why 44.1 kHz?</p>\n  <p>Firstly, because the <a href=\"https://en.wikipedia.org/wiki/Hearing_range\" class=\"external\" rel=\" noopener\">hearing range</a> of human ears is roughly 20 Hz to 20,000 Hz. Via the <a href=\"https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\" class=\"external\" rel=\" noopener\">Nyquist‚ÄìShannon sampling theorem</a>, the sampling frequency must be greater than twice the maximum frequency one wishes to reproduce. Therefore, the sampling rate has to be <em>greater</em> than 40,000 Hz.</p>\n  <p>Secondly, signals must be <a href=\"https://en.wikipedia.org/wiki/Low-pass_filter\" class=\"external\" rel=\" noopener\">low-pass filtered</a> before sampling, otherwise <a href=\"https://en.wikipedia.org/wiki/Aliasing\" class=\"external\" rel=\" noopener\">aliasing</a> occurs. While an ideal low-pass filter would perfectly pass frequencies below 20 kHz (without attenuating them) and perfectly cut off frequencies above 20 kHz, in practice, a <a href=\"https://en.wikipedia.org/wiki/Transition_band\" class=\"external\" rel=\" noopener\">transition band</a> is necessary, where frequencies are partly attenuated. The wider this transition band is, the easier and more economical it is to make an <a href=\"https://en.wikipedia.org/wiki/Anti-aliasing_filter\" class=\"external\" rel=\" noopener\">anti-aliasing filter</a>. The 44.1 kHz sampling frequency allows for a 2.05 kHz transition band.</p>\n</div>\n<p>If you use this call above, you will get a stereo buffer with two channels that, when played back on an <a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a> running at 44100 Hz (very common, most normal sound cards run at this rate), will last for 0.5 seconds: 22,050 frames/44,100 Hz = 0.5 seconds.</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">const</span> context <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> buffer <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioBuffer</span><span class=\"token punctuation\">(</span>context<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token literal-property property\">numberOfChannels</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">length</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n  <span class=\"token literal-property property\">sampleRate</span><span class=\"token operator\">:</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<p>If you use this call, you will get a mono buffer (single-channel buffer) that, when played back on an <a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a> running at 44,100 Hz, will be automatically <em>resampled</em> to 44,100 Hz (and therefore yield 44,100 frames), and last for 1.0 second: 44,100 frames/44,100 Hz = 1 second.</p>\n<div class=\"notecard note\" id=\"sect4\">\n  <p><strong>Note:</strong> Audio resampling is very similar to image resizing. Say you've got a 16 x 16 image but want it to fill a 32 x 32 area. You resize (or resample) it. The result has less quality (it can be blurry or edgy, depending on the resizing algorithm), but it works, with the resized image taking up less space. Resampled audio is the same: you save space, but, in practice, you cannot correctly reproduce high-frequency content or treble sound.</p>\n</div>"}},{"type":"prose","value":{"id":"planar_versus_interleaved_buffers","title":"Planar versus interleaved buffers","isH3":true,"content":"<p>The Web Audio API uses a planar buffer format. The left and right channels are stored like this:</p>\n<pre class=\"notranslate\">LLLLLLLLLLLLLLLLRRRRRRRRRRRRRRRR (for a buffer of 16 frames)\n</pre>\n<p>This structure is widespread in audio processing, making it easy to process each channel independently.</p>\n<p>The alternative is to use an interleaved buffer format:</p>\n<pre class=\"notranslate\">LRLRLRLRLRLRLRLRLRLRLRLRLRLRLRLR (for a buffer of 16 frames)\n</pre>\n<p>This format is prevalent for storing and playing back audio without much processing, for example: .WAV files or a decoded MP3 stream.</p>\n<p>Because the Web Audio API is designed for processing, it exposes <em>only</em> planar buffers. It uses the planar format but converts the audio to the interleaved format when it sends it to the sound card for playback. Conversely, when the API decodes an MP3, it starts with the interleaved format and converts it to the planar format for processing.</p>"}},{"type":"prose","value":{"id":"audio_channels","title":"Audio channels","isH3":false,"content":"<p>Each audio buffer may contain different numbers of channels. Most modern audio devices use the basic <em>mono</em> (only one channel) and <em>stereo</em> (left and right channels) settings. Some more complex sets support <em>surround sound</em> settings (like <em>quad</em> and <em>5.1</em>), which can lead to a richer sound experience thanks to their high channel count. We usually represent the channels with the standard abbreviations detailed in the table below:</p>\n<table>\n  <thead>\n    <tr>\n      <th>Name</th>\n      <th>Channels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><em>Mono</em></td>\n      <td><code>0: M: mono</code></td>\n    </tr>\n    <tr>\n      <td><em>Stereo</em></td>\n      <td><code>0: L: left 1: R: right</code></td>\n    </tr>\n    <tr>\n      <td><em>Quad</em></td>\n      <td><code>0: L: left 1: R: right 2: SL: surround left 3: SR: surround right</code></td>\n    </tr>\n    <tr>\n      <td><em>5.1</em></td>\n      <td><code>0: L: left 1: R: right 2: C: center 3: LFE: subwoofer 4: SL: surround left 5: SR: surround right</code></td>\n    </tr>\n  </tbody>\n</table>"}},{"type":"prose","value":{"id":"up-mixing_and_down-mixing","title":"Up-mixing and down-mixing","isH3":true,"content":"<p>When the numbers of channels of the input and the output don't match, up-mixing, or down-mixing, must be done. The following rules, controlled by setting the <a href=\"/en-US/docs/Web/API/AudioNode/channelInterpretation\"><code>AudioNode.channelInterpretation</code></a> property to <code>speakers</code> or <code>discrete</code>, apply:</p>\n<table class=\"standard-table\">\n  <thead>\n    <tr>\n      <th scope=\"row\">Interpretation</th>\n      <th scope=\"col\">Input channels</th>\n      <th scope=\"col\">Output channels</th>\n      <th scope=\"col\">Mixing rules</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"13\" scope=\"row\"><code>speakers</code></th>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Up-mix from mono to stereo</em>.<br>The <code>M</code> input\n        channel is used for both output channels (<code>L</code> and\n        <code>R</code>).<br><code>output.L = input.M<br>output.R = input.M</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Up-mix from mono to quad.</em><br>The <code>M</code> input channel\n        is used for non-surround output channels (<code>L</code> and\n        <code>R</code>). Surround output channels (<code>SL</code> and\n        <code>SR</code>) are silent.<br><code>output.L = input.M<br>output.R = input.M<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from mono to 5.1.</em><br>The <code>M</code> input channel\n        is used for the center output channel (<code>C</code>). All the others\n        (<code>L</code>, <code>R</code>, <code>LFE</code>, <code>SL</code>, and\n        <code>SR</code>) are silent.<br><code>output.L = 0<br>output.R = 0</code><br><code>output.C = input.M<br>output.LFE = 0<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from stereo to mono</em>.<br>Both input channels (<code>L</code>\n        and <code>R</code>) are equally combined to produce the unique output\n        channel (<code>M</code>).<br><code>output.M = 0.5 * (input.L + input.R)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Up-mix from stereo to quad.</em><br>The <code>L</code> and\n        <code>R </code>input channels are used for their non-surround respective\n        output channels (<code>L</code> and <code>R</code>). Surround output\n        channels (<code>SL</code> and <code>SR</code>) are silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from stereo to 5.1.</em><br>The <code>L</code> and\n        <code>R </code>input channels are used for their non-surround respective\n        output channels (<code>L</code> and <code>R</code>). Surround output\n        channels (<code>SL</code> and <code>SR</code>), as well as the center\n        (<code>C</code>) and subwoofer (<code>LFE</code>) channels, are left\n        silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = 0<br>output.SR = 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from quad to mono</em>.<br>All four input channels\n        (<code>L</code>, <code>R</code>, <code>SL</code>, and <code>SR</code>)\n        are equally combined to produce the unique output channel\n        (<code>M</code>).<br><code>output.M = 0.25 * (input.L + input.R + </code><code>input.SL + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Down-mix from quad to stereo</em>.<br>Both left input channels\n        (<code>L</code> and <code>SL</code>) are equally combined to produce the\n        unique left output channel (<code>L</code>). And similarly, both right\n        input channels (<code>R</code> and <code>SR</code>) are equally combined\n        to produce the unique right output channel (<code>R</code>).<br><code>output.L = 0.5 * (input.L + input.SL</code><code>)</code><br><code>output.R = 0.5 * (input.R + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Up-mix from quad to 5.1.</em><br>The <code>L</code>,\n        <code>R</code>, <code>SL</code>, and <code>SR</code> input channels are\n        used for their respective output channels (<code>L</code> and\n        <code>R</code>). Center (<code>C</code>) and subwoofer\n        (<code>LFE</code>) channels are left silent.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = input.SL<br>output.SR = input.SR</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to mono.</em><br>The left (<code>L</code> and\n        <code>SL</code>), right (<code>R</code> and <code>SR</code>) and central\n        channels are all mixed together. The surround channels are slightly\n        attenuated, and the regular lateral channels are power-compensated to\n        make them count as a single channel by multiplying by <code>‚àö2/2</code>.\n        The subwoofer (<code>LFE</code>) channel is lost.<br><code>output.M = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL +\ninput.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to stereo.</em><br>The central channel\n        (<code>C</code>) is summed with each lateral surround channel (<code>SL</code>\n        or <code>SR</code>) and mixed to each lateral channel. As it is mixed\n        down to two channels, it is mixed at a lower power: in each case, it is\n        multiplied by <code>‚àö2/2</code>. The subwoofer (<code>LFE</code>)\n        channel is lost.<br><code>output.L = input.L + 0.7071 * (input.C + input.SL)<br>output.R =\ninput.R </code><code>+ 0.7071 * (input.C + input.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Down-mix from 5.1 to quad.</em><br>The central (<code>C</code>) is\n        mixed with the lateral non-surround channels (<code>L</code> and\n        <code>R</code>). As it is mixed down to two channels, it is mixed at a\n        lower power: in each case, it is multiplied by <code>‚àö2/2</code>. The\n        surround channels are passed unchanged. The subwoofer (<code>LFE</code>)\n        channel is lost.<br><code>output.L = input.L + 0.7071 * input.C<br>output.R = input.R +\n0.7071 * input.C<br>output.SL = input.SL<br>output.SR =\ninput.SR</code>\n      </td>\n    </tr>\n    <tr>\n      <td colspan=\"2\">Other, non-standard layouts</td>\n      <td>\n        Non-standard channel layouts behave as if\n        <code>channelInterpretation</code> is set to\n        <code>discrete</code>.<br>The specification explicitly allows the future definition of new speaker layouts. Therefore, this fallback is not future-proof as the behavior of the browsers for a specific number of channels may change in the future.\n      </td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" scope=\"row\"><code>discrete</code></th>\n      <td>any (<code>x</code>)</td>\n      <td>any (<code>y</code>) where <code>x&lt;y</code></td>\n      <td>\n        <em>Up-mix discrete channels.</em><br>Fill each output channel with\n        its input counterpart ‚Äî that is, the input channel with the same index.\n        Channels with no corresponding input channels are left silent.\n      </td>\n    </tr>\n    <tr>\n      <td>any (<code>x</code>)</td>\n      <td>any (<code>y</code>) where <code>x&gt;y</code></td>\n      <td>\n        <em>Down-mix discrete channels.</em><br>Fill each output channel with\n        its input counterpart ‚Äî that is, the input channel with the same index.\n        Input channels with no corresponding output channels are dropped.\n      </td>\n    </tr>\n  </tbody>\n</table>"}},{"type":"prose","value":{"id":"visualizations","title":"Visualizations","isH3":false,"content":"<p>In general, we get the output over time to produce audio visualizations, usually reading its gain or frequency data. Then, using a graphical tool, we turn the obtained data into a visual representation, such as a graph. The Web Audio API has an <a href=\"/en-US/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a> available that doesn't alter the audio signal passing through it. Additionally, it outputs the audio data, allowing us to process it via a technology such as <a href=\"/en-US/docs/Web/HTML/Element/canvas\"><code>&lt;canvas&gt;</code></a>.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fttaudiodata_en.svg\" alt=\"Without modifying the audio stream, the node allows to get the frequency and time-domain data associated with it, using an FFT.\" width=\"693\" height=\"206\" loading=\"lazy\">\n</p>\n<p>You can grab data using the following methods:</p>\n<dl>\n  <dt id=\"analysernode.getfloatfrequencydata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData\"><code>AnalyserNode.getFloatFrequencyData()</code></a></dt>\n  <dd>\n    <p>Copies the current frequency data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code></a> array passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getbytefrequencydata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData\"><code>AnalyserNode.getByteFrequencyData()</code></a></dt>\n  <dd>\n    <p>Copies the current frequency data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getfloattimedomaindata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getFloatTimeDomainData\"><code>AnalyserNode.getFloatTimeDomainData()</code></a></dt>\n  <dd>\n    <p>Copies the current waveform, or time-domain, data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code></a> array passed into it.</p>\n  </dd>\n  <dt id=\"analysernode.getbytetimedomaindata\"><a href=\"/en-US/docs/Web/API/AnalyserNode/getByteTimeDomainData\"><code>AnalyserNode.getByteTimeDomainData()</code></a></dt>\n  <dd>\n    <p>Copies the current waveform, or time-domain, data into a <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code></a> (unsigned byte array) passed into it.</p>\n  </dd>\n</dl>\n<div class=\"notecard note\" id=\"sect5\">\n  <p><strong>Note:</strong> For more information, see our <a href=\"/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">Visualizations with Web Audio API</a> article.</p>\n</div>"}},{"type":"prose","value":{"id":"spatializations","title":"Spatializations","isH3":false,"content":"<p>Audio spatialization allows us to model the position and behavior of an audio signal at a certain point in physical space, simulating the listener hearing that audio. In the Web Audio API, spatialization is handled by the <a href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code></a> and the <a href=\"/en-US/docs/Web/API/AudioListener\"><code>AudioListener</code></a>.</p>\n<p>The panner uses right-hand Cartesian coordinates to describe the audio source's <em>position</em> as a vector and its <em>orientation</em> as a 3D directional cone. The cone can be pretty large, for example, for omnidirectional sources.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/pannernode_en.svg\" alt=\"The PannerNode defines a spatial position and orientation for a given signal.\" width=\"799\" height=\"340\" loading=\"lazy\">\n</p>\n<p>Similarly, the Web Audio API describes the listener using right-hand Cartesian coordinates: their <em>position</em> as one vector and their <em>orientation</em> as two direction vectors, <em>up</em> and <em>front</em>. These vectors define the direction of the top of the listener's head and the direction the listener's nose is pointing. The vectors are perpendicular to one another.</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudiolistenerreduced.png\" alt=\"We see the position, up, and front vectors of an AudioListener, with the up and front vectors at 90¬∞ from the other.\" width=\"634\" height=\"250\" loading=\"lazy\">\n</p>\n<div class=\"notecard note\" id=\"sect6\">\n  <p><strong>Note:</strong> For more information, see our <a href=\"/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\">Web audio spatialization basics</a> article.</p>\n</div>"}},{"type":"prose","value":{"id":"fan-in_and_fan-out","title":"Fan-in and Fan-out","isH3":false,"content":"<p>In audio terms, <strong>fan-in</strong> describes the process by which a <a href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a> takes a series of <em>mono</em> input sources and outputs a single multi-channel signal:</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanin.svg\" alt=\"\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>\n<p><strong>Fan-out</strong> describes the opposite process, whereby a <a href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code></a> takes a multi-channel input source and outputs multiple <em>mono</em> output signals:</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanout.svg\" alt=\"\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>"}}],"toc":[{"text":"Audio graphs","id":"audio_graphs"},{"text":"Audio data: what's in a sample","id":"audio_data_whats_in_a_sample"},{"text":"Audio buffers: frames, samples, and channels","id":"audio_buffers_frames_samples_and_channels"},{"text":"Audio channels","id":"audio_channels"},{"text":"Visualizations","id":"visualizations"},{"text":"Spatializations","id":"spatializations"},{"text":"Fan-in and Fan-out","id":"fan-in_and_fan-out"}],"summary":"This article explains some of the audio theory behind how the features of the Web Audio API work to help you make informed decisions while designing how your app routes audio. If you are not already a sound engineer, it will give you enough background to understand why the Web Audio API works as it does.","popularity":0.0011,"modified":"2022-08-15T04:42:34.000Z","other_translations":[{"title":"Les concepts de base de la Web Audio API","locale":"fr","native":"Fran√ßais"},{"title":"Basic concepts behind Web Audio API","locale":"ja","native":"Êó•Êú¨Ë™û"},{"title":"Web Audio APIÏùò Í∏∞Î≥∏ Í∞úÎÖê","locale":"ko","native":"ÌïúÍµ≠Ïñ¥"},{"title":"ÁΩëÈ°µÈü≥È¢ëÊé•Âè£ÁöÑÂü∫Êú¨Ê¶ÇÂøµ","locale":"zh-CN","native":"‰∏≠Êñá (ÁÆÄ‰Ωì)"}],"source":{"folder":"en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api","github_url":"https://github.com/mdn/content/blob/style/old/files/en-us/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md","last_commit_url":"https://github.com/mdn/content/commit/98913882aa1e4648b84b4e008d50fe50960eb10c","filename":"index.md"},"parents":[{"uri":"/en-US/docs/Web","title":"Web technology for developers"},{"uri":"/en-US/docs/Web/API","title":"Web APIs"},{"uri":"/en-US/docs/Web/API/Web_Audio_API","title":"Web Audio API"},{"uri":"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","title":"Basic concepts behind Web Audio API"}],"pageTitle":"Basic concepts behind Web Audio API - Web APIs | MDN","noIndexing":false}}</script></body></html>