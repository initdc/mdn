{"doc":{"isMarkdown":true,"isTranslated":false,"isActive":true,"flaws":{},"title":"BaseAudioContext","mdn_url":"/en-US/docs/Web/API/BaseAudioContext","locale":"en-US","native":"English (US)","sidebarHTML":"<ol><li><strong><a href=\"/en-US/docs/Web/API/Web_Audio_API\">Web Audio API</a></strong></li><li><strong><a href=\"/en-US/docs/Web/API/BaseAudioContext\"><code>BaseAudioContext</code></a></strong></li><li class=\"toggle\"><details open=\"\"><summary>Properties</summary><ol><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/audioWorklet\"><code>audioWorklet</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/currentTime\"><code>currentTime</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/destination\"><code>destination</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/listener\"><code>listener</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/sampleRate\"><code>sampleRate</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/state\"><code>state</code></a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>Methods</summary><ol><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createAnalyser\"><code>createAnalyser()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createBiquadFilter\"><code>createBiquadFilter()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createBuffer\"><code>createBuffer()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createBufferSource\"><code>createBufferSource()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createChannelMerger\"><code>createChannelMerger()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createChannelSplitter\"><code>createChannelSplitter()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createConstantSource\"><code>createConstantSource()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createConvolver\"><code>createConvolver()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createDelay\"><code>createDelay()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createDynamicsCompressor\"><code>createDynamicsCompressor()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createGain\"><code>createGain()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createIIRFilter\"><code>createIIRFilter()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createOscillator\"><code>createOscillator()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createPanner\"><code>createPanner()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createPeriodicWave\"><code>createPeriodicWave()</code></a></li><li><svg class=\"icon icon-deprecated\" tabindex=\"0\">\n    <use xlink:href=\"/assets/badges.svg#icon-deprecated\"></use>\n</svg><a href=\"/en-US/docs/Web/API/BaseAudioContext/createScriptProcessor\"><code>createScriptProcessor()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createStereoPanner\"><code>createStereoPanner()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/createWaveShaper\"><code>createWaveShaper()</code></a></li><li><a href=\"/en-US/docs/Web/API/BaseAudioContext/decodeAudioData\"><code>decodeAudioData()</code></a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>Inheritance:</summary><ol><li><a href=\"/en-US/docs/Web/API/EventTarget\"><code>EventTarget</code></a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>Related pages for Web Audio API</summary><ol><li><a href=\"/en-US/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioBufferSourceNode\"><code>AudioBufferSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioDestinationNode\"><code>AudioDestinationNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioListener\"><code>AudioListener</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioNode\"><code>AudioNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioParam\"><code>AudioParam</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioProcessingEvent\"><code>AudioProcessingEvent</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioScheduledSourceNode\"><code>AudioScheduledSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorklet\"><code>AudioWorklet</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletGlobalScope\"><code>AudioWorkletGlobalScope</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletNode\"><code>AudioWorkletNode</code></a></li><li><a href=\"/en-US/docs/Web/API/AudioWorkletProcessor\"><code>AudioWorkletProcessor</code></a></li><li><a href=\"/en-US/docs/Web/API/BiquadFilterNode\"><code>BiquadFilterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ConstantSourceNode\"><code>ConstantSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/ConvolverNode\"><code>ConvolverNode</code></a></li><li><a href=\"/en-US/docs/Web/API/DelayNode\"><code>DelayNode</code></a></li><li><a href=\"/en-US/docs/Web/API/DynamicsCompressorNode\"><code>DynamicsCompressorNode</code></a></li><li><a href=\"/en-US/docs/Web/API/GainNode\"><code>GainNode</code></a></li><li><a href=\"/en-US/docs/Web/API/IIRFilterNode\"><code>IIRFilterNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaElementAudioSourceNode\"><code>MediaElementAudioSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaStreamAudioDestinationNode\"><code>MediaStreamAudioDestinationNode</code></a></li><li><a href=\"/en-US/docs/Web/API/MediaStreamAudioSourceNode\"><code>MediaStreamAudioSourceNode</code></a></li><li><a href=\"/en-US/docs/Web/API/OfflineAudioCompletionEvent\"><code>OfflineAudioCompletionEvent</code></a></li><li><a href=\"/en-US/docs/Web/API/OfflineAudioContext\"><code>OfflineAudioContext</code></a></li><li><a href=\"/en-US/docs/Web/API/OscillatorNode\"><code>OscillatorNode</code></a></li><li><a href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/PeriodicWave\"><code>PeriodicWave</code></a></li><li><a href=\"/en-US/docs/Web/API/StereoPannerNode\"><code>StereoPannerNode</code></a></li><li><a href=\"/en-US/docs/Web/API/WaveShaperNode\"><code>WaveShaperNode</code></a></li></ol></details></li></ol>","body":[{"type":"prose","value":{"id":null,"title":null,"isH3":false,"content":"<p>The <code>BaseAudioContext</code> interface of the <a href=\"/en-US/docs/Web/API/Web_Audio_API\">Web Audio API</a> acts as a base definition for online and offline audio-processing graphs, as represented by <a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a> and <a href=\"/en-US/docs/Web/API/OfflineAudioContext\"><code>OfflineAudioContext</code></a> respectively. You wouldn't use <code>BaseAudioContext</code> directly â€” you'd use its features via one of these two inheriting interfaces.</p>\n<p>A <code>BaseAudioContext</code> can be a target of events, therefore it implements the <a href=\"/en-US/docs/Web/API/EventTarget\"><code>EventTarget</code></a> interface.</p><svg viewBox=\"-1 -1 650 42\" preserveAspectRatio=\"xMinYMin meet\">\n  <a style=\"text-decoration: none;\" xlink:href=\"/en-US/docs/Web/API/EventTarget\">\n    <rect x=\"0\" y=\"0\" width=\"88\" height=\"25\" fill=\"#fff\" stroke=\"#D4DDE4\" stroke-width=\"2px\"></rect>\n    <text x=\"44\" y=\"16\" font-size=\"10px\" fill=\"#4D4E53\" text-anchor=\"middle\">\n      EventTarget\n    </text>\n  </a>\n  <line x1=\"88\" y1=\"14\" x2=\"118\" y2=\"14\" stroke=\"#D4DDE4\" \"=\"\"></line>\n  <polyline points=\"88,14 98,9 98,19 88,14\" stroke=\"#D4DDE4\" fill=\"#fff\"></polyline>\n  <a style=\"text-decoration: none;\" xlink:href=\"/en-US/docs/Web/API/BaseAudioContext\" aria-current=\"page\">\n    <rect x=\"118\" y=\"0\" width=\"128\" height=\"25\" fill=\"#F4F7F8\" stroke=\"#D4DDE4\" stroke-width=\"2px\"></rect>\n    <text x=\"182\" y=\"16\" font-size=\"10px\" fill=\"#4D4E53\" text-anchor=\"middle\">\n      BaseAudioContext\n    </text>\n  </a></svg>"}},{"type":"prose","value":{"id":"properties","title":"Properties","isH3":false,"content":"<dl>\n  <dt id=\"baseaudiocontext.audioworklet\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/audioWorklet\"><code>BaseAudioContext.audioWorklet</code></a> <span title=\"This value may not be changed.\" class=\"badge inline readonly\">Read only </span> <span class=\"notecard inline secure\" title=\"This feature is available only in secure contexts (HTTPS)\">Secure context</span></dt>\n  <dd>\n    <p>Returns the <a href=\"/en-US/docs/Web/API/AudioWorklet\"><code>AudioWorklet</code></a> object, which can be used to create and manage <a href=\"/en-US/docs/Web/API/AudioNode\"><code>AudioNode</code></a>s in which JavaScript code implementing the <a href=\"/en-US/docs/Web/API/AudioWorkletProcessor\"><code>AudioWorkletProcessor</code></a> interface are run in the background to process audio data.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.currenttime\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/currentTime\"><code>BaseAudioContext.currentTime</code></a> <span title=\"This value may not be changed.\" class=\"badge inline readonly\">Read only </span></dt>\n  <dd>\n    <p>Returns a double representing an ever-increasing hardware time in seconds used for scheduling. It starts at <code>0</code>.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.destination\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/destination\"><code>BaseAudioContext.destination</code></a> <span title=\"This value may not be changed.\" class=\"badge inline readonly\">Read only </span></dt>\n  <dd>\n    <p>Returns an <a href=\"/en-US/docs/Web/API/AudioDestinationNode\"><code>AudioDestinationNode</code></a> representing the final destination of all audio in the context. It can be thought of as the audio-rendering device.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.listener\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/listener\"><code>BaseAudioContext.listener</code></a> <span title=\"This value may not be changed.\" class=\"badge inline readonly\">Read only </span></dt>\n  <dd>\n    <p>Returns the <a href=\"/en-US/docs/Web/API/AudioListener\"><code>AudioListener</code></a> object, used for 3D spatialization.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.samplerate\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/sampleRate\"><code>BaseAudioContext.sampleRate</code></a> <span title=\"This value may not be changed.\" class=\"badge inline readonly\">Read only </span></dt>\n  <dd>\n    <p>Returns a float representing the sample rate (in samples per second) used by all nodes in this context. The sample-rate of an <a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a> cannot be changed.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.state\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/state\"><code>BaseAudioContext.state</code></a> <span title=\"This value may not be changed.\" class=\"badge inline readonly\">Read only </span></dt>\n  <dd>\n    <p>Returns the current state of the <code>AudioContext</code>.</p>\n  </dd>\n</dl>"}},{"type":"prose","value":{"id":"events","title":"Events","isH3":true,"content":"<dl>\n  <dt id=\"statechange\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/statechange_event\" title=\"statechange\"><code>statechange</code></a></dt>\n  <dd>\n    <p>Fired when the <code>AudioContext</code>'s state changes due to the calling of one of the state change methods (<a href=\"/en-US/docs/Web/API/AudioContext/suspend\"><code>AudioContext.suspend</code></a>, <a href=\"/en-US/docs/Web/API/AudioContext/resume\"><code>AudioContext.resume</code></a>, or <a href=\"/en-US/docs/Web/API/AudioContext/close\"><code>AudioContext.close</code></a>).</p>\n  </dd>\n</dl>"}},{"type":"prose","value":{"id":"methods","title":"Methods","isH3":false,"content":"<p><em>Also implements methods from the interface</em> <a href=\"/en-US/docs/Web/API/EventTarget\"><code>EventTarget</code></a>.</p>\n<dl>\n  <dt id=\"baseaudiocontext.createanalyser\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createAnalyser\"><code>BaseAudioContext.createAnalyser()</code></a></dt>\n  <dd>\n    <p>Creates an <a href=\"/en-US/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a>, which can be used to expose audio time and frequency data and for example to create data visualizations.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createbiquadfilter\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createBiquadFilter\"><code>BaseAudioContext.createBiquadFilter()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/BiquadFilterNode\"><code>BiquadFilterNode</code></a>, which represents a second order filter configurable as several different common filter types: high-pass, low-pass, band-pass, etc</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createbuffer\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createBuffer\"><code>BaseAudioContext.createBuffer()</code></a></dt>\n  <dd>\n    <p>Creates a new, empty <a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a> object, which can then be populated by data and played via an <a href=\"/en-US/docs/Web/API/AudioBufferSourceNode\"><code>AudioBufferSourceNode</code></a>.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createbuffersource\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createBufferSource\"><code>BaseAudioContext.createBufferSource()</code></a></dt>\n  <dd>\n    <p>Creates an <a href=\"/en-US/docs/Web/API/AudioBufferSourceNode\"><code>AudioBufferSourceNode</code></a>, which can be used to play and manipulate audio data contained within an <a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a> object. <a href=\"/en-US/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a>s are created using <a href=\"/en-US/docs/Web/API/BaseAudioContext/createBuffer\" title=\"AudioContext.createBuffer()\"><code>AudioContext.createBuffer()</code></a> or returned by <a href=\"/en-US/docs/Web/API/BaseAudioContext/decodeAudioData\" title=\"AudioContext.decodeAudioData()\"><code>AudioContext.decodeAudioData()</code></a> when it successfully decodes an audio track.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createconstantsource\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createConstantSource\"><code>BaseAudioContext.createConstantSource()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/ConstantSourceNode\"><code>ConstantSourceNode</code></a> object, which is an audio source that continuously outputs a monaural (one-channel) sound signal whose samples all have the same value.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createchannelmerger\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createChannelMerger\"><code>BaseAudioContext.createChannelMerger()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a>, which is used to combine channels from multiple audio streams into a single audio stream.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createchannelsplitter\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createChannelSplitter\"><code>BaseAudioContext.createChannelSplitter()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code></a>, which is used to access the individual channels of an audio stream and process them separately.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createconvolver\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createConvolver\"><code>BaseAudioContext.createConvolver()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/ConvolverNode\"><code>ConvolverNode</code></a>, which can be used to apply convolution effects to your audio graph, for example a reverberation effect.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createdelay\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createDelay\"><code>BaseAudioContext.createDelay()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/DelayNode\"><code>DelayNode</code></a>, which is used to delay the incoming audio signal by a certain amount. This node is also useful to create feedback loops in a Web Audio API graph.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createdynamicscompressor\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createDynamicsCompressor\"><code>BaseAudioContext.createDynamicsCompressor()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/DynamicsCompressorNode\"><code>DynamicsCompressorNode</code></a>, which can be used to apply acoustic compression to an audio signal.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.creategain\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createGain\"><code>BaseAudioContext.createGain()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/GainNode\"><code>GainNode</code></a>, which can be used to control the overall volume of the audio graph.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createiirfilter\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createIIRFilter\"><code>BaseAudioContext.createIIRFilter()</code></a></dt>\n  <dd>\n    <p>Creates an <a href=\"/en-US/docs/Web/API/IIRFilterNode\"><code>IIRFilterNode</code></a>, which represents a second order filter configurable as several different common filter types.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createoscillator\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createOscillator\"><code>BaseAudioContext.createOscillator()</code></a></dt>\n  <dd>\n    <p>Creates an <a href=\"/en-US/docs/Web/API/OscillatorNode\"><code>OscillatorNode</code></a>, a source representing a periodic waveform. It basically generates a tone.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createpanner\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createPanner\"><code>BaseAudioContext.createPanner()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code></a>, which is used to spatialize an incoming audio stream in 3D space.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createperiodicwave\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createPeriodicWave\"><code>BaseAudioContext.createPeriodicWave()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/PeriodicWave\"><code>PeriodicWave</code></a>, used to define a periodic waveform that can be used to determine the output of an <a href=\"/en-US/docs/Web/API/OscillatorNode\"><code>OscillatorNode</code></a>.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createscriptprocessor\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createScriptProcessor\"><code>BaseAudioContext.createScriptProcessor()</code></a> <svg class=\"icon icon-deprecated\" tabindex=\"0\">\n    <use xlink:href=\"/assets/badges.svg#icon-deprecated\"></use>\n</svg></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/ScriptProcessorNode\"><code>ScriptProcessorNode</code></a>, which can be used for direct audio processing via JavaScript.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createstereopanner\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createStereoPanner\"><code>BaseAudioContext.createStereoPanner()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/StereoPannerNode\"><code>StereoPannerNode</code></a>, which can be used to apply stereo panning to an audio source.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.createwaveshaper\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/createWaveShaper\"><code>BaseAudioContext.createWaveShaper()</code></a></dt>\n  <dd>\n    <p>Creates a <a href=\"/en-US/docs/Web/API/WaveShaperNode\"><code>WaveShaperNode</code></a>, which is used to implement non-linear distortion effects.</p>\n  </dd>\n  <dt id=\"baseaudiocontext.decodeaudiodata\"><a href=\"/en-US/docs/Web/API/BaseAudioContext/decodeAudioData\"><code>BaseAudioContext.decodeAudioData()</code></a></dt>\n  <dd>\n    <p>Asynchronously decodes audio file data contained in an <a href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/ArrayBuffer\"><code>ArrayBuffer</code></a>. In this case, the <code>ArrayBuffer</code> is usually loaded from an <a href=\"/en-US/docs/Web/API/XMLHttpRequest\"><code>XMLHttpRequest</code></a>'s <code>response</code> attribute after setting the <code>responseType</code> to <code>arraybuffer</code>. This method only works on complete files, not fragments of audio files.</p>\n  </dd>\n</dl>"}},{"type":"prose","value":{"id":"examples","title":"Examples","isH3":false,"content":"<p>Basic audio context declaration:</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">const</span> audioContext <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<p>Cross browser variant:</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">const</span> AudioContext <span class=\"token operator\">=</span> window<span class=\"token punctuation\">.</span>AudioContext <span class=\"token operator\">||</span> window<span class=\"token punctuation\">.</span>webkitAudioContext<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> audioContext <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n<span class=\"token keyword\">const</span> oscillatorNode <span class=\"token operator\">=</span> audioContext<span class=\"token punctuation\">.</span><span class=\"token function\">createOscillator</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> gainNode <span class=\"token operator\">=</span> audioContext<span class=\"token punctuation\">.</span><span class=\"token function\">createGain</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> finish <span class=\"token operator\">=</span> audioContext<span class=\"token punctuation\">.</span>destination<span class=\"token punctuation\">;</span>\n</code></pre></div>"}},{"type":"specifications","value":{"title":"Specifications","id":"specifications","isH3":false,"specifications":[{"bcdSpecificationURL":"https://webaudio.github.io/web-audio-api/#BaseAudioContext","title":"Web Audio API"}],"query":"api.BaseAudioContext"}},{"type":"browser_compatibility","value":{"title":"Browser compatibility","id":"browser_compatibility","isH3":false,"query":"api.BaseAudioContext","dataURL":"/en-US/docs/Web/API/BaseAudioContext/bcd.json"}},{"type":"prose","value":{"id":"see_also","title":"See also","isH3":false,"content":"<ul>\n  <li><a href=\"/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API\">Using the Web Audio API</a></li>\n  <li><a href=\"/en-US/docs/Web/API/AudioContext\"><code>AudioContext</code></a></li>\n  <li><a href=\"/en-US/docs/Web/API/OfflineAudioContext\"><code>OfflineAudioContext</code></a></li>\n</ul>"}}],"toc":[{"text":"Properties","id":"properties"},{"text":"Methods","id":"methods"},{"text":"Examples","id":"examples"},{"text":"Specifications","id":"specifications"},{"text":"Browser compatibility","id":"browser_compatibility"},{"text":"See also","id":"see_also"}],"summary":"The BaseAudioContext interface of the Web Audio API acts as a base definition for online and offline audio-processing graphs, as represented by AudioContext and OfflineAudioContext respectively. You wouldn't use BaseAudioContext directly â€” you'd use its features via one of these two inheriting interfaces.","popularity":0.0011,"modified":"2022-09-09T05:07:36.000Z","other_translations":[{"title":"BaseAudioContext","locale":"es","native":"EspaÃ±ol"},{"title":"BaseAudioContext","locale":"fr","native":"FranÃ§ais"},{"title":"BaseAudioContext","locale":"ja","native":"æ—¥æœ¬èªž"},{"title":"BaseAudioContext","locale":"ko","native":"í•œêµ­ì–´"},{"title":"BaseAudioContext","locale":"zh-CN","native":"ä¸­æ–‡ (ç®€ä½“)"}],"source":{"folder":"en-us/web/api/baseaudiocontext","github_url":"https://github.com/mdn/content/blob/style/old/files/en-us/web/api/baseaudiocontext/index.md","last_commit_url":"https://github.com/mdn/content/commit/bf30e32f3b51f59080f2c76795beadb247a551eb","filename":"index.md"},"parents":[{"uri":"/en-US/docs/Web","title":"Web technology for developers"},{"uri":"/en-US/docs/Web/API","title":"Web APIs"},{"uri":"/en-US/docs/Web/API/BaseAudioContext","title":"BaseAudioContext"}],"pageTitle":"BaseAudioContext - Web APIs | MDN","noIndexing":false}}