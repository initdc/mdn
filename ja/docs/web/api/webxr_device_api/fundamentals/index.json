{"doc":{"isMarkdown":true,"isTranslated":true,"isActive":true,"flaws":{},"title":"WebXR の基礎","mdn_url":"/ja/docs/Web/API/WebXR_Device_API/Fundamentals","locale":"ja","native":"日本語","sidebarHTML":"<ol><li><strong><a href=\"/ja/docs/Web/API/WebXR_Device_API\">WebXR Device API</a></strong></li><li class=\"toggle\"><details open=\"\"><summary>ガイド</summary><ol><li><a href=\"/ja/docs/Web/API/WebXR_Device_API/Fundamentals\">Fundamentals of WebXR</a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>インターフェイス</summary><ol><li><a href=\"/ja/docs/Web/API/XRAnchor\"><code>XRAnchor</code></a></li><li><a href=\"/ja/docs/Web/API/XRBoundedReferenceSpace\"><code>XRBoundedReferenceSpace</code></a></li><li><a href=\"/ja/docs/Web/API/XRCPUDepthInformation\"><code>XRCPUDepthInformation</code></a></li><li><a href=\"/ja/docs/Web/API/XRDepthInformation\"><code>XRDepthInformation</code></a></li><li><a href=\"/ja/docs/Web/API/XRFrame\"><code>XRFrame</code></a></li><li><a href=\"/ja/docs/Web/API/XRInputSource\"><code>XRInputSource</code></a></li><li><a href=\"/ja/docs/Web/API/XRInputSourceArray\"><code>XRInputSourceArray</code></a></li><li><a href=\"/ja/docs/Web/API/XRInputSourceEvent\"><code>XRInputSourceEvent</code></a></li><li><a href=\"/ja/docs/Web/API/XRInputSourcesChangeEvent\"><code>XRInputSourcesChangeEvent</code></a></li><li><a href=\"/ja/docs/Web/API/XRPose\"><code>XRPose</code></a></li><li><a href=\"/ja/docs/Web/API/XRReferenceSpace\"><code>XRReferenceSpace</code></a></li><li><a href=\"/ja/docs/Web/API/XRReferenceSpaceEvent\"><code>XRReferenceSpaceEvent</code></a></li><li><a href=\"/ja/docs/Web/API/XRRenderState\"><code>XRRenderState</code></a></li><li><a href=\"/ja/docs/Web/API/XRRigidTransform\"><code>XRRigidTransform</code></a></li><li><a href=\"/ja/docs/Web/API/XRSession\"><code>XRSession</code></a></li><li><a href=\"/ja/docs/Web/API/XRSessionEvent\"><code>XRSessionEvent</code></a></li><li><a href=\"/ja/docs/Web/API/XRSpace\"><code>XRSpace</code></a></li><li><a href=\"/ja/docs/Web/API/XRSystem\"><code>XRSystem</code></a></li><li><a href=\"/ja/docs/Web/API/XRView\"><code>XRView</code></a></li><li><a href=\"/ja/docs/Web/API/XRViewerPose\"><code>XRViewerPose</code></a></li><li><a href=\"/ja/docs/Web/API/XRViewport\"><code>XRViewport</code></a></li><li><a href=\"/ja/docs/Web/API/XRWebGLBinding\"><code>XRWebGLBinding</code></a></li><li><a href=\"/ja/docs/Web/API/XRWebGLDepthInformation\"><code>XRWebGLDepthInformation</code></a></li><li><a href=\"/ja/docs/Web/API/XRWebGLLayer\"><code>XRWebGLLayer</code></a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>プロパティ</summary><ol><li><a href=\"/ja/docs/Web/API/Navigator/xr\"><code>Navigator.xr</code></a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>メソッド</summary><ol><li><a href=\"/ja/docs/Web/API/WebGLRenderingContext/makeXRCompatible\"><code>WebGLRenderingContext.makeXRCompatible()</code></a></li></ol></details></li><li class=\"toggle\"><details open=\"\"><summary>イベント</summary><ol><li><a href=\"/ja/docs/Web/API/XRReferenceSpace/reset_event\"><code>XRReferenceSpace</code>: <code>reset</code></a></li><li><a href=\"/ja/docs/Web/API/XRSession/end_event\"><code>XRSession</code>: <code>end</code></a></li><li><a href=\"/ja/docs/Web/API/XRSession/inputsourceschange_event\"><code>XRSession</code>: <code>inputsourceschange</code></a></li><li><a href=\"/ja/docs/Web/API/XRSession/select_event\"><code>XRSession</code>: <code>select</code></a></li><li><a href=\"/ja/docs/Web/API/XRSession/selectend_event\"><code>XRSession</code>: <code>selectend</code></a></li><li><a href=\"/ja/docs/Web/API/XRSession/selectstart_event\"><code>XRSession</code>: <code>selectstart</code></a></li><li><a href=\"/ja/docs/Web/API/XRSession/visibilitychange_event\"><code>XRSession</code>: <code>visibilitychange</code></a></li><li><a href=\"/ja/docs/Web/API/XRSystem/devicechange_event\"><code>XRSystem</code>: <code>devicechange</code></a></li></ol></details></li></ol>","body":[{"type":"prose","value":{"id":null,"title":null,"isH3":false,"content":"<p>WebXRは、 <a href=\"/ja/docs/Web/API/WebXR_Device_API\">WebXR 機器 API</a> を中核として、拡張現実と仮想現実（AR および VR）の両方をウェブ上で実現するために必要な機能を提供します。これらの技術を合わせて、<strong>複合現実 (MR)</strong> または<strong>クロス現実 (XR)</strong> と呼びます。複合現実は大規模で複雑なテーマであり、学ぶべきことが多く、また、ユーザーにとって魅力的な体験を生み出すために他の多くの API をまとめる必要があります。</p>\n<p>このガイドでは、 WebXR とは何か、どのように動作するかの概要と、ウェブ向けの拡張現実や仮想現実の体験の開発を始めるために必要な予備的な基礎知識を提供します。</p>"}},{"type":"prose","value":{"id":"webxr_とは何であり、何でないのか","title":"WebXR とは何であり、何でないのか","isH3":false,"content":"<p>WebXR は、ウェブコンテンツやアプリが、 VR ヘッドセットや AR ヘッドセットやメガネなどの複合現実ハードウェアとインターフェイスするために使用する API です。これには、三次元体験をシミュレートするために必要なビューのレンダリングプロセスの管理と、ヘッドセット（またはその他のモーションセンシング機器）の動きを感知して、ユーザーに表示する画像を更新するために必要なデータを提供する機能の両方が含まれます。</p>\n<p>WebXR はさらに、携帯型 VR コントローラーや特殊な複合現実ゲームパッドなどの制御機器からの入力を受け入れるための対応も行っています。</p>\n<p>WebXR はレンダリング技術ではないので、三次元データを管理したり、ディスプレイにレンダリングしたりする機能は提供しません。これは重要な事実として覚えておいてください。 WebXR はシーンを描画する際に関連するタイミングやスケジューリング、さまざまな視点を管理しますが、モデルをロードして管理する方法や、レンダリングしてテクスチャを貼る方法などはわかりません。その部分は完全にあなた次第です。幸いなことに、 WebGL と WebGL ベースのさまざまなフレームワークとライブラリーが用意されており、これらすべてを簡単に扱うことができます。</p>"}},{"type":"prose","value":{"id":"webxr_は_webvr_とどう違うのか","title":"WebXR は WebVR とどう違うのか","isH3":true,"content":"<p>WebVR は、仕様作成者がウェブ上で仮想現実 API を作成するための最適なアプローチを決定するために設計された実験的な API と見なされていました。ブラウザー実装者は WebVR のサポートをブラウザーに追加し、ウェブ開発者が実験を行えるようにしました。しかしすぐに、ウェブ上の仮想現実の API を完成させるには、 WebVR を「修正」しようとするよりも、新しい仕様を開始する方がより理にかなっていることが明らかになりました。</p>\n<p>それが WebXR の誕生につながりました。根本的な違いは、 WebXR は仮想現実だけでなく、仮想オブジェクトとユーザーの周辺環境を融合させる拡張現実にも対応していることです。</p>\n<p>もう一つの大きな違いは、 WebVR がコントローラーへの対応を<a href=\"/ja/docs/Web/API/Gamepad_API\">ゲームパッド API</a> に依存していたのに対し、 WebXR はほとんどの複合現実ヘッドセットで使用される高度な<a href=\"/en-US/docs/Web/API/WebXR_Device_API/Inputs\" class=\"only-in-en-us\" title=\"Currently only available in English (US)\">入力コントローラー (en-US)</a>の対応を統合していることも大きな違いです。WebXR では、主な選択アクションとスクイーズアクションはイベントを使用して直接対応し、その他の制御は <a href=\"/ja/docs/Web/API/Gamepad\"><code>Gamepad</code></a> オブジェクトの WebXR 固有の特別な実装を介して利用することができます。</p>"}},{"type":"prose","value":{"id":"基本概念","title":"基本概念","isH3":false,"content":"<p>XR のコードを開発する方法を学ぶ前に知っておくべき基本的な概念について考えてみましょう。</p>"}},{"type":"prose","value":{"id":"視野角","title":"視野角","isH3":true,"content":"<p><strong>視野角</strong> (<strong>FOV</strong>) とは、昔のフィルムカメラから現代のデジタルビデオカメラ、さらにはコンピューターやモバイル機器のカメラに至るまで、あらゆる映像技術に適用される用語です。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/WebXR_Device_API/Fundamentals/binocular-vision.svg\" alt=\"\" width=\"550\" height=\"720\" loading=\"lazy\">\n</p>\n<h4 id=\"視野角とは？\">視野角とは？</h4>\n<p>視野とは、周囲を見ることができる範囲のことです。視野の幅は、度またはラジアン単位で指定され、視野の左端から右端までの円弧を定義する角度として測定されます。</p>\n<p>人間の目は、約 135° の視野を取ることができます。健康な両目があると仮定すると、視野角は約 200° から 220° になります。なぜ、両眼で見ると視野が広くなるのに、単眼の 2 倍にはならないのでしょうか。それは、両眼の視野が大きく重なり合うからです。この重なりによって、奥行きを感じることができるのですが、その視野は約 115° です。重なり合った部分の外側では、単眼視に戻ります。</p>\n<p>この図は、 FOV の概念を示すもので、左目は青い三角形、右目は赤い三角形です。薄茶色の重なり合った部分が両眼視で、奥行きを感じることができる部分です。よく見ると、それぞれの目が微妙に異なる形で物体を見ており、その 2 つが合成されて三次元の形状になっていることがわかります。</p>\n<p>一般的に、アプリケーションは水平の視野角のみを定義し管理します。詳しくは、<a href=\"/ja/docs/Web/API/WebXR_Device_API/Rendering\">レンダリングと WebXR フレームアニメーションコールバック</a>のの<a href=\"/ja/docs/Web/API/WebXR_Device_API/Rendering#the_optics_of_3d\">三次元の光学</a>を参照してください。</p>\n<h4 id=\"視野角と複合現実機器\">視野角と複合現実機器</h4>\n<p>ユーザーの目が仮想世界が完全に自分を取り囲んでいると錯覚するほど広い視野を実現するには、視野角が少なくとも両眼視領域の幅に近づく必要があります。基本的なヘッドセットは通常 90° 前後から始まり、最高のヘッドセットでは一般的に 150° 前後の視野を持ちます。視野角はレンズの大きさとユーザーの目との距離の問題であるため、ユーザーの眼球にレンズを装着せずに視野角を広げるには限界があります。</p>\n<p>視野が広ければ、ユーザーの没入感を大幅に向上させることができます。しかし、視野角を広げると、その分、ヘッドセットの重量やコストが増加します。</p>"}},{"type":"prose","value":{"id":"自由度","title":"自由度","isH3":true,"content":"<p><strong>自由度</strong> (degrees of freedom) という用語は、ユーザーが仮想世界内でどの程度自由に動けるかを示すものです。これは、 WebXR のハードウェア構成が、どれだけの種類の動きを認識し、仮想シーンに再現することができるかに直接関係しています。</p>\n<p>\n  <strong>図: 3 軸の自由度を持つハードウェアで可能な動き（ヨー、ロール、ピッチ）を示した図</strong>\n  <img src=\"/en-US/docs/Web/API/WebXR_Device_API/Fundamentals/3-degrees-of-freedom-min.svg\" alt=\"3 軸の自由度を持つハードウェアで可能な動き（ヨー、ロール、ピッチ）を示した図\" width=\"918\" height=\"918\" loading=\"lazy\">\n</p>\n<h4 id=\"回転運動の自由度\">回転運動の自由度</h4>\n<p>最初の 3 つの自由度は <strong>回転の自由度</strong> です。回転の自由度は以下の通りです。</p>\n<ul>\n  <li>ピッチ (pitch): 見上げる・見下ろす</li>\n  <li>ヨー (yaw): 左右を向く</li>\n  <li>ロール (roll): 左右に傾ける</li>\n</ul>\n<p>これらのケースでは、ユーザーは空間の同じ場所に留まりながら、3つの軸のうちの1つまたは複数を回転させて、見ている方向を変えます。軸の自由度が2つのシステムは、ユーザーが左右や上下を見ることは感知できても、それ以外の動きを報告することはできません。</p>\n<p>一般的なベースラインのヘッドセットは、 3 軸の自由度を持ち、 3 軸すべてで回転を認識します。これは、よく <strong>3DoF</strong> という略語で呼ばれます。</p>\n<h4 id=\"平行移動の自由度\">平行移動の自由度</h4>\n<p>前後、左右、上下の 3 つの軸の自由度は、平行移動の自由度です。 6 つの自由度に対応していることを、 <strong>6DoF</strong> と呼びます。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/WebXR_Device_API/Fundamentals/xr-translation-headset.png\" alt=\"WebXR の設定で、 3 つの軸のそれぞれを中心に回転する様子を示した図\" width=\"640\" height=\"500\" loading=\"lazy\">\n</p>\n<p>より高度なヘッドセットでは、少なくとも平行運動の検出に最低限対応しているものもありますが、空間を通してより実質的な動きを捉えるには、通常、カメラ（可視光線または赤外線を使用）などの外部センサーが必要です。</p>"}},{"type":"prose","value":{"id":"webxr_セッションモード","title":"WebXR セッションモード","isH3":true,"content":"<p>WebXR は、同じ API を使用して、 AR （拡張現実）セッションと VR （仮想現実）セッションの両方に対応しています。どちらの種類のセッションを作成するかは、セッションを作成するときに指定します。これは、作成したいセッションの種類に応じた適切なセッションモード文字列を指定することで行われます。</p>\n<h4 id=\"仮想現実\">仮想現実</h4>\n<p>VR 環境では、前景のオブジェクトから背景やスカイボックスまで、画像全体がアプリやサイトによってデジタル処理で作成されます。フレーム描画コードは、残像が発生する可能性を避けるために、各フレームで各ビューのすべてのピクセルを再描画する必要があります。プラットフォームによっては、以前にクリアされたフレームを提供する場合もありますが、フレームごとに各ピクセルを 2 回タッチする必要がないように、フレームバッファーを消去しないようにして、パフォーマンスを最適化する場合もあります。</p>\n<p>WebXR では、2 つの VR セッションモードが利用可能です。<strong>インライン</strong>と<strong>没入型</strong>です。前者はセッションモード文字列 <code>inline</code> で指定され、ウェブブラウザーの文書のコンテキスト内でレンダリングシーンを表示し、表示するために特別な XR ハードウェアは必要ありません。没入型セッションモードは、セッションモード <code>immersive-vr</code> で示されます。このセッションモードは、ヘッドセットなどの XR 機器を必要とし、ユーザーのそれぞれの目に表示されるディスプレイを使用して、世界全体をレンダリングシーンで置き換えます。</p>\n<h4 id=\"拡張現実\">拡張現実</h4>\n<p>拡張現実 (AR) では、ユーザーは自分の周りの物理的な現実環境の上に表示されたレンダリングイメージを見ることになります。 AR は常に没入型の体験であり、シーンは（画面上の箱に囲まれているのではなく）ユーザーの周りの世界全体であるため、 AR セッションモードは <code>immersive-ar</code> のみです。</p>\n<p>AR 機器には、基本的に 2 つの種類があります。</p>\n<ul>\n  <li>カメラでユーザーの目の前の世界を撮影し、その画像の上に WebXR コンテンツを描画し、画面に画像を表示する機器です。端末の画面上に 2D で表示する携帯電話や、両眼のカメラでステレオ撮影し、奥行きを確保した上で、両眼の背景を撮影した WebXR シーンを描画するゴーグルなどがこれにあたります。</li>\n  <li>透明なメガネを使い、レンダリング画像を重ね合わせながら世界を見ることができる機器。ユーザーは、デジタル写真の羅列ではなく、現実の世界を直接見ることができます。</li>\n</ul>\n<p>どちらの種類の機器も、 VR セッションを表示することが可能であるはずです。 WebXR は、一般に、使用する機器の種類を問いません。レンダリング処理は、各フレームをレンダリングする前に背景やスカイボックスを消去しないことを除いて、VRの場合とほとんど同じです。</p>"}},{"type":"prose","value":{"id":"webxr_ハードウェアの種類","title":"WebXR ハードウェアの種類","isH3":false,"content":"<p>最もシンプルな XR の表現は、シーンをユーザーの画面に直接レンダリングするもので、ウェブ文書のコンテキスト内、または全画面モードのいずれかになります。これは、ユーザーが専用の XR 機器を持っていない場合や、電話などの携帯端末で AR や VR のアプリを閲覧している場合によく見られる方法です。</p>\n<p>よりシンプルで低価格の XR 機器は、通常、内蔵コンピュータを使用するか、スマートフォンに接続し、基本的にモバイル CPU と GPU を使用してアプリを実行し、画像をレンダリングしてユーザーに表示します。より高性能なソリューションでは、アプリケーションの実行とグラフィック処理をデスクトップコンピューターなどの外部機器に任せて、ケーブルを使ってコンピューターに接続するか、無線ネットワークを使って画像を受信し、ユーザーに表示します。</p>"}},{"type":"prose","value":{"id":"ヘッドセット","title":"ヘッドセット","isH3":true,"content":"<p>没入型 VR 体験の多くは、ゴーグルや何らかのヘッドセットを使って行われます。 VR ヘッドセットは頭に装着し、ストラップで頭の後ろを固定し、 1～2 台のディスプレイの画面をレンズで目に映します。それぞれの目に微妙に異なる画像を表示することで、奥行きを感じさせ、三次元を疑似体験することができます。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/WebXR_Device_API/Fundamentals/publicdomainq-virtual_reality_headset.svg\" alt=\"標準的な VR ヘッドセットのイメージ図\" width=\"810\" height=\"607\" loading=\"lazy\">\n</p>\n<p>大半のヘッドセットでは、フレームが半分に分割された単一のディスプレイを使用し、その半分がユーザーのそれぞれの目に焦点を合わせています。例えば、ヘッドセットが 2560 x 1440 の画面を使用し、左半分が左目の表示に、右半分が右目の表示に使われる場合、フレームバッファーは次のように使用されます。</p>\n<p>\n  <img src=\"/ja/docs/Web/API/WebXR_Device_API/Fundamentals/twoviewsoneframebuffer.svg\" alt=\"フレームバッファーを両眼の視点で分割する様子を示した図\" width=\"751\" height=\"504\" loading=\"lazy\">\n</p>\n<p>最も単純なヘッドセットは、統合されたセンサーを持たず、スクリーンの各半分を対応する目に焦点を合わせます。この代表的な例が <a href=\"https://arvr.google.com/cardboard/\" class=\"external\" rel=\" noopener\">Google Cardboard</a>、 Google が最初に作ったヘッドセットの一種で、段ボールやその他の安価な材料を使って安価に作ることができます。これらの機器は、多くの場合、携帯電話をヘッドセットにはめ込み、その画面とオンボードのグラフィックプロセッサーを使用して、 XR シーンをレンダリングして表示することで動作します。</p>\n<p>より高度なヘッドセットにはディスプレイが内蔵されており、ゴムやストラップ、またはマジックテープの留め具を使って頭に装着します。 これらのヘッドセットは、統合されたスピーカーとマイク、および/または外部のものを取り付けるためのコネクターを含む場合があります。さらに、これらのヘッドセットは、ヘッドセットが空間内を移動するときに検出するための様々なセンサーを有していてもよい。含まれるセンサーの型と数によって、ユーザーが持つ<a href=\"#%E8%87%AA%E7%94%B1%E5%BA%A6\">自由度</a>の数が決定されます。</p>"}},{"type":"prose","value":{"id":"ゴーグルとメガネ","title":"ゴーグルとメガネ","isH3":true,"content":"<p>XR ゴーグルは、シミュレーションシーンの奥行きを再現するために必要なシーンのビューをレンダリングするために、グラフィック表示面を目の前に置くという点で、ヘッドセットと似ています。</p>\n<p>しかし、ゴーグルは現実の世界を通過し、ユーザーの物理的な環境の上にレンダリングされた画像を重ね合わせるという違いがあります。これは、完全なヘッドセットで必要とされるような、世界をデジタルで再現することなく行われます。その代わり、ディスプレイの表面は透明で、何も表示していなければ、通常のメガネをかけているのと基本的に同じです。オブジェクトが描画されると、ゴーグルのレンズ上に描画され、その部分から物理的な環境が見えなくなります。</p>"}},{"type":"prose","value":{"id":"caves","title":"CAVEs","isH3":true,"content":"<p><strong>Cave Automated Virtual Environment</strong> (<strong>CAVE</strong>) は没入型VR環境で、壁（場合によっては天井や床も）にシーンが投影または表示されるため、ユーザーはシミュレーションに完全に囲まれ、シーンに没入することができます。ユーザーは三次元メガネを着用し、投影された画像に三次元効果を加えるとともに、システムが前景のオブジェクトを世界にレンダリングするための手段を提供します。</p>\n<p>ユーザーの行動は、ユーザーが身につけたり、手に持ったりするモーションセンサーや、最近では赤外線カメラで検知してモニターすることもあります。また、室内のあちこちに設置されたスピーカーからは、臨場感あふれるサウンドが聞こえてきます。</p>\n<p>これらは、日常的に利用されるものではなく、実験的なもの、デモンストレーションのためのもの、あるいは大きな組織で利用されるものがほとんどです。欠点としては、 CAVE では壁より近いものをシミュレーションすることができないことです。</p>"}},{"type":"prose","value":{"id":"健康と安全に関する重要な注意事項","title":"健康と安全に関する重要な注意事項","isH3":false,"content":"<p>仮想の三次元の世界を作るという行為は、要するに、目が光を集める仕組みと、集めたデータを脳が解釈する仕組みを利用したものですから、ソフトウェア設計者や開発者は、その結果が正しいかどうか、いつも以上に注意する責任があることを心に留めておくことが大切です。</p>"}},{"type":"prose","value":{"id":"vr_酔い","title":"VR 酔い","isH3":true,"content":"<p><a title=\"VR 酔い\" href=\"https://ja.wikipedia.org/wiki/VR酔い\" class=\"external\" rel=\" noopener\">VR 酔い</a>とは、仮想現実を体験した人が、体験中や体験後しばらくの間、不快感や方向感覚の喪失、あるいは深刻な吐き気などを感じる症状のことです。</p>\n<p>仮想現実の何が不快感や気持ち悪さを引き起こすのかについては、さまざまな説がありますが、その多くは、脳が考えていることと見ていることの間に微妙な違いがあっても、こうした症状を引き起こすという考え方に重点を置いています。</p>\n<p>欠陥やズレ、歪みがあると、目と脳が混乱し、目の痛みや頭痛から、場合によってはめまいや激しい吐き気まで、さまざまな症状を引き起こす可能性があります。また、ヘッドセットの包括的な性質を考えると、発作を誘発する可能性のあるものを表示しないように注意することが重要です。ユーザーは、苦痛を与えるような画像を表示された場合、すぐに目をそらすことができないかもしれません。</p>"}},{"type":"prose","value":{"id":"物理的なリスク","title":"物理的なリスク","isH3":true,"content":"<p>没入型仮想現実のもう一つの潜在的な問題は、ユーザーがヘッドセットを装着したまま部屋の中を移動した場合、物理的な障害物に衝突してしまうことです。安全な環境でない限り、物理的な環境の中で安全だとわかっている空間をシミュレーションするなど、ユーザーの動きを制限する手がかりを提供することが重要です。</p>\n<p>ユーザーのヘッドセットが機器につながれている場合、ユーザーがヘッドセットのコードを引っ張ったり引っ張ったりするような動きを促したり誘惑したりしないようにするのがよいでしょう。これは、怪我を引き起こすだけでなく、ユーザーのヘッドセットまたは機器（電話かコンピュータかにかかわらず）に大きな損傷を与える可能性があります。</p>\n<p>どのようなユーザーにとっても危険な状況になる可能性があるコンテンツがある場合は、警告メッセージを表示する必要があります。同様に、可能であれば座ったままでいること、完全没入型バーチャルリアリティを体験する場合は、ヘッドセットを装着したまま動き回ることに注意するよう、ユーザーに注意を促すとよいでしょう。後悔するよりも、安全であることが一番です。</p>"}},{"type":"prose","value":{"id":"フレームワークの役割","title":"フレームワークの役割","isH3":false,"content":"<p>三次元グラフィック、特に複合現実には、複雑な数学、データ管理、その他の複雑なタスクが多く含まれるため、ほとんどの場合、シーンのレンダリングに WebGL を直接使用することはないでしょう。その代わりに、WebGL の上に構築されたフレームワークや ライブラリーを使用して、より便利に作業を行うことが多いでしょう。</p>\n<p>WebGL API を直接使用するのではなく、フレームワークを使用することの利点は、ライブラリが仮想カメラ機能を実装している傾向があることです。OpenGL （およびその拡張である WebGL）はカメラビューを直接提供しませんが、代わりにカメラビューをシミュレーションするライブラリーを使用すれば、特に仮想世界を自由に移動できるコードを構築するときに、作業がずっとずっと楽になります。</p>\n<p><a href=\"/ja/docs/Web/API/WebGL_API\">WebGL</a> は、WebXR セッションに三次元の世界をレンダリングするために使用されるので、まず、WebGL の一般的な使用法と三次元グラフィックの基本についてよく理解しておく必要があります。</p>"}},{"type":"prose","value":{"id":"汎用三次元フレームワーク","title":"汎用三次元フレームワーク","isH3":true,"content":"<p>これらのフレームワークは、汎用的なプログラミングや、ロジックを自分で作りたい場合のゲーム開発に向いています。三次元シーンの作成やアニメーションを、文脈に関係なく行うことができるように設計されています。</p>\n<ul>\n  <li><a href=\"https://aframe.io/\" class=\"external\" rel=\" noopener\">A-Frame</a> （特に WebXR ベースのアプリを作成するために設計されています）</li>\n  <li><a href=\"https://www.babylonjs.com/\" class=\"external\" rel=\" noopener\">Babylon.js</a></li>\n  <li><a href=\"https://threejs.org/\" class=\"external\" rel=\" noopener\">Three.js</a></li>\n</ul>"}},{"type":"prose","value":{"id":"ゲームのツールキット","title":"ゲームのツールキット","isH3":true,"content":"<p>ゲームのツールキットは、ゲーム開発者向けに設計されており、物理モデル、入力制御システム、アセット管理、三次元サウンド再生など、ゲームに特化した機能が含まれていることが多いです。</p>\n<ul>\n  <li><a href=\"https://playcanvas.com/\" class=\"external\" rel=\" noopener\">PlayCanvas</a></li>\n</ul>"}},{"type":"prose","value":{"id":"次のステップ","title":"次のステップ","isH3":false,"content":"<p>これらの基本情報が分かったら、複合現実の世界へ次のステップを踏み出す準備が整いました。以下の記事が参考になります。</p>\n<ul>\n  <li><a href=\"/en-US/docs/Web/API/WebXR_Device_API/Lifecycle\" class=\"only-in-en-us\" title=\"Currently only available in English (US)\">WebXR アプリケーションのライフサイクル (en-US)</a></li>\n  <li><a href=\"/ja/docs/Web/API/WebXR_Device_API/Startup_and_shutdown\">WebXR セッションの起動と終了</a></li>\n  <li><a href=\"/ja/docs/Web/API/WebXR_Device_API/Movement_and_motion\">移動、向き、動作: WebXR の例</a></li>\n</ul>"}}],"toc":[{"text":"WebXR とは何であり、何でないのか","id":"webxr_とは何であり、何でないのか"},{"text":"基本概念","id":"基本概念"},{"text":"WebXR ハードウェアの種類","id":"webxr_ハードウェアの種類"},{"text":"健康と安全に関する重要な注意事項","id":"健康と安全に関する重要な注意事項"},{"text":"フレームワークの役割","id":"フレームワークの役割"},{"text":"次のステップ","id":"次のステップ"}],"summary":"WebXRは、 WebXR 機器 API を中核として、拡張現実と仮想現実（AR および VR）の両方をウェブ上で実現するために必要な機能を提供します。これらの技術を合わせて、複合現実 (MR) またはクロス現実 (XR) と呼びます。複合現実は大規模で複雑なテーマであり、学ぶべきことが多く、また、ユーザーにとって魅力的な体験を生み出すために他の多くの API をまとめる必要があります。","popularity":0,"modified":"2022-10-01T03:41:16.000Z","other_translations":[{"title":"Fundamentals of WebXR","locale":"en-US","native":"English (US)"}],"source":{"folder":"ja/web/api/webxr_device_api/fundamentals","github_url":"https://github.com/mdn/translated-content/blob/main/files/ja/web/api/webxr_device_api/fundamentals/index.md","last_commit_url":"https://github.com/mdn/translated-content/commit/921c46a374ab0a9f4cc809af0370f8c412e54701","filename":"index.md"},"parents":[{"uri":"/ja/docs/Web","title":"開発者向けのウェブ技術"},{"uri":"/ja/docs/Web/API","title":"Web API"},{"uri":"/ja/docs/Web/API/WebXR_Device_API","title":"WebXR Device API"},{"uri":"/ja/docs/Web/API/WebXR_Device_API/Fundamentals","title":"WebXR の基礎"}],"pageTitle":"WebXR の基礎 - Web API | MDN","noIndexing":false}}