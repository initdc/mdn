{"doc":{"isMarkdown":true,"isTranslated":true,"isActive":true,"flaws":{},"title":"Basic concepts behind Web Audio API","mdn_url":"/ja/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","locale":"ja","native":"日本語","sidebarHTML":"","body":[{"type":"prose","value":{"id":null,"title":null,"isH3":false,"content":"<p>この記事では、アプリを経由した音声伝達方法の設計をする際に、十分な情報に基づいた決断をする助けとなるよう、 Web Audio API の特徴がいかに働いているか、その背後にある音声理論について説明します。この記事はあなたを熟練のサウンドエンジニアにさせることはできないものの、Web Audio API が動く理由を理解するための十分な背景を提供します。</p>"}},{"type":"prose","value":{"id":"オーディオグラフ","title":"オーディオグラフ","isH3":false,"content":"<p><a href=\"/ja/docs/Web/API/Web_Audio_API\">Web Audio API</a> は、<a href=\"/ja/docs/Web/API/AudioContext\">オーディオコンテキスト</a>内部でのオーディオオペレーションに対するハンドリングを含み、<strong>モジュラールーティング</strong>を可能とするためにデザインされてきました。基本となる音声操作を成す要素は、<strong>オーディオルーチングラフ</strong>を形作るために、互いに接続される<strong>オーディオノード</strong>によります。様々なタイプのチャンネルレイアウトと共に、いくつかの音声源もまた、単一のコンテキスト内部にて支持されています。このモジュラーデザインは、複雑なオーディオ機能を、ダイナミックな音響効果と共に創造するために必要な柔軟性を与えています。</p>\n<p>オーディオノードは、それら入出力を経由し接続され、単一あるいは複数の音声源から開始される一連のチェーンを作り、一つ以上のノードを経由しつつ、最終的な行き先に届き終了します。ただし、たとえば、音声データを視覚化することのみを求める場合などは、このような目的地は省いて構いません。シンプルで典型的な Web Audio ワークフローでは、以下のようになります:</p>\n<ol>\n  <li>オーディオコンテキストの作成。</li>\n  <li><a href=\"/ja/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a> タグ、オシレーター、ストリーム等といった、該当コンテキスト内での音声源の作成。</li>\n  <li>リバーブ、バイカッドフィルター、パンナーコンプレッサーといった、音響効果用ノードの作成。</li>\n  <li>例えばあなたのシステムのスピーカーなど、音声の最終的な行き先の選択。</li>\n  <li>音声効果を（あるのならば）かけた後、最後に選択した行き先へ届いて終了する、音声源からの接続の確立。</li>\n</ol>\n<p><strong>チャンネルの記法</strong></p>\n<p>信号上で利用できるオーディオチャンネルの数字は、2.0 や 5.1 のように、しばしば、数値の形式で表現されます。これは<a title=\"channel notation\" href=\"https://ja.wikipedia.org/wiki/Surround_sound#Channel_notation\" class=\"external\" rel=\" noopener\">channel notation</a>と呼ばれます。最初の数値は、該当の信号が含んでいるオーディオチャンネルの数です。ピリオドの後にある数値は、低音増強用出力として確保されているチャンネルの数を示しています。それらはしばしば<strong>サブウーファー</strong>とも称されます。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudioAPI_en.svg\" alt=\"A simple box diagram with an outer box labeled Audio context, and three inner boxes labeled Sources, Effects and Destination. The three inner boxes have arrow between them pointing from left to right, indicating the flow of audio information.\" width=\"643\" height=\"143\" loading=\"lazy\">\n</p>\n<p>各入出力は、互いに特定のオーディオレイアウトを代表する、一つ以上のオーディオ<strong>チャンネル</strong>により構成されます。個別のチャンネル構造それぞれは、モノラル、ステレオ、クアッド、5.1 等を含んでサポートされています。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/mdn.png\" alt=\"Show the ability of AudioNodes to connect via their inputs and outputs and the channels inside these inputs/outputs.\" width=\"1308\" height=\"750\" loading=\"lazy\">\n</p>\n<p>音声源はいくつかの方法で取得できます:</p>\n<ul>\n  <li>JavaScript 内部で(オシレーターのような)オーディオノードにより、直接音声を生成。</li>\n  <li>未加工の PCM データから生成(この場合、該当オーディオコンテキストは、対応している音声フォーマット形式へのデコード手段を有しています)。</li>\n  <li>(<a href=\"/ja/docs/Web/HTML/Element/video\"><code>&lt;video&gt;</code></a> や <a href=\"/ja/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a> のような)HTML media elements より取得。</li>\n  <li>(webcam やマイクロフォンのような)<a href=\"/ja/docs/Web/API/WebRTC_API\">Web RTC</a> <a href=\"/ja/docs/Web/API/MediaStream\"><code>MediaStream</code></a> により直接取得。</li>\n</ul>"}},{"type":"prose","value":{"id":"オーディオデータ_サンプルとは？","title":"オーディオデータ: サンプルとは？","isH3":false,"content":"<p>オーディオシグナルのプロセス中では、<strong>サンプリング</strong>とは<a href=\"https://en.wikipedia.org/wiki/Discrete_time_and_continuous_time\" class=\"external\" rel=\" noopener\">連続的な信号</a>を<a href=\"Discrete%20time%20and%20continuous%20time\">離散的な信号</a>へ変換することを意味します。あるいは別の言い方をすると、バンドのライブ演奏のような連続的な音波を、コンピューターがその音声を区別されたブロックで処理できるようになる(離散時間信号の)一連のサンプルに変換します。</p>\n<p>より詳しい情報は、Wikipedia の <a href=\"https://ja.wikipedia.org/wiki/%E6%A8%99%E6%9C%AC%E5%8C%96\" class=\"external\" rel=\" noopener\">Sampling(Signal processing)</a>から見ることができます。</p>"}},{"type":"prose","value":{"id":"オーディオバッファー_フレーム、サンプル、チャンネルセクション","title":"オーディオバッファー: フレーム、サンプル、チャンネルセクション","isH3":false,"content":"<p><a href=\"/ja/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a>は、チャンネルの数(モノラルの場合は 1,ステレオの場合は 2、等)、バッファー長すなわちバッファーに格納されているサンプルフレーム数、サンプルレートすなわち 1 秒間に再生されるサンプルフレーム数を、そのパラメーターとして持っています。</p>\n<p>例として、float32 型の値 1 つは、(ステレオの場合は左側あるいは右側といった)特定のチャンネルにおいて、各特定時点おけるオーディオストリームの値を表しています。フレームまたはサンプルフレームは、一定時点における、音声を再生する全チャンネル分の値すべてのまとまりです。全チャンネルの全サンプルが一緒に再生されます(ステレオならば 2 チャンネル分、5.1 ならば 6 チャンネル分、等)。</p>\n<p>サンプルレートとは、1 秒間に再生される、それらサンプルの数(または、1 フレームのサンプルすべてが同時に再生させることから、フレーム)であり、単位は Hz で測定されます。サンプルレートが高まるにつれ、音質は向上します。</p>\n<p>モノラルとステレオのオーディオバッファーを見てみましょう。それらは 1 秒間、44100Hz で再生されます。</p>\n<ul>\n  <li>モノラルバッファーは 44100 サンプル、44100 フレームで構成され、length プロパティは 44100 となる。</li>\n  <li>ステレオバッファーは 88200 サンプル、44100 フレームで構成され、length プロパティは、そのフレーム数に等しいため 44100 のままである。</li>\n</ul>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/sampleframe-english.png\" alt=\"A diagram showing several frames in an audio buffer in a long line, each one containing two samples, as the buffer has two channels, it is stereo.\" width=\"1245\" height=\"219\" loading=\"lazy\">\n</p>\n<p>バッファが再生されると、最も左のサンプルフレームが聞こえ、次にそのサンプルフレームのすぐ隣のサンプルフレームが続いてゆきます。ステレオの場合、両方のチャンネルから同時に聴こえます。サンプルフレームは、チャンネルの数とは独立しているため非常に便利であり、正確に音声を取り扱う有効な手段として、時間を表現してくれます。</p>\n<div class=\"notecard note\" id=\"sect1\">\n  <p><strong>Note:</strong> フレーム数から秒数を得るためには、フレーム数をサンプルレートで単に除算するだけです。サンプル数からフレーム数を得るためには、チャンネル数で単に除算するだけです。</p>\n</div>\n<p>以下はいくつかの単純なサンプルです:</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">var</span> context <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">var</span> buffer <span class=\"token operator\">=</span> context<span class=\"token punctuation\">.</span><span class=\"token function\">createBuffer</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span> <span class=\"token number\">44100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<div class=\"notecard note\" id=\"sect2\">\n  <p><strong>Note:</strong> <a href=\"https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%B8%E3%82%BF%E3%83%AB%E3%82%AA%E3%83%BC%E3%83%87%E3%82%A3%E3%82%AA\" class=\"external\" rel=\" noopener\">デジタルオーディオに</a>おいて、<strong>44100</strong><a href=\"https://ja.wikipedia.org/wiki/%E3%83%98%E3%83%AB%E3%83%84\" class=\"external\" rel=\" noopener\">Hz</a>(<strong>44.1kHz</strong> とも表記される)は一般的な<a href=\"https://ja.wikipedia.org/wiki/%E6%A8%99%E6%9C%AC%E5%8C%96\" class=\"external\" rel=\" noopener\">サンプリング周波数</a>です。なぜ 44.1kHz なのでしょう？</p>\n  <p>第一に、人間の耳の<a href=\"https://en.wikipedia.org/wiki/Hearing_range\" class=\"external\" rel=\" noopener\">可聴範囲</a>は、大雑把に 20 から 20000Hz の範囲です。<a href=\"https://ja.wikipedia.org/wiki/%E6%A8%99%E6%9C%AC%E5%8C%96%E5%AE%9A%E7%90%86\" class=\"external\" rel=\" noopener\">Nyquist-Shannon のサンプリング定理</a>により、サンプリング周波数は再現したい最大周波数の 2 倍以上でなくてはなりません。したがって、サンプリングレートは 40kHz 以上でなくてはなりません。</p>\n  <p>第二に、シグナルはサンプリング前に、<a href=\"https://ja.wikipedia.org/wiki/%E6%8A%98%E3%82%8A%E8%BF%94%E3%81%97%E9%9B%91%E9%9F%B3\" class=\"external\" rel=\" noopener\">偽信号</a>の発生をさせないため、<a href=\"https://ja.wikipedia.org/wiki/%E3%83%AD%E3%83%BC%E3%83%91%E3%82%B9%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF\" class=\"external\" rel=\" noopener\">ローパスフィルタリング</a>されていなければなりません。理想的ローパスフィルターが 20kHz 以下の周波数を(減衰させずに)完璧に通し、20kHz 以上の周波数を完璧に遮断する一方で、実際には、周波数が部分的に減衰する場所となる、<a href=\"https://en.wikipedia.org/wiki/Transition_band\" class=\"external\" rel=\" noopener\">トランジションバンド</a>が必要です。このバンドが広くなるにつれ、<a href=\"https://en.wikipedia.org/wiki/Anti-aliasing_filter\" class=\"external\" rel=\" noopener\">減衰フィルター</a>を作るのは簡単かつ効率的になります。44.1kHz サンプリング周波数は、2.05kHz のトランジションバンドを与えます。</p>\n</div>\n<p>この呼び出しをする場合、チャンネル数 2 のステレオバッファーを取得し、AudioContext 上で(非常に一般的で、通常のサウンドカードではほとんどはレートとなる)44100Hz にて再生される音源が、0.5 秒間続きます: 22050 フレーム/44100Hz = 0.5 秒。</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">var</span> context <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">var</span> buffer <span class=\"token operator\">=</span> context<span class=\"token punctuation\">.</span><span class=\"token function\">createBuffer</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<p>この呼び出しをする場合、モノラルバッファーをチャンネル数 1 で取得し、AudioContext 上で 44100Hz にて再生される際に自動で 44100Hz へ<em>再サンプリングされ</em>(したがって 44100 フレームとなり)、1 秒間続きます: 44100 フレーム/44100Hz = 1 秒。</p>\n<div class=\"notecard note\" id=\"sect3\">\n  <p><strong>Note:</strong> オーディオの再サンプリングは、画像のサイズ変更と非常に似たものです。例えば 16x16 の画像があり、32x32 のスペースを満たしたいとします。サイズ変更(あるいは再サンプリング)を行い、結果として(サイズ変更アルゴリズムの違いに依存して、ぼやけたりエッジができたりと)画質の低下を伴いますが、スペースを減らすサイズ変更済み画像が作れます。再サンプリングされたオーディオもまったく同じです。スペースを保てるものの、実際には高音域のコンテンツや高音を適切に再現することはできません。</p>\n</div>"}},{"type":"prose","value":{"id":"バッファーセクションの平面性対交差性","title":"バッファーセクションの平面性対交差性","isH3":true,"content":"<p>Web AudioAPI は平面的なバッファー形式を採用しています。左右のチャンネルは、以下のように格納されています:</p>\n<pre class=\"notranslate\">LLLLLLLLLLLLLLLLRRRRRRRRRRRRRRRR (for a buffer of 16 frames)\n</pre>\n<p>これは音声処理における一般的な形式です: これにより各チャンネルの独立した処理が簡単になります。</p>\n<p>代わりの方式としては、交差的な形式が用いられます:</p>\n<pre class=\"notranslate\">LRLRLRLRLRLRLRLRLRLRLRLRLRLRLRLR (for a buffer of 16 frames)\n</pre>\n<p>この形式は、大掛かりな処理を必要としない音声を、格納し再生する目的として一般的です。例えばデコード済み MP3 音楽ストリームなどがあります。</p>\n<p>Web Audio API は、音声処理に適することを理由に、平面的なバッファー<strong>のみ</strong>を採用しています。通常は平面的ですが、再生用に音声がサウンドカードへ送られる際は、交差的に変換されます。反対に、MP3 音声がデコードされる場合、元々は交差形式だったものの、音声処理のため平面形式へと変換されます。</p>"}},{"type":"prose","value":{"id":"オーディオチャンネル","title":"オーディオチャンネル","isH3":false,"content":"<p>異なるオーディオバッファーでは、それぞれ異なった数のチャンネルを含んでいます: より基本的なモノラルとステレオ(それぞれチャンネル数 1 と 2)から始まり、より複雑なクアッドもしくは 5.1 のような、異なるサウンドサンプルが各チャンネルに含まれ、よりリッチな音声体験を導くセットもあります。チャンネルは通常、以下のテーブルに示される、標準的な略語によって示されます:</p>\n<table>\n  <thead>\n    <tr>\n      <th><em>Mono</em></th>\n      <th><code>0: M: mono</code></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><em>Stereo</em></td>\n      <td><code>0: L: left 1: R: right</code></td>\n    </tr>\n    <tr>\n      <td><em>Quad</em></td>\n      <td><code>0: L: left 1: R: right 2: SL: surround left 3: SR: surround right</code></td>\n    </tr>\n    <tr>\n      <td><em>5.1</em></td>\n      <td><code>0: L: left 1: R: right 2: C: center 3: LFE: subwoofer 4: SL: surround left 5: SR: surround right</code></td>\n    </tr>\n  </tbody>\n</table>"}},{"type":"prose","value":{"id":"アップミキシングとダウンミキシング","title":"アップミキシングとダウンミキシング","isH3":true,"content":"<p>入力と出力のチャンネル数が一致しない場合、以下のルールに基づいてアップミキシングまたはダウンミキシングが行われます。これは<a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/AudioNode/channelInterpretation\"><code>AudioNode.channelInterpretation</code> <small>(en-US)</small></a> プロパティを <code>speakers</code> または <code>discrete</code> へとセットしてコントロールできます。</p>\n<table class=\"standard-table\">\n  <thead>\n    <tr>\n      <th scope=\"row\">説明</th>\n      <th scope=\"col\">入力チャンネル</th>\n      <th scope=\"col\">出力チャンネル</th>\n      <th scope=\"col\">ミキシングルール</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th colspan=\"1\" rowspan=\"13\" scope=\"row\"><code>speakers</code></th>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>モノラルからステレオへのアップミックス。</em><br><code>M</code>入力チャンネルは(<code>L</code> と\n        <code>R</code> の)両出力チャンネル用に使われます。<br><code>output.L = input.M<br>output.R = input.M</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>モノラルからクアッドへのアップミックス。</em><br><code>M</code>\n        入力チャンネルは(<code>L</code> と\n        <code>R</code> の)ノンサラウンド出力チャンネル用に使われます。(<code>SL</code>\n        と <code>SR</code> の)サラウンド出力チャンネルは無音です。<br><code>output.L = input.M<br>output.R = input.M<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>モノラルから 5.1.へのアップミックス。</em><br><code>M</code>\n        入力チャンネルは(<code>C</code>\n        の)ノン中央出力チャンネル用に使われます。その他すべての(<code>L</code>,\n        <code>R</code>, <code>LFE</code>, <code>SL</code>,\n        <code>SR</code>)出力チャンネルは無音です。<br><code>output.L = 0<br>output.R = 0</code><br><code>output.C = input.M<br>output.LFE = 0<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>モノラルからステレオへのダウンミックス。</em><br>(<code>L</code>\n        と\n        <code>R</code>\n        の)両入力チャンネルは等しく結合され、単一出力チャンネル(<code>M</code>)になります。<br><code>output.M = 0.5 * (input.L + input.R)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>ステレオからクアッドへのアップミックス。</em><br>左右(<code>L</code>\n        と <code>R</code>)入力チャンネルはそれぞれ(<code>L</code> と\n        <code>R</code> の)ノンサラウンド出力チャンネル用に使われます。(<code>SL</code>\n        と <code>SR</code> の)サラウンド出力チャンネルは無音です。<br><code>output.L = input.L<br>output.R = input.R<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>ステレオから 5.1.へのアップミックス。</em><br>左右(<code>L</code>\n        と <code>R</code>)入力チャンネルはそれぞれ(<code>L</code> と\n        <code>R</code> の)ノンサラウンド出力チャンネル用に使われます。(<code>SL</code>\n        と\n        <code>SR</code>\n        の)サラウンド出力チャンネル、中央チャンネル(<code>C</code>)、サブウーファー(<code>LFE</code>)はすべて同様に無音です。<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = 0<br>output.SR = 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>クアッドからモノラルへのダウンミックス。</em><br>(<code>L</code>,\n        <code>R</code>, <code>SL</code>,\n        <code>SR</code>)入力チャンネルは等しく結合され、単一出力チャンネル(<code>M</code>)になります。<br><code>output.M = 0.25 * (input.L + input.R + input.SL + input.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>クアッドからステレオへのダウンミックス。両左入力チャンネル</em><br>(<code>L</code> と\n        <code>SL</code>)は等しく結合され、単一左出力チャンネル(<code>L</code>)になります。同様に、<em>両右入力チャンネル</em>(<code>R</code>\n        と\n        <code>SR</code>)は等しく結合され、単一右出力チャンネル(<code>R</code>)になります。<br><code>output.L = 0.5 * (input.L + input.SL</code><code>)</code><br><code>output.R = 0.5 * (input.R + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>クアッドから 5.1.へのアップミックス。</em><br>(<code>L</code>,\n        <code>R</code>, <code>SL</code>,\n        <code>SR</code>)入力チャンネルはそれぞれ(<code>L</code> と\n        <code>R</code>\n        の)出力チャンネル用に使われます。中央チャンネル(<code>C</code>)、サブウーファー(<code>LFE</code>)は無音です。<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = input.SL<br>output.SR = input.SR</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>5.1.からモノラルへのダウンミックス。左チャンネル</em>(<code>L</code>, <code>SL</code>)、右チャンネル(<code>R</code>,\n        <code>SR</code>)、中央チャンネルはそれぞれ混合されます。サラウンドチャンネルは僅かに減衰され、標準側面チャンネルは利用のために、単一チャンネルを<code>√2/2</code>\n        で乗算したものとして出力が補強されます。サブウーファー<code>(LFE)</code>チャンネルは失われます。<br><code>output.M = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL +\ninput.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>2</code> <em>(Stereo)</em></td>\n      <td>\n        <em>5.1.からステレオへのダウンミックス。</em><br>中央チャンネルは各側面サラウンドチャンネル(<code>SL</code> と\n        <code>SR</code>)と合算され、各側面チャンネルへ混合されます。2\n        チャンネルへとミックスダウンされる過程は低労力で行われ、各々の場合について、<code>√2/2</code>\n        で乗算されます。サブウーファー (<code>LFE</code>)\n        チャンネルは失われます。<br><code>output.L = input.L + 0.7071 * (input.C + input.SL)<br>output.R =\ninput.R </code><code>+ 0.7071 * (input.C + input.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>5.1.からクアッドへのダウンミックス。</em><br>中央チャンネルは各側面ノンサラウンドチャンネル(<code>L</code> と\n        <code>R</code>)と合算され、各側面チャンネルへ混合されます。2\n        チャンネルへとミックスダウンされる過程は低労力で行われ、各々の場合について、<code>√2/2</code>\n        で乗算されます。サラウンドチャンネルへの伝達には変化がありません。サブウーファー\n        (<code>LFE</code>) チャンネルは失われます。<br><code>output.L = input.L + 0.7071 * input.C<br>output.R = input.R +\n0.7071 * input.C<br><code>output.SL = input.SL<br>output.SR = input.SR</code></code>\n      </td>\n    </tr>\n    <tr>\n      <td colspan=\"2\" rowspan=\"1\">その他、非標準レイアウト</td>\n      <td>\n        非標準チャンネルレイアウトは <code>channelInterpretation</code> を\n        <code>discrete</code>\n        へセットしたものとして操作されます。<br>新たなスピーカーレイアウトの将来的な定義を、仕様書でははっきりと認めています。従ってこの予備部分は、チャンネルに用いられる特定の数字が示すブラウザーの挙動が、将来的に変更された場合のために用意されたものではありません。\n      </td>\n    </tr>\n    <tr>\n      <th colspan=\"1\" rowspan=\"2\" scope=\"row\"><code>discrete</code></th>\n      <td rowspan=\"1\">各 (<code>x</code>)</td>\n      <td rowspan=\"1\">各 (<code>y</code>) (<code>x&lt;y</code> の場合)</td>\n      <td><em>離散チャンネルのアップミックス。</em><br>各出力チャンネルに、それに対応する同番号の入力チャンネルによる入力をします。対応する入力チャンネルが存在しないチャンネルは無音になります。</td>\n    </tr>\n    <tr>\n      <td rowspan=\"1\">各 (<code>x</code>)</td>\n      <td rowspan=\"1\">各 (<code>y</code>) (<code>x&gt;y</code> の場合)</td>\n      <td><em>離散チャンネルのダウンミックス。</em><br>各出力チャンネルに、それに対応する同番号の入力チャンネルによる入力をします。対応する出力チャンネルが存在しない入力チャンネルは無視されます。</td>\n    </tr>\n  </tbody>\n</table>"}},{"type":"prose","value":{"id":"視覚化","title":"視覚化","isH3":false,"content":"<p>原則、オーディオ視覚化は出力オーディオデータ(基本的にはゲインまたは周波数データ)に時間毎にアクセスすることで行われ、更にグラフなどの視覚化出力へ渡すためのグラフィカル技術が用いられます。Web Audio API は <a href=\"/ja/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a> で、経由して渡されたオーディオ信号を変換せず利用することができます。ただしその出力は、<a href=\"/ja/docs/Web/HTML/Element/canvas\"><code>&lt;canvas&gt;</code></a> などのような視覚化技術へ受け渡せるオーディオデータです。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fttaudiodata_en.svg\" alt=\"Without modifying the audio stream, the node allows to get the frequency and time-domain data associated to it, using a FFT.\" width=\"693\" height=\"206\" loading=\"lazy\">\n</p>\n<p>以下のメソッドを利用して、データの取得が可能です。</p>\n<dl>\n  <dt id=\"analysernode.getfloatfrequencydata_en-us\"><a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData\"><code>AnalyserNode.getFloatFrequencyData()</code> <small>(en-US)</small></a></dt>\n  <dd>\n    <p>現在の周波数データを渡された <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code> <small>(en-US)</small></a> 型配列にコピーします。</p>\n  </dd>\n</dl><!---->\n<dl>\n  <dt id=\"analysernode.getbytefrequencydata_en-us\"><a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData\"><code>AnalyserNode.getByteFrequencyData()</code> <small>(en-US)</small></a></dt>\n  <dd>\n    <p>現在の周波数データを渡された <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code> <small>(en-US)</small></a> 型配列(符号なしバイト配列)にコピーします。</p>\n  </dd>\n</dl><!---->\n<dl>\n  <dt id=\"analysernode.getfloattimedomaindata_en-us\"><a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/AnalyserNode/getFloatTimeDomainData\"><code>AnalyserNode.getFloatTimeDomainData()</code> <small>(en-US)</small></a></dt>\n  <dd>\n    <p>現在の波形データまたはタイムドメインデータを渡された<a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code> <small>(en-US)</small></a>型配列にコピーします。</p>\n  </dd>\n  <dt id=\"analysernode.getbytetimedomaindata_en-us\"><a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/AnalyserNode/getByteTimeDomainData\"><code>AnalyserNode.getByteTimeDomainData()</code> <small>(en-US)</small></a></dt>\n  <dd>\n    <p>現在の波形データまたはタイムドメインデータを渡された<a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code> <small>(en-US)</small></a>型配列(符号なしバイト配列)にコピーします。</p>\n  </dd>\n</dl>\n<div class=\"notecard note\" id=\"sect4\">\n  <p><strong>Note:</strong> より詳しい情報は、Web Audio API 記事中の <a href=\"/ja/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">Visualizations with Web Audio API</a> を参照してください。</p>\n</div>"}},{"type":"prose","value":{"id":"立体化","title":"立体化","isH3":false,"content":"<p>(Web Audio API の <a href=\"/ja/docs/Web/API/PannerNode\"><code>PannerNode</code></a> と <a href=\"/ja/docs/Web/API/AudioListener\"><code>AudioListener</code></a> ノードによって操作される)立体音響化によって、オーディオシグナルの、空間を介した点における位置や振る舞いをモデル化することができ、その音声をリスナーが聞くことができます。</p>\n<p>パンナーの位置は、直行座標の右側に描かれています。ドップラー効果を作るに必要な、速度ベクトルを用いた移動、そして方向コーンを用いた方向性があります。このコーンは、例えば無指向性音源などのため、とても大きくなり得ます。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/pannernode_en.svg\" alt=\"The PannerNode brings a spatial position and velocity and a directionality for a given signal.\" width=\"799\" height=\"340\" loading=\"lazy\">\n</p>\n<p>リスナーの位置は、直行座標の右側に描かれています。度ベクトルを用いた移動と、リスナーの頭がポイントされている(頭部側と顔面側の)二方向ベクターを用いた方向性があります。これらはそれぞれリスナーの頭部の頂点からの方向と、リスナーの鼻にポイントされている方向とを定義しており、これらは直角となっています。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/WebAudioListenerReduced.png\" alt=\"We see the position, up and front vectors of an AudioListener, with the up and front vectors at 90° from the other.\" width=\"634\" height=\"250\" loading=\"lazy\">\n</p>\n<div class=\"notecard note\" id=\"sect5\">\n  <p><strong>Note:</strong> より詳しい情報は、<a href=\"/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\" class=\"only-in-en-us\" title=\"Currently only available in English (US)\">Web audio spatialization basics (en-US)</a> を参照してください。</p>\n</div>"}},{"type":"prose","value":{"id":"ファンインとファンアウト","title":"ファンインとファンアウト","isH3":false,"content":"<p>オーディオ用語では、<strong>fan-in</strong> は<a href=\"/ja/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code></a> が一連の単一入力ソースを受け、単一マルチチャンネル信号を出力するプロセスを意味します。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanin.svg\" alt=\"\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>\n<p><strong>Fan-out</strong> はその対となるプロセスを意味します。<a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code> <small>(en-US)</small></a> が一つのマルチチャンネル入力源を受け、複数のモノラル出力信号を出力します。</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanout.svg\" alt=\"\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>"}}],"toc":[{"text":"オーディオグラフ","id":"オーディオグラフ"},{"text":"オーディオデータ: サンプルとは？","id":"オーディオデータ_サンプルとは？"},{"text":"オーディオバッファー: フレーム、サンプル、チャンネルセクション","id":"オーディオバッファー_フレーム、サンプル、チャンネルセクション"},{"text":"オーディオチャンネル","id":"オーディオチャンネル"},{"text":"視覚化","id":"視覚化"},{"text":"立体化","id":"立体化"},{"text":"ファンインとファンアウト","id":"ファンインとファンアウト"}],"summary":"この記事では、アプリを経由した音声伝達方法の設計をする際に、十分な情報に基づいた決断をする助けとなるよう、 Web Audio API の特徴がいかに働いているか、その背後にある音声理論について説明します。この記事はあなたを熟練のサウンドエンジニアにさせることはできないものの、Web Audio API が動く理由を理解するための十分な背景を提供します。","popularity":0,"modified":"2022-10-01T03:41:16.000Z","other_translations":[{"title":"Basic concepts behind Web Audio API","locale":"en-US","native":"English (US)"},{"title":"Les concepts de base de la Web Audio API","locale":"fr","native":"Français"},{"title":"Web Audio API의 기본 개념","locale":"ko","native":"한국어"},{"title":"网页音频接口的基本概念","locale":"zh-CN","native":"中文 (简体)"}],"source":{"folder":"ja/web/api/web_audio_api/basic_concepts_behind_web_audio_api","github_url":"https://github.com/mdn/translated-content/blob/main/files/ja/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md","last_commit_url":"https://github.com/mdn/translated-content/commit/921c46a374ab0a9f4cc809af0370f8c412e54701","filename":"index.md"},"parents":[{"uri":"/ja/docs/Web","title":"開発者向けのウェブ技術"},{"uri":"/ja/docs/Web/API","title":"Web API"},{"uri":"/ja/docs/Web/API/Web_Audio_API","title":"Web Audio API"},{"uri":"/ja/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","title":"Basic concepts behind Web Audio API"}],"pageTitle":"Basic concepts behind Web Audio API - Web API | MDN","noIndexing":false}}