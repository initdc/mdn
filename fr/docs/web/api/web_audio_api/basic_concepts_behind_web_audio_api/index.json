{"doc":{"isMarkdown":true,"isTranslated":true,"isActive":true,"flaws":{},"title":"Les concepts de base de la Web Audio API","mdn_url":"/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","locale":"fr","native":"Français","sidebarHTML":"","body":[{"type":"prose","value":{"id":null,"title":null,"isH3":false,"content":"<p>Cet article explique une partie de la théorie sur laquelle s'appuient les fonctionnalités de la Web Audio API. Il ne fera pas de vous un ingénieur du son, mais vous donnera les bases nécessaires pour comprendre pourquoi la Web Audio API fonctionne de cette manière, et vous permettre de mieux l'utiliser.</p>"}},{"type":"prose","value":{"id":"graphes_audio","title":"Graphes audio","isH3":false,"content":"<p>La Web Audio API implique d'effectuer le traitement du son dans un <strong>contexte</strong> <strong>audio</strong>; elle a été conçue sur le principe de <strong>routage modulaire</strong>. Les opérations basiques sont effectuées dans <strong>noeuds audio</strong>, qui sont liés entre eux pour former un <strong>graphe de routage audio</strong>. Un seul contexte peut supporter plusieurs sources — avec différentes configurations de canaux. Cette architecture modulaire assure la flexibilité nécessaire pour créer des fonctions audio complexes avec des effets dynamiques.</p>\n<p>Les noeuds audio sont liés au niveau de leur entrée et leur sortie, formant une chaîne qui commence avec une ou plusieurs sources, traverse un ou plusieurs noeuds, et se termine avec une sortie spécifique (bien qu'il ne soit pas nécessaire de spécifier une sortie si, par exemple, vous souhaitez seulement visualiser des données audio). Un scénario simple, représentatif de la Web Audio API, pourrait ressembler à ceci&nbsp;:</p>\n<ol>\n  <li>Création d'un contexte audio</li>\n  <li>Dans ce contexte, création des sources — telles que <code>&lt;audio&gt;</code>, oscillateur, flux</li>\n  <li>Création des noeuds d'effets, tels que réverb, filtres biquad, balance, compresseur</li>\n  <li>Choix final de la sortie audio, par exemple les enceintes du système</li>\n  <li>Connection des sources aux effets, et des effets à la sortie.</li>\n</ol>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/webaudioapi_en.svg\" alt=\"Diagramme simple composé de trois rectangles intitulés Sources, Effets et Sortie, reliés par des flèches, de gauche à droite, qui indiquent le sens du flux d'informations audio.\" width=\"643\" height=\"143\" loading=\"lazy\">\n</p>\n<p>Chaque entrée ou sortie est composée de plusieurs <strong>canaux,</strong> chacun correspondant à une configuration audio spécifique. Tout type de canal discret est supporté, y compris <em>mono</em>, <em>stereo</em>, <em>quad</em>, <em>5.1</em>, etc.</p>\n<p>\n  <img src=\"/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/audionodes.svg\" alt=\"Diagramme qui montre comment les AudioNodes sont reliés par leurs entrées et sorties, et la configuration des canaux à l'intérieur de ces entrées/sorties.\" width=\"630\" height=\"360\" loading=\"lazy\">\n</p>\n<p>Les sources audio peuvent être de provenance variée&nbsp;:</p>\n<ul>\n  <li>générées directement en JavaScript avec un noeud audio (tel qu'un oscillateur)</li>\n  <li>créées à partir de données PCM brutes (le contexte audio a des méthodes pour décoder les formats audio supportés)</li>\n  <li>fournies par une balise HTML media (telle que <a href=\"/fr/docs/Web/HTML/Element/video\"><code>&lt;video&gt;</code></a> ou <a href=\"/fr/docs/Web/HTML/Element/audio\"><code>&lt;audio&gt;</code></a>)</li>\n  <li>récupérées directement avec <a href=\"/fr/docs/Web/API/WebRTC_API\">WebRTC</a> <a href=\"/fr/docs/Web/API/MediaStream\"><code>MediaStream</code></a> (une webcam ou un microphone)</li>\n</ul>"}},{"type":"prose","value":{"id":"données_audio_ce_quon_trouve_dans_un_échantillon","title":"Données audio: ce qu'on trouve dans un échantillon","isH3":false,"content":"<p>Lors du traitement d'un signal audio, <strong>l'échantillonage</strong> désigne la conversion d'un <a href=\"http://en.wikipedia.org/wiki/Continuous_signal\" class=\"external\" rel=\" noopener\">signal continu</a> en <a href=\"http://en.wikipedia.org/wiki/Discrete_signal\" class=\"external\" rel=\" noopener\">signal discret</a>; formulé autrement, une onde de son continue, comme un groupe qui joue en live, est convertie en une séquence d'échantillons (un signal temporel discret) qui permet à l'ordinateur de traiter le son en blocs distincts.</p>\n<p>On peut trouver davantage de détails sur la page Wikipédia <a href=\"https://fr.wikipedia.org/wiki/%C3%89chantillonnage_(signal)\" class=\"external\" rel=\" noopener\">Echantillonage (signal)</a>.</p>"}},{"type":"prose","value":{"id":"mémoire_tampon_trames_échantillons_et_canaux","title":"Mémoire tampon&nbsp;: trames, échantillons et canaux","isH3":false,"content":"<p>Un <a href=\"/fr/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a> prend comme paramètres un nombre de canaux (1 pour mono, 2 pour stéréo, etc), une longueur, qui correspond au nombre de trames d'échantillon dans la mémoire tampon, et un taux d'échantillonage, qui indique le nombre de trames d'échantillons lues par seconde.</p>\n<p>Un échantillon est une valeur float32 unique, qui correspond à la valeur du flux audio à un point précis dans le temps, sur un canal spécifique (gauche ou droit dans le cas de la stéréo). Une trame, ou trame d'échantillon est l'ensemble de toutes les valeurs pour tous les canaux (deux pour la stéréo, six pour le 5.1, etc.) à un point précis dans le temps.</p>\n<p>Le taux d'échantillonage est le nombre d'échantillons (ou de trames, puisque tous les échantillons d'une trame jouent en même temps) qui sont joués en une seconde, exprimés en Hz. Plus le taux d'échantillonage est élevé, meilleure est la qualité du son.</p>\n<p>Prenons deux <a href=\"/fr/docs/Web/API/AudioBuffer\"><code>AudioBuffer</code></a>, l'un en mono et l'autre en stéréo, chacun d'une durée de 1 seconde et d'une fréquence de 44100Hz:</p>\n<ul>\n  <li>le mono aura 44100 échantillons, et 44100 trames. Sa propriété <code>length</code> vaudra 44100.</li>\n  <li>le stéréo aura 88200 échantillons, et 44100 trames. Sa propriété <code>length</code> vaudra aussi 44100, puisqu'elle correspond au nombre de trames.</li>\n</ul>\n<p>\n  <img src=\"/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/sampleframe.svg\" alt=\"Le diagramme montre une succession de tames dans un buffer audio. Comme le buffer est composé de deux canaux (stéréo), chaque trame contient deux échantillons.\" width=\"622\" height=\"110\" loading=\"lazy\">\n</p>\n<p>Lorsqu'un noeud de mémoire tampon est lu, on entend d'abord la trame la trame la plus à gauche, puis celle qui la suit à droite, etc. Dans le cas de la stéréo, on entend les deux canaux en même temps. Les trames d'échantillon sont très utiles, car elles représentent le temps indépendamment du nombre de canaux.</p>\n<div class=\"notecard note\" id=\"sect1\">\n  <p><strong>Note :</strong> Pour obtenir le temps en secondes à partir du nombre de trames, diviser le nombre de trames par le taux d'échantillonage. Pour obtenir le nombre de trames à partir du nombre d'échantillons, diviser le nombre d'échantillons par le nombre de canaux.</p>\n</div>\n<p>Voici quelques exemples simples:</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">var</span> contexte <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">var</span> memoireTampon <span class=\"token operator\">=</span> contexte<span class=\"token punctuation\">.</span><span class=\"token function\">createBuffer</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span> <span class=\"token number\">44100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<div class=\"notecard note\" id=\"sect2\">\n  <p><strong>Note :</strong> <strong>44,100 <a href=\"https://en.wikipedia.org/wiki/Hertz\" class=\"external\" rel=\" noopener\">Hz</a></strong> (que l'on peut aussi écrire <strong>44.1 kHz</strong>) est un <a href=\"https://en.wikipedia.org/wiki/Sampling_frequency\" class=\"external\" rel=\" noopener\">taux d'échantillonage</a> couramment utilisé. Pourquoi 44.1kHz ?</p>\n  <p>D'abord, parce ce que le <a href=\"https://en.wikipedia.org/wiki/Hearing_range\" class=\"external\" rel=\" noopener\">champ auditif</a> qui peut être perçu par des oreilles humaines se situe à peu près entre 20 Hz et 20,000 Hz, et que selon le <a href=\"https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\" class=\"external\" rel=\" noopener\">théorème d'échantillonage de Nyquist–Shannon</a> la fréquence d'échantillonage doit être supérieure à deux fois la fréquence maximum que l'on souhaite reproduire; le taux d'échantillonage doit donc être supérieur à 40 kHz.</p>\n  <p>De plus, le signal doit être traité par un <a href=\"https://en.wikipedia.org/wiki/Low-pass_filter\" class=\"external\" rel=\" noopener\">filtre passe-bas</a> avant d'être échantilloné, afin d'éviter le phénomène d'<a href=\"https://en.wikipedia.org/wiki/Aliasing\" class=\"external\" rel=\" noopener\">aliasing</a>, et, si en théorie un filtre passe-bas idéal devrait être capable de laisser passer les fréquences inférieures à 20 kHz (sans les atténuer) et de couper parfaitement les fréquences supérieures à 20 kHz, en pratique une <a href=\"https://en.wikipedia.org/wiki/Transition_band\" class=\"external\" rel=\" noopener\">bande de transition</a> dans laquelle les fréquences sont partiellement atténuées est nécessaire. Plus la bande de transition est large, plus il est facile et économique de faire un <a href=\"https://en.wikipedia.org/wiki/Anti-aliasing_filter\" class=\"external\" rel=\" noopener\">filtre anti-aliasing</a>. Le taux d'échantillonage 44.1 kHz laisse une bande de transition de 2.05 kHz.</p>\n</div>\n<p>Ce code génère une mémoire tampon stéréo (deux canaux) qui, lorsqu'elle est lue dans un AudioContext à 44100Hz (configuration répandue, la plupart des cartes sons tournant à cette fréquence), dure 0.5 secondes: 22050 trames / 44100Hz = 0.5 secondes.</p>\n<div class=\"code-example\"><pre class=\"brush: js notranslate\"><code><span class=\"token keyword\">var</span> contexte <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">AudioContext</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">var</span> memoireTampon <span class=\"token operator\">=</span> context<span class=\"token punctuation\">.</span><span class=\"token function\">createBuffer</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">,</span> <span class=\"token number\">22050</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n</code></pre></div>\n<p>Ce code génère une mémoire tampon mono (un seul canal) qui, lorsqu'elle est lue dans un AudioContext à 44100Hzz, est automatiquement *rééchantillonnée* à 44100Hz (et par conséquent produit 44100 trames), et dure 1.0 seconde: 44100 frames / 44100Hz = 1 seconde.</p>\n<div class=\"notecard note\" id=\"sect3\">\n  <p><strong>Note :</strong> le rééchantillonnage audio est très similaire à la redimension d'une image&nbsp;: imaginons que vous ayiez une image de 16 x 16, mais que vous vouliez remplir une surface de 32x32: vous la redimensionnez (rééchantillonnez). Le résultat est de qualité inférieure (il peut être flou ou crénelé, en fonction de l'algorithme de redimensionnement), mais cela fonctionne, et l'image redimensionnée prend moins de place que l'originale. C'est la même chose pour le rééchantillonnage audio — vous gagnez de la place, mais en pratique il sera difficle de reproduire correctement des contenus de haute fréquence (c'est-à-dire des sons aigus).</p>\n</div>","titleAsText":"Mémoire tampon : trames, échantillons et canaux"}},{"type":"prose","value":{"id":"mémoire_tampon_linéaire_ou_entrelacée","title":"Mémoire tampon linéaire ou entrelacée","isH3":true,"content":"<p>La Web Audio API utilise un format de mémoire tampon linéaire : les canaux gauche et droite sont stockés de la façon suivante :</p>\n<pre class=\"notranslate\">LLLLLLLLLLLLLLLLRRRRRRRRRRRRRRRR (pour un buffer de 16 trames)\n</pre>\n<p>C'est assez courant dans le traitement audio, car cela permet de traiter facilement chaque canal de façon indépendante.</p>\n<p>L'alternative est d'utiliser un format entrelacé:</p>\n<pre class=\"notranslate\">LRLRLRLRLRLRLRLRLRLRLRLRLRLRLRLR (pour un buffer de 16 trames)\n</pre>\n<p>Ce format est communément utilisé pour stocker et lire du son avec très peu de traitement, comme par exemple pour un flux de MP3 décodé.</p>\n<p>La Web Audio API expose *uniquement* des buffer linéaires, car elle est faite pour le traitement du son. Elle fonctionne en linéaire, mais convertit les données au format entrelacé au moment de les envoyer à la carte son pour qu'elles soient jouées. A l'inverse, lorsqu'un MP3 est décodé, le format d'origine entrelacé est converti en linéaire pour le traitement.</p>"}},{"type":"prose","value":{"id":"canaux_audio","title":"Canaux audio","isH3":false,"content":"<p>Une mémoire tampon audio peut contenir différents nombres de canaux, depuis les configurations simple mono (un seul canal) ou stéréo (canal gauche et canal droit) en allant jusquà des configurations plus complexe comme le quad ou le 5.1, pour lesquels chaque canal contient plusieurs échantillons de sons, ce qui permet une expérience sonore plus riche. Les canaux sont généralement représentés par les abbréviations standard détaillées dans le tableau ci-après :</p>\n<table class=\"standard-table\">\n  <tbody>\n    <tr>\n      <td><em>Mono</em></td>\n      <td><code>0: M: mono</code></td>\n    </tr>\n    <tr>\n      <td><em>Stereo</em></td>\n      <td><code>0: L: gauche<br>1: R: droit</code></td>\n    </tr>\n    <tr>\n      <td><em>Quad</em></td>\n      <td><code>0: L: gauche<br>1: R: droit<br>2: SL: surround gauche<br>3: SR:\nsurround droit</code></td>\n    </tr>\n    <tr>\n      <td><em>5.1</em></td>\n      <td><code>0: L: gauche<br>1: R: droit<br>2: C: centre<br>3: LFE:\nsubwoofer<br>4: SL: surround gauche<br>5: SR: surround droit</code></td>\n    </tr>\n  </tbody>\n</table>"}},{"type":"prose","value":{"id":"conversion_ascendante_et_descendante","title":"Conversion ascendante et descendante","isH3":true,"content":"<p>Lorsque le nombre de canaux n'est pas le même en entrée et en sortie, on effectue une conversion ascendante ou descendante selon les règles suivantes. Cela peut être plus ou moins controllé en assignant la valeur <code>speakers</code> ou <code>discrete</code> à la propriété <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/AudioNode/channelInterpretation\"><code>AudioNode.channelInterpretation</code> <small>(en-US)</small></a> .</p>\n<table class=\"standard-table\">\n  <thead>\n    <tr>\n      <th scope=\"row\">Interprétation</th>\n      <th scope=\"col\">Canaux d'entrée</th>\n      <th scope=\"col\">Canaux de sortie</th>\n      <th scope=\"col\">Règles de conversion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th colspan=\"1\" rowspan=\"13\" scope=\"row\"><code>speakers</code></th>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>2</code> <em>(Stéréo)</em></td>\n      <td>\n        <em>Conversion ascendante de mono vers stéréo</em>.<br>Le canal\n        d'entrée <code>M</code> est utilisé pour les deux canaux de sortie\n        (<code>L</code> et <code>R</code>).<br><code>output.L = input.M<br>output.R = input.M</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Conversion ascendante de mono vers quad.</em><br>Le canal d'entrée\n        <code>M</code> est utilisé pour les canaux de sortie autres que surround\n        (<code>L</code> et <code>R</code>). Les canaux de sortie surround (<code>SL</code>\n        et <code>SR</code>) sont silencieux.<br><code>output.L = input.M<br>output.R = input.M<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Conversion ascendante de mono vers 5.1.</em><br>Le canal d'entrée\n        <code>M</code> est utilisé pour le canal de sortie central\n        (<code>C</code>). Tous les autres canaux (<code>L</code>,\n        <code>R</code>, <code>LFE</code>, <code>SL</code>, et <code>SR</code>)\n        sont silencieux.<br><code>output.L = 0<br>output.R = 0</code><br><code>output.C = input.M<br>output.LFE = 0<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stéréo)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Conversion descendante de stéréo vers mono</em>.<br>Les deux\n        canaux d'entrée (<code>L</code> et <code>R</code>) sont combinées pour\n        produire l'unique canal de sortie (<code>M</code>).<br><code>output.M = 0.5 * (input.L + input.R)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stéréo)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Conversion ascendante de stéréo vers quad.</em><br>Les canaux\n        d'entrée <code>L</code> et <code>R </code>input sont utilisés pour leurs\n        équivalents respectifs non-surround en sortie (<code>L</code> et\n        <code>R</code>). Les canaux de sortie surround (<code>SL</code> et\n        <code>SR</code>) sont silencieux.<br><code>output.L = input.L<br>output.R = input.R<br>output.SL = 0<br>output.SR\n= 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>2</code> <em>(Stéréo)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Conversion ascendante de stéréo vers 5.1.</em><br>Les canaux\n        d'entrée <code>L</code> et <code>R </code>sont utilisés pour leurs\n        équivalents respectifs non-surround en sortie (<code>L</code> et\n        <code>R</code>). Les canaux de sortie surround (<code>SL</code> et\n        <code>SR</code>), ainsi que le canal central (<code>C</code>) et le\n        canal subwoofer (<code>LFE</code>) restent silencieux.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = 0<br>output.SR = 0</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Conversion descendante de quad vers mono</em>.<br>Les quatre\n        canaux de sortie (<code>L</code>, <code>R</code>, <code>SL</code>, et\n        <code>SR</code>) sont combinés pour produire l'unique canal de sortie\n        (<code>M</code>).<br><code>output.M = 0.25 * (input.L + input.R + </code><code>input.SL + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>2</code> <em>(Stéréo)</em></td>\n      <td>\n        <em>Conversion descendante de quad vers stéréo</em>.<br>Les deux\n        canaux d'entrée à gauche (<code>L</code> and <code>SL</code>) sont\n        combinés pour produire l'unique canal de sortie à gauche\n        (<code>L</code>). De la même façon, les deux canaux d'entrée à droite\n        (<code>R</code> et <code>SR</code>) sont combinés pour produire l'unique\n        canal de sortie à droite (<code>R</code>).<br><code>output.L = 0.5 * (input.L + input.SL</code><code>)</code><br><code>output.R = 0.5 * (input.R + input.SR</code><code>)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td>\n        <em>Conversion ascendante de quad vers 5.1.</em><br>Les canaux\n        d'entrée <code>L</code>, <code>R</code>, <code>SL</code>, et\n        <code>SR</code> sont utilisés pour leur canaux de sortie équivalents\n        respectifs (<code>L</code> and <code>R</code>). Le canal central\n        (<code>C</code>) et le canal subwoofer (<code>LFE</code>) restent\n        silencieux.<br><code>output.L = input.L<br>output.R = input.R<br>output.C = 0<br>output.LFE\n= 0<br>output.SL = input.SL<br>output.SR = input.SR</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>1</code> <em>(Mono)</em></td>\n      <td>\n        <em>Conversion descendante de 5.1 vers mono.</em><br>Les canaux de\n        gauche (<code>L</code> et <code>SL</code>), de droite (<code>R</code> et\n        <code>SR</code>) et central sont tous mixés ensemble. Les canaux\n        surround sont légèrement atténués et la puissance des canaux latéraux\n        est compensée pour la faire compter comme un seul canal en la\n        multipliant par <code>√2/2</code>. Le canal subwoofer (<code>LFE</code>)\n        est perdu.<br><code>output.M = 0.7071 * (input.L + input.R) + input.C + 0.5 * (input.SL +\ninput.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>2</code> <em>(Stéréo)</em></td>\n      <td>\n        <em>Conversion descendante de 5.1 vers stéréo.</em><br>Le canal\n        central (<code>C</code>) est additionné avec chacun des canaux latéraux\n        (<code>SL</code> et <code>SR</code>) puis combiné avec chacun des canaux\n        latéraux (L et R). Comme il est converti en deux canaux, il est mixé à\n        une puissance inférieure : multiplié par <code>√2/2</code>. Le canal\n        subwoofer (<code>LFE</code>) est perdu.<br><code>output.L = input.L + 0.7071 * (input.C + input.SL)<br>output.R =\ninput.R </code><code>+ 0.7071 * (input.C + input.SR)</code>\n      </td>\n    </tr>\n    <tr>\n      <td><code>6</code> <em>(5.1)</em></td>\n      <td><code>4</code> <em>(Quad)</em></td>\n      <td>\n        <em>Conversion descendante de 5.1 vers quad.</em><br>Le canal central\n        (<code>C</code>) est combiné avec les canaux latéraux non-surround\n        (<code>L</code> et <code>R</code>). Comme il est converti en deux\n        canaux, il est mixé à une puissance inférieure : multiplié par\n        <code>√2/2</code>. Les canaux surround restent inchangés. Le canal\n        subwoofer (<code>LFE</code>) est perdu.<br><code>output.L = input.L + 0.7071 * input.C<br>output.R = input.R +\n0.7071 * input.C<br><code>output.SL = input.SL<br>output.SR = input.SR</code></code>\n      </td>\n    </tr>\n    <tr>\n      <td colspan=\"2\" rowspan=\"1\">Autres configurations non-standard</td>\n      <td>\n        Les configurations non-standard sont traitées comme si la propriété\n        <code>channelInterpretation</code> avait la valeur\n        <code>discrete</code>.<br>La spécification autorise explicitement la\n        définition à venir de nouvelles configurations de sortie pour les\n        enceintes. Ce cas de figure n'est par conséquent pas garanti dans le\n        futur, car le comportement des navigateurs pour un nombre spécifique de\n        canaux pourrait être amené à changer.\n      </td>\n    </tr>\n    <tr>\n      <th colspan=\"1\" rowspan=\"2\" scope=\"row\"><code>discrete</code></th>\n      <td rowspan=\"1\">tout (<code>x</code>)</td>\n      <td rowspan=\"1\">tout (<code>y</code>) pour lequel <code>x&lt;y</code></td>\n      <td>\n        <em>Conversion ascendante de canaux discrets.</em><br>Remplit chaque\n        canal de sortie avec son équivalent en entrée, c'est-à-dire le canal qui\n        a le même index. Les canaux de sortie qui n'ont pas d'équivalent en\n        entrée restent silencieux.\n      </td>\n    </tr>\n    <tr>\n      <td rowspan=\"1\">tout (<code>x</code>)</td>\n      <td rowspan=\"1\">tout (<code>y</code>) pour lequel <code>x&gt;y</code></td>\n      <td>\n        <em>Conversion descendante de canaux discrets.</em><br>Remplit chaque\n        canal de sortie avec son équivalent en entrée, c'est-à-dire le canal qui\n        a le même index. Les canaux d'entrée qui n'ont pas d'équivalent en\n        sortie sont perdus.\n      </td>\n    </tr>\n  </tbody>\n</table>"}},{"type":"prose","value":{"id":"visualisations","title":"Visualisations","isH3":false,"content":"<p>Une visualisation audio consiste en général à utiliser un flux de données audio dans le temps (souvent des informations de gain ou de fréquence) pour générer un affichage graphique (comme un graphe). La Web Audio API possède un <a href=\"/fr/docs/Web/API/AnalyserNode\"><code>AnalyserNode</code></a> qui n'altère pas le signal audio qui le traverse, permettant de générer des données qui peuvent être utilisées par une technologie de visualisation telle que <a href=\"/fr/docs/Web/HTML/Element/canvas\"><code>&lt;canvas&gt;</code></a>.</p>\n<p>\n  <img src=\"/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fttaudiodata.svg\" alt=\"Le noeud permet de récupérer la fréquence et le domaine temporel en utilisant FFT, et ce sans modifier le flux audio\" width=\"693\" height=\"206\" loading=\"lazy\">\n</p>\n<p>On peut accéder aux données en utilisant les méthodes suivantes:</p>\n<dl>\n  <dt id=\"analysernode.getfloatfrequencydata\"><a href=\"/fr/docs/Web/API/AnalyserNode/getFloatFrequencyData\"><code>AnalyserNode.getFloatFrequencyData()</code></a></dt>\n  <dd>\n    <p>Copie les données de fréquence dans le tableau <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code> <small>(en-US)</small></a> passé en argument.</p>\n  </dd>\n</dl><!---->\n<dl>\n  <dt id=\"analysernode.getbytefrequencydata\"><a href=\"/fr/docs/Web/API/AnalyserNode/getByteFrequencyData\"><code>AnalyserNode.getByteFrequencyData()</code></a></dt>\n  <dd>\n    <p>Copies les données de fréquence dans le tableau d'octets non signés <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code> <small>(en-US)</small></a> passé en argument.</p>\n  </dd>\n</dl><!---->\n<dl>\n  <dt id=\"analysernode.getfloattimedomaindata\"><a href=\"/fr/docs/Web/API/AnalyserNode/getFloatTimeDomainData\"><code>AnalyserNode.getFloatTimeDomainData()</code></a></dt>\n  <dd>\n    <p>Copie les données de l'onde de forme, ou domaine temporel, dans le <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Float32Array\"><code>Float32Array</code> <small>(en-US)</small></a> passé en argument.</p>\n  </dd>\n  <dt id=\"analysernode.getbytetimedomaindata\"><a href=\"/fr/docs/Web/API/AnalyserNode/getByteTimeDomainData\"><code>AnalyserNode.getByteTimeDomainData()</code></a></dt>\n  <dd>\n    <p>Copie les données de l'onde de forme, ou domaine temporel, dans le tableau d'octets non signés <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array\"><code>Uint8Array</code> <small>(en-US)</small></a> passé en argument.</p>\n  </dd>\n</dl>\n<div class=\"notecard note\" id=\"sect4\">\n  <p><strong>Note :</strong> Pour plus d'informations, voir notre article <a href=\"/fr/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">Visualizations with Web Audio API</a>.</p>\n</div>"}},{"type":"prose","value":{"id":"spatialisations","title":"Spatialisations","isH3":false,"content":"<p>Une spatialisation audio (gérée par les noeuds <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/PannerNode\"><code>PannerNode</code> <small>(en-US)</small></a> et <a href=\"/fr/docs/Web/API/AudioListener\"><code>AudioListener</code></a> dans la Web Audio API) permet de modéliser la position et le comportement d'un signal audio situé dans l'espace, ainsi que l'auditeur qui perçoit ce signal.</p>\n<p>La position du panoramique est décrite avec des coodonnées cartésiennes selon la règle de la main droite, son mouvement à l'aide d'un vecteur de vélocité (nécessaire pour la création d'effets Doppler) et sa direction avec un cone de direction. Le cone peut être très large, par exemple dans le cas de sources omnidirectionnelles.</p>\n<p>\n  <img src=\"/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/pannernode.svg\" alt=\"Le PannerNode donne la position dans l'espace, la vélocité et la direction d'un signal donné\" width=\"799\" height=\"340\" loading=\"lazy\">\n</p>\n<p>La position de l'auditeur est décrite avec des coodonnées cartésiennes selon la règle de la main droite, son mouvement à l'aide d'un vecteur de vélocité et la direction vers laquelle elle pointe en utilisant deux vecteurs de direction : haut et face. Ceux-ci définissent respectivement la direction vers laquelle pointent le haut de la tête et le bout du nez de l'auditeur, et forment un angle droit entre eux.</p>\n<p>\n  <img src=\"/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/listener.svg\" alt=\"On voit la position d'un auditeur, ainsi que les vecteurs de direction haut et de face qui forment un angle de 90°\" width=\"720\" height=\"249\" loading=\"lazy\">\n</p>\n<div class=\"notecard note\" id=\"sect5\">\n  <p><strong>Note :</strong> For more information, see our <a href=\"/fr/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\">Web audio spatialization basics</a> article.</p>\n</div>"}},{"type":"prose","value":{"id":"fan-in_et_fan-out","title":"Fan-in et Fan-out","isH3":false,"content":"<p>En audio, <strong>fan-in</strong> désigne le processus par lequel un <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/ChannelMergerNode\"><code>ChannelMergerNode</code> <small>(en-US)</small></a> prend une série d'entrées mono entrée et restitue un seul signal multi-canaux :</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanin.svg\" alt=\"\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>\n<p><strong>Fan-out</strong> désigne le processus opposé, par lequel un <a class=\"only-in-en-us\" title=\"Currently only available in English (US)\" href=\"/en-US/docs/Web/API/ChannelSplitterNode\"><code>ChannelSplitterNode</code> <small>(en-US)</small></a> prend une source multi-canaux en entrée et restitue plusieurs signaux mono en sortie:</p>\n<p>\n  <img src=\"/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API/fanout.svg\" alt=\"\" width=\"325\" height=\"258\" loading=\"lazy\">\n</p>"}}],"toc":[{"text":"Graphes audio","id":"graphes_audio"},{"text":"Données audio: ce qu'on trouve dans un échantillon","id":"données_audio_ce_quon_trouve_dans_un_échantillon"},{"text":"Mémoire tampon&nbsp;: trames, échantillons et canaux","id":"mémoire_tampon_trames_échantillons_et_canaux"},{"text":"Canaux audio","id":"canaux_audio"},{"text":"Visualisations","id":"visualisations"},{"text":"Spatialisations","id":"spatialisations"},{"text":"Fan-in et Fan-out","id":"fan-in_et_fan-out"}],"summary":"Cet article explique une partie de la théorie sur laquelle s'appuient les fonctionnalités de la Web Audio API. Il ne fera pas de vous un ingénieur du son, mais vous donnera les bases nécessaires pour comprendre pourquoi la Web Audio API fonctionne de cette manière, et vous permettre de mieux l'utiliser.","popularity":0,"modified":"2022-10-01T03:41:16.000Z","other_translations":[{"title":"Basic concepts behind Web Audio API","locale":"en-US","native":"English (US)"},{"title":"Basic concepts behind Web Audio API","locale":"ja","native":"日本語"},{"title":"Web Audio API의 기본 개념","locale":"ko","native":"한국어"},{"title":"网页音频接口的基本概念","locale":"zh-CN","native":"中文 (简体)"}],"source":{"folder":"fr/web/api/web_audio_api/basic_concepts_behind_web_audio_api","github_url":"https://github.com/mdn/translated-content/blob/main/files/fr/web/api/web_audio_api/basic_concepts_behind_web_audio_api/index.md","last_commit_url":"https://github.com/mdn/translated-content/commit/921c46a374ab0a9f4cc809af0370f8c412e54701","filename":"index.md"},"parents":[{"uri":"/fr/docs/Web","title":"Technologies web pour développeurs"},{"uri":"/fr/docs/Web/API","title":"Référence Web API"},{"uri":"/fr/docs/Web/API/Web_Audio_API","title":"Web Audio API"},{"uri":"/fr/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API","title":"Les concepts de base de la Web Audio API"}],"pageTitle":"Les concepts de base de la Web Audio API - Référence Web API | MDN","noIndexing":false}}